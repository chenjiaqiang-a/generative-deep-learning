{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "from utils.write import collapse_documents, test_data, glove, expand_answers, look_up_token, START_TOKEN, END_TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run params\n",
    "SECTION = 'write'\n",
    "RUN_ID = '0002'\n",
    "DATA_NAME = 'qa'\n",
    "RUN_FOLDER = 'run/{}/'.format(SECTION)\n",
    "RUN_FOLDER += '_'.join([RUN_ID, DATA_NAME])\n",
    "\n",
    "if not os.path.exists(RUN_FOLDER):\n",
    "    os.makedirs(RUN_FOLDER)\n",
    "    os.makedirs(os.path.join(RUN_FOLDER, 'viz'))\n",
    "    os.makedirs(os.path.join(RUN_FOLDER, 'images'))\n",
    "    os.makedirs(os.path.join(RUN_FOLDER, 'weights'))\n",
    "\n",
    "mode = 'build'  # 'load'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### LOAD DATA ####\n",
    "test_data_gen = test_data()\n",
    "batch = next(test_data_gen)\n",
    "batch = collapse_documents(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = glove.shape[0]\n",
    "EMBEDDING_DIMENS = glove.shape[1]\n",
    "\n",
    "GRU_UNITS = 100\n",
    "MAX_DOC_SIZE = None\n",
    "MAX_ANSWER_SIZE = None\n",
    "MAX_Q_SIZE = None\n",
    "\n",
    "#### TRAINING MODEL ####\n",
    "document_tokens = keras.Input(shape=(MAX_DOC_SIZE,), name='document_tokens')\n",
    "\n",
    "embedding = keras.layers.Embedding(\n",
    "    input_dim=VOCAB_SIZE,\n",
    "    output_dim=EMBEDDING_DIMENS,\n",
    "    weights=[glove],\n",
    "    mask_zero=True,\n",
    "    name='embedding'\n",
    ")\n",
    "document_emb = embedding(document_tokens)\n",
    "\n",
    "answer_outputs = keras.layers.Bidirectional(\n",
    "    keras.layers.GRU(GRU_UNITS, return_sequences=True),\n",
    "    name='answer_outputs'\n",
    ")(document_emb)\n",
    "answer_tags = keras.layers.Dense(\n",
    "    2, activation='softmax',\n",
    "    name='answer_tags'\n",
    ")(answer_outputs)\n",
    "\n",
    "encoder_input_mask = keras.Input(\n",
    "    shape=(MAX_ANSWER_SIZE, MAX_DOC_SIZE),\n",
    "    name='encoder_input_mask'\n",
    ")\n",
    "encoder_inputs = keras.layers.Lambda(\n",
    "    lambda x: tf.matmul(x[0], x[1]),\n",
    "    name=\"encoder_inputs\"\n",
    ")([encoder_input_mask, answer_outputs])\n",
    "encoder_cell = keras.layers.GRU(\n",
    "    2 * GRU_UNITS, name='encoder_cell'\n",
    ")(encoder_inputs)\n",
    "\n",
    "decoder_inputs = keras.Input(shape=(MAX_Q_SIZE,), name='decoder_inputs')\n",
    "decoder_emb = embedding(decoder_inputs)\n",
    "decoder_emb.trainable = False\n",
    "decoder_cell = keras.layers.GRU(\n",
    "    2 * GRU_UNITS,\n",
    "    return_sequences=True,\n",
    "    name='decoder_cell'\n",
    ")\n",
    "decoder_states = decoder_cell(decoder_emb, initial_state=[encoder_cell])\n",
    "\n",
    "decoder_projection = keras.layers.Dense(\n",
    "    VOCAB_SIZE, name='decoder_projection',\n",
    "    activation='softmax', use_bias=False\n",
    ")\n",
    "decoder_outputs = decoder_projection(decoder_states)\n",
    "\n",
    "total_model = keras.Model([document_tokens, decoder_inputs, encoder_input_mask],\n",
    "                          [answer_tags, decoder_outputs])\n",
    "\n",
    "#### INFERENCE MODEL ####\n",
    "decoder_inputs_dynamic = keras.Input(shape=(1,), name=\"decoder_inputs_dynamic\")\n",
    "decoder_emb_dynamic = embedding(decoder_inputs_dynamic)\n",
    "decoder_init_state_dynamic = keras.Input(\n",
    "    shape=(2 * GRU_UNITS,),\n",
    "    name='decoder_init_state_dynamic'\n",
    ")  # the embedding of the previous word\n",
    "decoder_states_dynamic = decoder_cell(decoder_emb_dynamic, \n",
    "                                      initial_state=[decoder_init_state_dynamic])\n",
    "decoder_outputs_dynamic = decoder_projection(decoder_states_dynamic)\n",
    "\n",
    "answer_model = keras.Model(document_tokens, [answer_tags])\n",
    "decoder_initial_state_model = keras.Model([document_tokens, encoder_input_mask], [encoder_cell])\n",
    "question_model = keras.Model([decoder_inputs_dynamic, decoder_init_state_dynamic],\n",
    "                             [decoder_outputs_dynamic, decoder_states_dynamic])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Â LOAD MODEL WEIGHTS ####\n",
    "model_num = 1\n",
    "\n",
    "total_model.load_weights(os.path.join(RUN_FOLDER, 'weights/weights_{}.h5'.format(model_num)), by_name = True)\n",
    "question_model.load_weights(os.path.join(RUN_FOLDER, 'weights/weights_{}.h5'.format(model_num)), by_name = True)\n",
    "answer_model.load_weights(os.path.join(RUN_FOLDER, 'weights/weights_{}.h5'.format(model_num)), by_name = True)\n",
    "decoder_initial_state_model.load_weights(os.path.join(RUN_FOLDER, 'weights/weights_{}.h5'.format(model_num)), by_name = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# answer placement predictions\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "idx = 0\n",
    "\n",
    "answer_preds = answer_model.predict(batch[\"document_tokens\"])\n",
    "\n",
    "print('Predicted answer probabilities')\n",
    "ax = plt.gca()\n",
    "ax.xaxis.grid(True)\n",
    "plt.plot(answer_preds[idx, :, 1])\n",
    "plt.show()\n",
    "\n",
    "for i in range(len(batch['document_words'][idx])):\n",
    "    print(i, batch['document_words'][idx][i],\n",
    "          np.round(answer_preds[idx][i][1], 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set chosen answer position\n",
    "start_answer = 37\n",
    "end_answer = 39\n",
    "\n",
    "print(batch['document_words'][idx][start_answer:(1+end_answer)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_preds = answer_model.predict(batch[\"document_tokens\"])\n",
    "\n",
    "answers = [[0] * len(answer_preds[idx])]\n",
    "for i in range(start_answer, end_answer + 1):\n",
    "    answers[0][i] = 1\n",
    "\n",
    "answer_batch = expand_answers(batch, answers)\n",
    "\n",
    "next_decoder_init_state = decoder_initial_state_model.predict(\n",
    "    [answer_batch['document_tokens'][[idx]], answer_batch['answer_masks'][[idx]]])\n",
    "\n",
    "word_tokens = [START_TOKEN]\n",
    "questions = [look_up_token(START_TOKEN)]\n",
    "\n",
    "ended = False\n",
    "counter = 0\n",
    "\n",
    "while not ended:\n",
    "\n",
    "    counter += 1\n",
    "\n",
    "    word_preds, next_decoder_init_state = question_model.predict(\n",
    "        [word_tokens, next_decoder_init_state])\n",
    "\n",
    "    next_decoder_init_state = np.squeeze(next_decoder_init_state, axis=1)\n",
    "    word_tokens = np.argmax(word_preds, 2)[0]\n",
    "\n",
    "    questions.append(look_up_token(word_tokens[0]))\n",
    "\n",
    "    if word_tokens[0] == END_TOKEN or counter > 20:\n",
    "        ended = True\n",
    "\n",
    "questions = ' '.join(questions)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('TensorFlow')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d71d38bd0d71aa8fb096966ce492050b4e1d8055a06fdbaefbf5b2c66243d19c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
