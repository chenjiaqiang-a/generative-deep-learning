{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "load_saved_model = False\n",
    "train_model = True\n",
    "\n",
    "SECTION = 'write'\n",
    "RUN_ID = '0001'\n",
    "RUN_FOLDER = 'run/{}/'.format(SECTION)\n",
    "RUN_FOLDER += '_'.join([RUN_ID, 'aesop'])\n",
    "\n",
    "if not os.path.exists(RUN_FOLDER):\n",
    "    os.makedirs(RUN_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the text and perform some cleanup\n",
    "token_type = 'word'\n",
    "seq_length = 20\n",
    "\n",
    "filename = './data/aesop/data.txt'\n",
    "with open(filename, encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# Removing text before and after the main stories\n",
    "start = text.find(\"THE FOX AND THE GRAPES\\n\\n\\n\")\n",
    "end = text.find(\"ILLUSTRATIONS\\n\\n\\n[\")\n",
    "text = text[start:end]\n",
    "\n",
    "start_story = '| ' * seq_length\n",
    "\n",
    "text = start_story + text;\n",
    "text = text.lower()\n",
    "text = text.replace('\\n\\n\\n\\n\\n', start_story)\n",
    "text = text.replace('\\n', ' ')\n",
    "text = re.sub('  +', '. ', text).strip()\n",
    "text = text.replace('..', '.')\n",
    "\n",
    "text = re.sub('([!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~])', r' \\1 ', text)\n",
    "text = re.sub('\\s{2,}', ' ', text)\n",
    "\n",
    "print(len(text))\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization\n",
    "if token_type == 'word':\n",
    "    tokenizer = keras.preprocessing.text.Tokenizer(char_level=False, filters='')\n",
    "else:\n",
    "    tokenizer = keras.preprocessing.text.Tokenizer(char_level=True, filters='', lower=False)\n",
    "\n",
    "tokenizer.fit_on_texts([text])\n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "token_list = tokenizer.texts_to_sequences([text])[0]\n",
    "\n",
    "print(total_words)\n",
    "print(tokenizer.word_index)\n",
    "print(token_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sequences(token_list, step):\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    for i in range(0, len(token_list) - seq_length, step):\n",
    "        X.append(token_list[i : i + seq_length])\n",
    "        y.append(token_list[i + seq_length])\n",
    "    y = keras.utils.to_categorical(y, num_classes=total_words)\n",
    "\n",
    "    num_seq = len(X)\n",
    "    print('Number of sequences: ', num_seq)\n",
    "    return X, y, num_seq\n",
    "\n",
    "X, y, num_seq = generate_sequences(token_list, step=1)\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if load_saved_model:\n",
    "    model = keras.models.load_model(os.path.join(RUN_FOLDER, 'aesop_no_dropout_100.h5'))\n",
    "else:\n",
    "    n_units = 256\n",
    "    embedding_size = 100\n",
    "\n",
    "    text_in = keras.Input(shape=(None,))\n",
    "    embedding = keras.layers.Embedding(total_words, embedding_size)\n",
    "    x = embedding(text_in)\n",
    "    x = keras.layers.LSTM(n_units)(x)\n",
    "    text_out = keras.layers.Dense(total_words, activation='softmax')(x)\n",
    "\n",
    "    model = keras.Model(text_in, text_out)\n",
    "    optim = keras.optimizers.RMSprop(learning_rate=0.001)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optim)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_with_temp(preds, temperature=1.0):\n",
    "    # Helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probs = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probs)\n",
    "\n",
    "def generate_text(seed_text, next_word, model, max_sequence_len, temp):\n",
    "    output_text = seed_text\n",
    "    seed_text = start_story + seed_text\n",
    "\n",
    "    for _ in range(next_word):\n",
    "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "        token_list = token_list[-max_sequence_len:]\n",
    "        token_list = np.reshape(token_list, (1, max_sequence_len))\n",
    "\n",
    "        probs = model.predict(token_list, verbose=0)[0]\n",
    "        y_class = sample_with_temp(probs, temperature=temp)\n",
    "\n",
    "        if y_class == 0:\n",
    "            output_word = ''\n",
    "        else:\n",
    "            output_word = tokenizer.index_word[y_class]\n",
    "        \n",
    "        if output_word == '|':\n",
    "            break\n",
    "\n",
    "        if token_type == 'word':\n",
    "            output_text += output_word + ' '\n",
    "            seed_text += output_word + ' '\n",
    "        else:\n",
    "            output_text += output_word\n",
    "            seed_text += output_word\n",
    "    \n",
    "    return output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_epoch_end(epoch, logs):\n",
    "    seed_text = ''\n",
    "    gen_words = 500\n",
    "\n",
    "    print('Temp 0.2')\n",
    "    print(generate_text(seed_text, gen_words, model, seq_length, temp=0.2))\n",
    "    print('Temp 0.33')\n",
    "    print(generate_text(seed_text, gen_words, model, seq_length, temp=0.33))\n",
    "    print('Temp 0.5')\n",
    "    print(generate_text(seed_text, gen_words, model, seq_length, temp=0.5))\n",
    "    print('Temp 1.0')\n",
    "    print(generate_text(seed_text, gen_words, model, seq_length, temp=1))\n",
    "\n",
    "if train_model:\n",
    "    epochs = 1000\n",
    "    batch_size = 32\n",
    "    num_batches = int(len(X) / batch_size)\n",
    "    callback = keras.callbacks.LambdaCallback(on_epoch_end=on_epoch_end)\n",
    "    model.fit(X, y, epochs=epochs, batch_size=batch_size, callbacks=[callback], shuffle=True)\n",
    "    model.save(os.path.join(RUN_FOLDER, 'aesop_no_dropout_100.h5'));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_text = \"the ass and his lion . \"\n",
    "gen_words = 500\n",
    "temp = 0.2\n",
    "\n",
    "print(generate_text(seed_text, gen_words, model, seq_length, temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_text = \"the great man and the time . \"\n",
    "gen_words = 500\n",
    "temp = 1.0\n",
    "\n",
    "print(generate_text(seed_text, gen_words, model, seq_length, temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top10_next_words(seed_text, model):\n",
    "    print(seed_text)\n",
    "    seed_text = start_story + seed_text\n",
    "    token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "    token_list = token_list[-seq_length:]\n",
    "    token_list = np.reshape(token_list, (1, seq_length))\n",
    "\n",
    "    probs = model.predict(token_list, verbose=0)[0]\n",
    "\n",
    "    top_10_idx = np.flip(np.argsort(probs)[-10:])\n",
    "    top_10_probs = [probs[x] for x in top_10_idx]\n",
    "    top_10_words = tokenizer.sequences_to_texts([[x] for x in top_10_idx])\n",
    "\n",
    "    for prob, word in zip(top_10_probs, top_10_words):\n",
    "        print('{:<6.1%} : {}'.format(prob, word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top10_next_words('the fox and the stag . there was a', model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top10_next_words('the fox and the snake . one day a fox', model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top10_next_words('the dog and the hare . the dog was lying', model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top10_next_words('the farmer and his sheep . a farmer was', model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top10_next_words('the eagle and the sea .', model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top10_next_words('the lion said ,', model)\n",
    "top10_next_words('the lion said , and', model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_human_led_text(seed_text, model, max_sequence_len):\n",
    "    output_text = ''\n",
    "    seed_text = start_story\n",
    "\n",
    "    while 1:\n",
    "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "        token_list = token_list[-max_sequence_len:]\n",
    "        token_list = np.reshape(token_list, (1, max_sequence_len))\n",
    "\n",
    "        probs = model.predict(token_list, verbose=0)[0]\n",
    "\n",
    "        top_10_idx = np.flip(np.argsort(probs)[-10:])\n",
    "        top_10_probs = [probs[x] for x in top_10_idx]\n",
    "        top_10_words = tokenizer.sequences_to_texts([[x] for x in top_10_idx])\n",
    "\n",
    "        for prob, word in zip(top_10_probs, top_10_words):\n",
    "            print('{:<6.1%} : {}'.format(prob, word))\n",
    "\n",
    "        chosen_word = input()\n",
    "\n",
    "        if chosen_word == '|':\n",
    "            break\n",
    "\n",
    "        seed_text += chosen_word + ' '\n",
    "        output_text += chosen_word + ' '\n",
    "\n",
    "        clear_output()\n",
    "\n",
    "        print(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_human_led_text(model, 20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('TensorFlow')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d71d38bd0d71aa8fb096966ce492050b4e1d8055a06fdbaefbf5b2c66243d19c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
