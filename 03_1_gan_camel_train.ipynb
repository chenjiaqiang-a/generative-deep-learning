{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAN训练\n",
    "## 引入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from models import GAN\n",
    "from utils import load_safari"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 环境参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run params\n",
    "SECTION = \"gan\"\n",
    "RUN_ID = \"0001\"\n",
    "DATA_NAME = \"camel\"\n",
    "RUN_FOLDER = f\"run/{SECTION}/\"\n",
    "RUN_FOLDER += \"_\".join([RUN_ID, DATA_NAME])\n",
    "\n",
    "if not os.path.exists(RUN_FOLDER):\n",
    "    os.makedirs(RUN_FOLDER)\n",
    "    os.makedirs(os.path.join(RUN_FOLDER, \"viz\"))\n",
    "    os.makedirs(os.path.join(RUN_FOLDER, \"images\"))\n",
    "    os.makedirs(os.path.join(RUN_FOLDER, \"weights\"))\n",
    "\n",
    "MODE = \"build\" # \"load\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加载数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80000, 28, 28, 1)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(x_train, y_train) = load_safari(DATA_NAME)\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1b717eb3a60>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x_train[200, :, :, 0], cmap=\"Greys\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 神经网络结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan = GAN(\n",
    "    input_dim=(28, 28, 1),\n",
    "    discriminator_conv_filters=[64, 64, 128, 128],\n",
    "    discriminator_conv_kernel_size=[5, 5, 5, 5],\n",
    "    discriminator_conv_strides=[2, 2, 2, 1],\n",
    "    discriminator_batch_norm_momentum=None,\n",
    "    discriminator_activation=\"relu\",\n",
    "    discriminator_dropout_rate=0.4,\n",
    "    discriminator_learning_rate=0.0008,\n",
    "    generator_initial_dense_layer_size=(7, 7, 64),\n",
    "    generator_upsample=[2, 2, 1, 1],\n",
    "    generator_conv_filters=[128, 64, 64, 1],\n",
    "    generator_conv_kernel_size=[5, 5, 5, 5],\n",
    "    generator_conv_strides=[1, 1, 1, 1],\n",
    "    generator_batch_norm_momentum=0.9,\n",
    "    generator_activation=\"relu\",\n",
    "    generator_dropout_rate=None,\n",
    "    generator_learning_rate=0.0004,\n",
    "    optimizer=\"rmsprop\",\n",
    "    z_dim=100\n",
    ")\n",
    "\n",
    "if MODE == \"build\":\n",
    "    gan.save(RUN_FOLDER)\n",
    "else:\n",
    "    gan.load_weights(os.path.join(RUN_FOLDER, \"weights/weights.h5\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "discriminator_input (InputLa [(None, 28, 28, 1)]       0         \n",
      "_________________________________________________________________\n",
      "discriminator_conv_0 (Conv2D (None, 14, 14, 64)        1664      \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "discriminator_conv_1 (Conv2D (None, 7, 7, 64)          102464    \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "discriminator_conv_2 (Conv2D (None, 4, 4, 128)         204928    \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "discriminator_conv_3 (Conv2D (None, 4, 4, 128)         409728    \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 2049      \n",
      "=================================================================\n",
      "Total params: 720,833\n",
      "Trainable params: 720,833\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "gan.discriminator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "generator_input (InputLayer) [(None, 100)]             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 3136)              316736    \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 3136)              12544     \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 3136)              0         \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d (UpSampling2D) (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "generator_conv_0 (Conv2D)    (None, 14, 14, 128)       204928    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 14, 14, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2 (None, 28, 28, 128)       0         \n",
      "_________________________________________________________________\n",
      "generator_conv_1 (Conv2D)    (None, 28, 28, 64)        204864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 28, 28, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 28, 28, 64)        0         \n",
      "_________________________________________________________________\n",
      "generator_conv_2 (Conv2DTran (None, 28, 28, 64)        102464    \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 28, 28, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 28, 28, 64)        0         \n",
      "_________________________________________________________________\n",
      "generator_conv_3 (Conv2DTran (None, 28, 28, 1)         1601      \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 28, 28, 1)         0         \n",
      "=================================================================\n",
      "Total params: 844,161\n",
      "Trainable params: 837,377\n",
      "Non-trainable params: 6,784\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "gan.generator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "EPOCHS = 6000\n",
    "PRINT_EVERY_N_BATCHES = 5\n",
    "\n",
    "gan.train(\n",
    "    x_train,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    run_folder=RUN_FOLDER,\n",
    "    print_every_n_batches=PRINT_EVERY_N_BATCHES\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练损失可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "plt.plot([x[0] for x in gan.d_losses], color=\"black\", linewidth=0.25)\n",
    "plt.plot([x[1] for x in gan.d_losses], color=\"green\", linewidth=0.25)\n",
    "plt.plot([x[2] for x in gan.d_losses], color=\"red\", linewidth=0.25)\n",
    "plt.plot([x[0] for x in gan.g_losses], color=\"orange\", linewidth=0.25)\n",
    "\n",
    "plt.xlabel(\"batch\", fontsize=18)\n",
    "plt.ylabel(\"loss\", fontsize=16)\n",
    "plt.xlim(0, 2000)\n",
    "plt.ylim(0, 2)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "plt.plot([x[3] for x in gan.d_losses], color=\"black\", linewidth=0.25)\n",
    "plt.plot([x[4] for x in gan.d_losses], color=\"green\", linewidth=0.25)\n",
    "plt.plot([x[5] for x in gan.d_losses], color=\"red\", linewidth=0.25)\n",
    "plt.plot([x[1] for x in gan.g_losses], color=\"orange\", linewidth=0.25)\n",
    "\n",
    "plt.xlabel(\"batch\", fontsize=18)\n",
    "plt.ylabel(\"accuracy\", fontsize=16)\n",
    "plt.xlim(0, 2000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 生成图片及对比与生成图片相似的原始图片"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_images(img1, img2):\n",
    "    return np.mean(np.abs(img1 - img2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r, c = 5, 5\n",
    "\n",
    "idx = np.random.randint(0, x_train.shape[0], 32)\n",
    "true_imgs = (x_train[idx] + 1) * 0.5\n",
    "\n",
    "fig, axs = plt.subplots(r, c, figsize=(15, 15))\n",
    "cnt = 0\n",
    "for i in range(r):\n",
    "    for j in range(c):\n",
    "        axs[i, j].imshow(true_imgs[cnt], cmap=\"Greys\")\n",
    "        axs[i, j].axis(\"off\")\n",
    "        cnt += 1\n",
    "fig.savefig(os.path.join(RUN_FOLDER, \"images/real.png\"))\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r, c = 5, 5\n",
    "noise = np.random.normal(0, 1, (r * c, gan.z_dim))\n",
    "gen_imgs = gan.generator.predict(noise)\n",
    "gen_imgs = 0.5 * (gen_imgs + 1)\n",
    "\n",
    "fig, axs = plt.subplots(r, c, figsize=(15, 15))\n",
    "cnt = 0\n",
    "for i in range(r):\n",
    "    for j in range(c):\n",
    "        axs[i, j].imshow(np.squeeze(gen_imgs[cnt, :, :, :]), cmap=\"Greys\")\n",
    "        axs[i, j].axis(\"off\")\n",
    "        cnt += 1\n",
    "fig.savefig(os.path.join(RUN_FOLDER, \"images/sample.png\"))\n",
    "plt.close()\n",
    "\n",
    "fig, axs = plt.subplots(r, c, figsize=(15, 15))\n",
    "cnt = 0\n",
    "for i in range(r):\n",
    "    for j in range(c):\n",
    "        c_diff = 99999\n",
    "        c_img = None\n",
    "        for k_idx, k in enumerate((x_train + 1) * 0.5):\n",
    "            diff = compare_images(gen_imgs[cnt, :, :, :], k)\n",
    "            if diff < c_diff:\n",
    "                c_img = np.copy(k)\n",
    "                c_diff = diff\n",
    "        axs[i, j].imshow(c_img, cmap=\"Greys\")\n",
    "        axs[i, j].axis(\"off\")\n",
    "        cnt += 1\n",
    "fig.savefig(os.path.join(RUN_FOLDER, \"images/sample_closest.png\"))\n",
    "plt.close()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "11608614dc3cb499a903f65b2116b9f07b688ea96b708ded1bd554c90e775c68"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('PyTorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
