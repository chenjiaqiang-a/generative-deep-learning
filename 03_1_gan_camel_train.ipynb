{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAN训练\n",
    "## 引入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from models import GAN\n",
    "from utils import load_safari\n",
    "\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 环境参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run params\n",
    "SECTION = \"gan\"\n",
    "RUN_ID = \"0001\"\n",
    "DATA_NAME = \"camel\"\n",
    "RUN_FOLDER = f\"run/{SECTION}/\"\n",
    "RUN_FOLDER += \"_\".join([RUN_ID, DATA_NAME])\n",
    "\n",
    "if not os.path.exists(RUN_FOLDER):\n",
    "    os.makedirs(RUN_FOLDER)\n",
    "    os.makedirs(os.path.join(RUN_FOLDER, \"viz\"))\n",
    "    os.makedirs(os.path.join(RUN_FOLDER, \"images\"))\n",
    "    os.makedirs(os.path.join(RUN_FOLDER, \"weights\"))\n",
    "\n",
    "MODE = \"build\" # \"load\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加载数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80000, 28, 28, 1)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(x_train, y_train) = load_safari(DATA_NAME)\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f6187ea9dd0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x_train[200, :, :, 0], cmap=\"Greys\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 神经网络结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /environment/miniconda3/lib/python3.7/site-packages/keras/layers/normalization/batch_normalization.py:532: _colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "gan = GAN(\n",
    "    input_dim=(28, 28, 1),\n",
    "    discriminator_conv_filters=[64, 64, 128, 128],\n",
    "    discriminator_conv_kernel_size=[5, 5, 5, 5],\n",
    "    discriminator_conv_strides=[2, 2, 2, 1],\n",
    "    discriminator_batch_norm_momentum=None,\n",
    "    discriminator_activation=\"relu\",\n",
    "    discriminator_dropout_rate=0.4,\n",
    "    discriminator_learning_rate=0.0008,\n",
    "    generator_initial_dense_layer_size=(7, 7, 64),\n",
    "    generator_upsample=[2, 2, 1, 1],\n",
    "    generator_conv_filters=[128, 64, 64, 1],\n",
    "    generator_conv_kernel_size=[5, 5, 5, 5],\n",
    "    generator_conv_strides=[1, 1, 1, 1],\n",
    "    generator_batch_norm_momentum=0.9,\n",
    "    generator_activation=\"relu\",\n",
    "    generator_dropout_rate=None,\n",
    "    generator_learning_rate=0.0004,\n",
    "    optimizer=\"rmsprop\",\n",
    "    z_dim=100\n",
    ")\n",
    "\n",
    "if MODE == \"build\":\n",
    "    gan.save(RUN_FOLDER)\n",
    "else:\n",
    "    gan.load_weights(os.path.join(RUN_FOLDER, \"weights/weights.h5\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " discriminator_input (InputL  [(None, 28, 28, 1)]      0         \n",
      " ayer)                                                           \n",
      "                                                                 \n",
      " discriminator_conv_0 (Conv2  (None, 14, 14, 64)       1664      \n",
      " D)                                                              \n",
      "                                                                 \n",
      " activation (Activation)     (None, 14, 14, 64)        0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 14, 14, 64)        0         \n",
      "                                                                 \n",
      " discriminator_conv_1 (Conv2  (None, 7, 7, 64)         102464    \n",
      " D)                                                              \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 7, 7, 64)          0         \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 7, 7, 64)          0         \n",
      "                                                                 \n",
      " discriminator_conv_2 (Conv2  (None, 4, 4, 128)        204928    \n",
      " D)                                                              \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, 4, 4, 128)         0         \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 4, 4, 128)         0         \n",
      "                                                                 \n",
      " discriminator_conv_3 (Conv2  (None, 4, 4, 128)        409728    \n",
      " D)                                                              \n",
      "                                                                 \n",
      " activation_3 (Activation)   (None, 4, 4, 128)         0         \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 4, 4, 128)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 2048)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 2049      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 720,833\n",
      "Trainable params: 720,833\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "gan.discriminator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " generator_input (InputLayer  [(None, 100)]            0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 3136)              316736    \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 3136)             12544     \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " activation_4 (Activation)   (None, 3136)              0         \n",
      "                                                                 \n",
      " reshape (Reshape)           (None, 7, 7, 64)          0         \n",
      "                                                                 \n",
      " up_sampling2d (UpSampling2D  (None, 14, 14, 64)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " generator_conv_0 (Conv2D)   (None, 14, 14, 128)       204928    \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 14, 14, 128)      512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_5 (Activation)   (None, 14, 14, 128)       0         \n",
      "                                                                 \n",
      " up_sampling2d_1 (UpSampling  (None, 28, 28, 128)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " generator_conv_1 (Conv2D)   (None, 28, 28, 64)        204864    \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 28, 28, 64)       256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_6 (Activation)   (None, 28, 28, 64)        0         \n",
      "                                                                 \n",
      " generator_conv_2 (Conv2DTra  (None, 28, 28, 64)       102464    \n",
      " nspose)                                                         \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 28, 28, 64)       256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_7 (Activation)   (None, 28, 28, 64)        0         \n",
      "                                                                 \n",
      " generator_conv_3 (Conv2DTra  (None, 28, 28, 1)        1601      \n",
      " nspose)                                                         \n",
      "                                                                 \n",
      " activation_8 (Activation)   (None, 28, 28, 1)         0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 844,161\n",
      "Trainable params: 837,377\n",
      "Non-trainable params: 6,784\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "gan.generator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/environment/miniconda3/lib/python3.7/site-packages/keras/engine/training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "0 [D loss: 0.732(R 0.691, F0.773)] [D acc: 0.297(R 0.594, F 0.000)] [G loss: 0.732] [G acc: 1.000]\n",
      "1 [D loss: 0.711(R 0.649, F0.773)] [D acc: 0.500(R 1.000, F 0.000)] [G loss: 0.711] [G acc: 1.000]\n",
      "2 [D loss: 0.597(R 0.482, F0.713)] [D acc: 0.500(R 1.000, F 0.000)] [G loss: 0.597] [G acc: 1.000]\n",
      "3 [D loss: 0.435(R 0.001, F0.869)] [D acc: 0.500(R 1.000, F 0.000)] [G loss: 0.435] [G acc: 1.000]\n",
      "4 [D loss: 0.524(R 0.138, F0.910)] [D acc: 0.500(R 1.000, F 0.000)] [G loss: 0.524] [G acc: 0.938]\n",
      "5 [D loss: 3.233(R 0.056, F6.411)] [D acc: 0.500(R 1.000, F 0.000)] [G loss: 3.233] [G acc: 0.984]\n",
      "6 [D loss: 0.693(R 0.553, F0.834)] [D acc: 0.500(R 1.000, F 0.000)] [G loss: 0.693] [G acc: 1.000]\n",
      "7 [D loss: 0.672(R 0.550, F0.795)] [D acc: 0.500(R 1.000, F 0.000)] [G loss: 0.672] [G acc: 1.000]\n",
      "8 [D loss: 0.666(R 0.520, F0.812)] [D acc: 0.500(R 1.000, F 0.000)] [G loss: 0.666] [G acc: 1.000]\n",
      "9 [D loss: 0.670(R 0.524, F0.816)] [D acc: 0.500(R 1.000, F 0.000)] [G loss: 0.670] [G acc: 1.000]\n",
      "10 [D loss: 0.665(R 0.510, F0.820)] [D acc: 0.500(R 1.000, F 0.000)] [G loss: 0.665] [G acc: 1.000]\n",
      "11 [D loss: 0.666(R 0.500, F0.832)] [D acc: 0.500(R 1.000, F 0.000)] [G loss: 0.666] [G acc: 1.000]\n",
      "12 [D loss: 0.667(R 0.497, F0.836)] [D acc: 0.500(R 1.000, F 0.000)] [G loss: 0.667] [G acc: 1.000]\n",
      "13 [D loss: 0.676(R 0.486, F0.866)] [D acc: 0.500(R 1.000, F 0.000)] [G loss: 0.676] [G acc: 1.000]\n",
      "14 [D loss: 0.681(R 0.479, F0.882)] [D acc: 0.500(R 1.000, F 0.000)] [G loss: 0.681] [G acc: 1.000]\n",
      "15 [D loss: 0.685(R 0.497, F0.873)] [D acc: 0.500(R 1.000, F 0.000)] [G loss: 0.685] [G acc: 1.000]\n",
      "16 [D loss: 0.697(R 0.510, F0.883)] [D acc: 0.500(R 1.000, F 0.000)] [G loss: 0.697] [G acc: 1.000]\n",
      "17 [D loss: 0.689(R 0.523, F0.854)] [D acc: 0.500(R 1.000, F 0.000)] [G loss: 0.689] [G acc: 1.000]\n",
      "18 [D loss: 0.686(R 0.540, F0.833)] [D acc: 0.500(R 1.000, F 0.000)] [G loss: 0.686] [G acc: 1.000]\n",
      "19 [D loss: 0.673(R 0.537, F0.810)] [D acc: 0.500(R 1.000, F 0.000)] [G loss: 0.673] [G acc: 1.000]\n",
      "20 [D loss: 0.649(R 0.525, F0.773)] [D acc: 0.500(R 1.000, F 0.000)] [G loss: 0.649] [G acc: 1.000]\n",
      "21 [D loss: 0.630(R 0.495, F0.766)] [D acc: 0.500(R 1.000, F 0.000)] [G loss: 0.630] [G acc: 0.469]\n",
      "22 [D loss: 0.585(R 0.444, F0.726)] [D acc: 0.500(R 1.000, F 0.000)] [G loss: 0.585] [G acc: 0.000]\n",
      "23 [D loss: 0.532(R 0.326, F0.738)] [D acc: 0.523(R 1.000, F 0.047)] [G loss: 0.532] [G acc: 0.000]\n",
      "24 [D loss: 0.451(R 0.236, F0.667)] [D acc: 0.984(R 1.000, F 0.969)] [G loss: 0.451] [G acc: 0.188]\n",
      "25 [D loss: 0.676(R 0.046, F1.306)] [D acc: 0.500(R 1.000, F 0.000)] [G loss: 0.676] [G acc: 0.797]\n",
      "26 [D loss: 1.685(R 0.257, F3.113)] [D acc: 0.500(R 1.000, F 0.000)] [G loss: 1.685] [G acc: 0.875]\n",
      "27 [D loss: 0.621(R 0.487, F0.754)] [D acc: 0.500(R 1.000, F 0.000)] [G loss: 0.621] [G acc: 0.906]\n",
      "28 [D loss: 0.613(R 0.439, F0.786)] [D acc: 0.508(R 1.000, F 0.016)] [G loss: 0.613] [G acc: 0.875]\n",
      "29 [D loss: 0.610(R 0.408, F0.811)] [D acc: 0.500(R 1.000, F 0.000)] [G loss: 0.610] [G acc: 0.953]\n",
      "30 [D loss: 0.545(R 0.411, F0.679)] [D acc: 0.852(R 1.000, F 0.703)] [G loss: 0.545] [G acc: 0.969]\n",
      "31 [D loss: 0.428(R 0.330, F0.527)] [D acc: 1.000(R 1.000, F 1.000)] [G loss: 0.428] [G acc: 0.984]\n",
      "32 [D loss: 0.245(R 0.221, F0.269)] [D acc: 0.992(R 0.984, F 1.000)] [G loss: 0.245] [G acc: 1.000]\n",
      "33 [D loss: 0.423(R 0.465, F0.381)] [D acc: 0.898(R 0.797, F 1.000)] [G loss: 0.423] [G acc: 1.000]\n",
      "34 [D loss: 0.080(R 0.084, F0.076)] [D acc: 1.000(R 1.000, F 1.000)] [G loss: 0.080] [G acc: 1.000]\n",
      "35 [D loss: 0.027(R 0.036, F0.018)] [D acc: 1.000(R 1.000, F 1.000)] [G loss: 0.027] [G acc: 1.000]\n",
      "36 [D loss: 0.023(R 0.033, F0.013)] [D acc: 0.992(R 0.984, F 1.000)] [G loss: 0.023] [G acc: 1.000]\n",
      "37 [D loss: 0.018(R 0.010, F0.026)] [D acc: 1.000(R 1.000, F 1.000)] [G loss: 0.018] [G acc: 1.000]\n",
      "38 [D loss: 0.119(R 0.012, F0.226)] [D acc: 0.977(R 1.000, F 0.953)] [G loss: 0.119] [G acc: 1.000]\n",
      "39 [D loss: 0.305(R 0.141, F0.469)] [D acc: 0.891(R 0.953, F 0.828)] [G loss: 0.305] [G acc: 1.000]\n",
      "40 [D loss: 0.274(R 0.258, F0.290)] [D acc: 0.898(R 0.906, F 0.891)] [G loss: 0.274] [G acc: 1.000]\n",
      "41 [D loss: 0.134(R 0.061, F0.206)] [D acc: 0.938(R 1.000, F 0.875)] [G loss: 0.134] [G acc: 1.000]\n",
      "42 [D loss: 0.309(R 0.175, F0.443)] [D acc: 0.891(R 0.953, F 0.828)] [G loss: 0.309] [G acc: 1.000]\n",
      "43 [D loss: 0.648(R 0.271, F1.025)] [D acc: 0.719(R 0.906, F 0.531)] [G loss: 0.648] [G acc: 1.000]\n",
      "44 [D loss: 0.423(R 0.487, F0.359)] [D acc: 0.828(R 0.828, F 0.828)] [G loss: 0.423] [G acc: 1.000]\n",
      "45 [D loss: 0.399(R 0.213, F0.586)] [D acc: 0.812(R 0.969, F 0.656)] [G loss: 0.399] [G acc: 1.000]\n",
      "46 [D loss: 0.617(R 0.581, F0.653)] [D acc: 0.734(R 0.828, F 0.641)] [G loss: 0.617] [G acc: 1.000]\n",
      "47 [D loss: 0.466(R 0.279, F0.652)] [D acc: 0.781(R 0.984, F 0.578)] [G loss: 0.466] [G acc: 1.000]\n",
      "48 [D loss: 0.731(R 0.364, F1.099)] [D acc: 0.617(R 0.938, F 0.297)] [G loss: 0.731] [G acc: 1.000]\n",
      "49 [D loss: 1.146(R 0.599, F1.694)] [D acc: 0.375(R 0.750, F 0.000)] [G loss: 1.146] [G acc: 0.359]\n",
      "50 [D loss: 0.663(R 0.601, F0.726)] [D acc: 0.547(R 0.969, F 0.125)] [G loss: 0.663] [G acc: 0.594]\n",
      "51 [D loss: 0.442(R 0.584, F0.300)] [D acc: 0.961(R 0.969, F 0.953)] [G loss: 0.442] [G acc: 0.922]\n",
      "52 [D loss: 0.480(R 0.633, F0.327)] [D acc: 0.820(R 0.781, F 0.859)] [G loss: 0.480] [G acc: 1.000]\n",
      "53 [D loss: 0.664(R 0.484, F0.843)] [D acc: 0.602(R 0.875, F 0.328)] [G loss: 0.664] [G acc: 0.906]\n",
      "54 [D loss: 0.547(R 0.517, F0.577)] [D acc: 0.734(R 0.875, F 0.594)] [G loss: 0.547] [G acc: 0.953]\n",
      "55 [D loss: 0.773(R 0.523, F1.023)] [D acc: 0.422(R 0.844, F 0.000)] [G loss: 0.773] [G acc: 0.000]\n",
      "56 [D loss: 0.621(R 0.524, F0.717)] [D acc: 0.648(R 0.953, F 0.344)] [G loss: 0.621] [G acc: 0.000]\n",
      "57 [D loss: 0.671(R 0.446, F0.897)] [D acc: 0.477(R 0.938, F 0.016)] [G loss: 0.671] [G acc: 0.000]\n",
      "58 [D loss: 0.621(R 0.526, F0.715)] [D acc: 0.641(R 0.891, F 0.391)] [G loss: 0.621] [G acc: 0.000]\n",
      "59 [D loss: 0.567(R 0.450, F0.685)] [D acc: 0.734(R 0.875, F 0.594)] [G loss: 0.567] [G acc: 0.016]\n",
      "60 [D loss: 0.592(R 0.380, F0.804)] [D acc: 0.547(R 0.906, F 0.188)] [G loss: 0.592] [G acc: 0.031]\n",
      "61 [D loss: 0.313(R 0.452, F0.174)] [D acc: 0.906(R 0.859, F 0.953)] [G loss: 0.313] [G acc: 0.062]\n",
      "62 [D loss: 0.708(R 0.432, F0.985)] [D acc: 0.500(R 0.891, F 0.109)] [G loss: 0.708] [G acc: 0.000]\n",
      "63 [D loss: 0.698(R 0.707, F0.689)] [D acc: 0.477(R 0.406, F 0.547)] [G loss: 0.698] [G acc: 0.000]\n",
      "64 [D loss: 0.644(R 0.525, F0.763)] [D acc: 0.641(R 0.938, F 0.344)] [G loss: 0.644] [G acc: 0.000]\n",
      "65 [D loss: 0.589(R 0.465, F0.712)] [D acc: 0.727(R 0.938, F 0.516)] [G loss: 0.589] [G acc: 0.000]\n",
      "66 [D loss: 0.589(R 0.412, F0.767)] [D acc: 0.672(R 0.938, F 0.406)] [G loss: 0.589] [G acc: 0.000]\n",
      "67 [D loss: 0.547(R 0.416, F0.677)] [D acc: 0.836(R 0.953, F 0.719)] [G loss: 0.547] [G acc: 0.000]\n",
      "68 [D loss: 0.552(R 0.301, F0.802)] [D acc: 0.688(R 0.953, F 0.422)] [G loss: 0.552] [G acc: 0.000]\n",
      "69 [D loss: 0.653(R 0.603, F0.702)] [D acc: 0.617(R 0.641, F 0.594)] [G loss: 0.653] [G acc: 0.000]\n",
      "70 [D loss: 0.525(R 0.297, F0.753)] [D acc: 0.742(R 0.984, F 0.500)] [G loss: 0.525] [G acc: 0.000]\n",
      "71 [D loss: 0.509(R 0.387, F0.630)] [D acc: 0.805(R 0.906, F 0.703)] [G loss: 0.509] [G acc: 0.000]\n",
      "72 [D loss: 0.356(R 0.170, F0.542)] [D acc: 0.883(R 0.984, F 0.781)] [G loss: 0.356] [G acc: 0.000]\n",
      "73 [D loss: 0.197(R 0.102, F0.292)] [D acc: 0.984(R 0.984, F 0.984)] [G loss: 0.197] [G acc: 0.000]\n",
      "74 [D loss: 0.186(R 0.090, F0.281)] [D acc: 0.945(R 0.969, F 0.922)] [G loss: 0.186] [G acc: 0.000]\n",
      "75 [D loss: 1.027(R 0.256, F1.797)] [D acc: 0.578(R 0.922, F 0.234)] [G loss: 1.027] [G acc: 0.016]\n",
      "76 [D loss: 0.543(R 0.566, F0.520)] [D acc: 0.797(R 0.781, F 0.812)] [G loss: 0.543] [G acc: 0.047]\n",
      "77 [D loss: 0.319(R 0.253, F0.385)] [D acc: 0.906(R 0.969, F 0.844)] [G loss: 0.319] [G acc: 0.016]\n",
      "78 [D loss: 0.312(R 0.259, F0.365)] [D acc: 0.891(R 0.953, F 0.828)] [G loss: 0.312] [G acc: 0.000]\n",
      "79 [D loss: 0.418(R 0.681, F0.154)] [D acc: 0.812(R 0.625, F 1.000)] [G loss: 0.418] [G acc: 0.578]\n",
      "80 [D loss: 0.630(R 0.055, F1.205)] [D acc: 0.656(R 0.969, F 0.344)] [G loss: 0.630] [G acc: 0.000]\n",
      "81 [D loss: 0.952(R 1.197, F0.708)] [D acc: 0.383(R 0.297, F 0.469)] [G loss: 0.952] [G acc: 0.062]\n",
      "82 [D loss: 0.576(R 0.380, F0.771)] [D acc: 0.695(R 0.891, F 0.500)] [G loss: 0.576] [G acc: 0.016]\n",
      "83 [D loss: 0.623(R 0.495, F0.751)] [D acc: 0.664(R 0.797, F 0.531)] [G loss: 0.623] [G acc: 0.031]\n",
      "84 [D loss: 0.652(R 0.555, F0.750)] [D acc: 0.555(R 0.594, F 0.516)] [G loss: 0.652] [G acc: 0.203]\n",
      "85 [D loss: 1.225(R 0.729, F1.721)] [D acc: 0.336(R 0.641, F 0.031)] [G loss: 1.225] [G acc: 0.297]\n",
      "86 [D loss: 0.727(R 0.605, F0.848)] [D acc: 0.422(R 0.688, F 0.156)] [G loss: 0.727] [G acc: 0.062]\n",
      "87 [D loss: 0.613(R 0.592, F0.635)] [D acc: 0.719(R 0.719, F 0.719)] [G loss: 0.613] [G acc: 0.016]\n",
      "88 [D loss: 0.628(R 0.556, F0.700)] [D acc: 0.672(R 0.766, F 0.578)] [G loss: 0.628] [G acc: 0.031]\n",
      "89 [D loss: 0.618(R 0.581, F0.656)] [D acc: 0.648(R 0.656, F 0.641)] [G loss: 0.618] [G acc: 0.078]\n",
      "90 [D loss: 0.650(R 0.609, F0.691)] [D acc: 0.617(R 0.719, F 0.516)] [G loss: 0.650] [G acc: 0.062]\n",
      "91 [D loss: 0.591(R 0.464, F0.718)] [D acc: 0.656(R 0.844, F 0.469)] [G loss: 0.591] [G acc: 0.031]\n",
      "92 [D loss: 0.641(R 0.526, F0.755)] [D acc: 0.633(R 0.781, F 0.484)] [G loss: 0.641] [G acc: 0.000]\n",
      "93 [D loss: 0.668(R 0.631, F0.705)] [D acc: 0.562(R 0.625, F 0.500)] [G loss: 0.668] [G acc: 0.000]\n",
      "94 [D loss: 0.592(R 0.474, F0.710)] [D acc: 0.711(R 0.875, F 0.547)] [G loss: 0.592] [G acc: 0.000]\n",
      "95 [D loss: 0.590(R 0.527, F0.653)] [D acc: 0.734(R 0.844, F 0.625)] [G loss: 0.590] [G acc: 0.000]\n",
      "96 [D loss: 0.506(R 0.414, F0.599)] [D acc: 0.797(R 0.844, F 0.750)] [G loss: 0.506] [G acc: 0.000]\n",
      "97 [D loss: 0.423(R 0.345, F0.501)] [D acc: 0.883(R 0.906, F 0.859)] [G loss: 0.423] [G acc: 0.000]\n",
      "98 [D loss: 0.640(R 0.648, F0.632)] [D acc: 0.695(R 0.719, F 0.672)] [G loss: 0.640] [G acc: 0.000]\n",
      "99 [D loss: 0.407(R 0.319, F0.495)] [D acc: 0.906(R 0.875, F 0.938)] [G loss: 0.407] [G acc: 0.000]\n",
      "INFO:tensorflow:Assets written to: ram://d273a9ac-16ad-4055-aaeb-86aa9c26351d/assets\n",
      "INFO:tensorflow:Assets written to: ram://7777c782-abd5-4b83-b437-dd625ff2e8f5/assets\n",
      "INFO:tensorflow:Assets written to: ram://e3789d29-7d9f-48e0-ae21-ceaeeb8b7ed7/assets\n",
      "100 [D loss: 0.267(R 0.218, F0.315)] [D acc: 0.938(R 0.938, F 0.938)] [G loss: 0.267] [G acc: 0.000]\n",
      "101 [D loss: 0.279(R 0.151, F0.406)] [D acc: 0.922(R 0.953, F 0.891)] [G loss: 0.279] [G acc: 0.000]\n",
      "102 [D loss: 0.439(R 0.419, F0.458)] [D acc: 0.828(R 0.797, F 0.859)] [G loss: 0.439] [G acc: 0.016]\n",
      "103 [D loss: 0.148(R 0.110, F0.186)] [D acc: 0.969(R 0.938, F 1.000)] [G loss: 0.148] [G acc: 0.000]\n",
      "104 [D loss: 0.126(R 0.081, F0.171)] [D acc: 0.969(R 0.969, F 0.969)] [G loss: 0.126] [G acc: 0.047]\n",
      "105 [D loss: 1.953(R 0.170, F3.737)] [D acc: 0.523(R 0.938, F 0.109)] [G loss: 1.953] [G acc: 0.000]\n",
      "106 [D loss: 1.101(R 1.404, F0.798)] [D acc: 0.359(R 0.141, F 0.578)] [G loss: 1.101] [G acc: 0.031]\n",
      "107 [D loss: 0.388(R 0.429, F0.346)] [D acc: 0.875(R 0.812, F 0.938)] [G loss: 0.388] [G acc: 0.016]\n",
      "108 [D loss: 0.403(R 0.329, F0.477)] [D acc: 0.891(R 0.906, F 0.875)] [G loss: 0.403] [G acc: 0.000]\n",
      "109 [D loss: 0.374(R 0.240, F0.508)] [D acc: 0.914(R 0.906, F 0.922)] [G loss: 0.374] [G acc: 0.312]\n",
      "110 [D loss: 0.351(R 0.170, F0.531)] [D acc: 0.938(R 0.984, F 0.891)] [G loss: 0.351] [G acc: 0.469]\n",
      "111 [D loss: 0.709(R 0.203, F1.214)] [D acc: 0.719(R 0.969, F 0.469)] [G loss: 0.709] [G acc: 0.406]\n",
      "112 [D loss: 0.518(R 0.316, F0.720)] [D acc: 0.758(R 0.906, F 0.609)] [G loss: 0.518] [G acc: 0.781]\n",
      "113 [D loss: 0.705(R 0.250, F1.160)] [D acc: 0.688(R 0.953, F 0.422)] [G loss: 0.705] [G acc: 0.672]\n",
      "114 [D loss: 0.710(R 0.366, F1.055)] [D acc: 0.664(R 0.922, F 0.406)] [G loss: 0.710] [G acc: 0.578]\n",
      "115 [D loss: 0.663(R 0.404, F0.923)] [D acc: 0.609(R 0.844, F 0.375)] [G loss: 0.663] [G acc: 0.594]\n",
      "116 [D loss: 0.697(R 0.458, F0.935)] [D acc: 0.625(R 0.906, F 0.344)] [G loss: 0.697] [G acc: 0.438]\n",
      "117 [D loss: 0.620(R 0.455, F0.785)] [D acc: 0.609(R 0.844, F 0.375)] [G loss: 0.620] [G acc: 0.250]\n",
      "118 [D loss: 0.678(R 0.572, F0.784)] [D acc: 0.594(R 0.828, F 0.359)] [G loss: 0.678] [G acc: 0.469]\n",
      "119 [D loss: 0.621(R 0.427, F0.815)] [D acc: 0.633(R 0.875, F 0.391)] [G loss: 0.621] [G acc: 0.141]\n",
      "120 [D loss: 0.639(R 0.473, F0.804)] [D acc: 0.570(R 0.812, F 0.328)] [G loss: 0.639] [G acc: 0.219]\n",
      "121 [D loss: 0.622(R 0.525, F0.719)] [D acc: 0.617(R 0.734, F 0.500)] [G loss: 0.622] [G acc: 0.172]\n",
      "122 [D loss: 0.503(R 0.385, F0.620)] [D acc: 0.750(R 0.891, F 0.609)] [G loss: 0.503] [G acc: 0.125]\n",
      "123 [D loss: 0.654(R 0.544, F0.764)] [D acc: 0.586(R 0.766, F 0.406)] [G loss: 0.654] [G acc: 0.016]\n",
      "124 [D loss: 0.603(R 0.371, F0.836)] [D acc: 0.641(R 0.938, F 0.344)] [G loss: 0.603] [G acc: 0.062]\n",
      "125 [D loss: 0.598(R 0.516, F0.680)] [D acc: 0.742(R 0.891, F 0.594)] [G loss: 0.598] [G acc: 0.141]\n",
      "126 [D loss: 0.545(R 0.378, F0.713)] [D acc: 0.680(R 0.875, F 0.484)] [G loss: 0.545] [G acc: 0.047]\n",
      "127 [D loss: 0.641(R 0.430, F0.852)] [D acc: 0.594(R 0.812, F 0.375)] [G loss: 0.641] [G acc: 0.047]\n",
      "128 [D loss: 0.640(R 0.525, F0.755)] [D acc: 0.617(R 0.812, F 0.422)] [G loss: 0.640] [G acc: 0.031]\n",
      "129 [D loss: 0.737(R 0.669, F0.805)] [D acc: 0.445(R 0.594, F 0.297)] [G loss: 0.737] [G acc: 0.062]\n",
      "130 [D loss: 0.707(R 0.577, F0.837)] [D acc: 0.578(R 0.828, F 0.328)] [G loss: 0.707] [G acc: 0.078]\n",
      "131 [D loss: 0.611(R 0.502, F0.720)] [D acc: 0.672(R 0.875, F 0.469)] [G loss: 0.611] [G acc: 0.078]\n",
      "132 [D loss: 0.687(R 0.560, F0.813)] [D acc: 0.508(R 0.781, F 0.234)] [G loss: 0.687] [G acc: 0.172]\n",
      "133 [D loss: 0.663(R 0.569, F0.758)] [D acc: 0.523(R 0.750, F 0.297)] [G loss: 0.663] [G acc: 0.062]\n",
      "134 [D loss: 0.620(R 0.613, F0.627)] [D acc: 0.625(R 0.672, F 0.578)] [G loss: 0.620] [G acc: 0.328]\n",
      "135 [D loss: 0.724(R 0.523, F0.926)] [D acc: 0.453(R 0.781, F 0.125)] [G loss: 0.724] [G acc: 0.016]\n",
      "136 [D loss: 0.671(R 0.637, F0.705)] [D acc: 0.633(R 0.734, F 0.531)] [G loss: 0.671] [G acc: 0.078]\n",
      "137 [D loss: 0.635(R 0.574, F0.697)] [D acc: 0.664(R 0.781, F 0.547)] [G loss: 0.635] [G acc: 0.141]\n",
      "138 [D loss: 0.717(R 0.597, F0.838)] [D acc: 0.500(R 0.688, F 0.312)] [G loss: 0.717] [G acc: 0.141]\n",
      "139 [D loss: 0.623(R 0.578, F0.668)] [D acc: 0.703(R 0.781, F 0.625)] [G loss: 0.623] [G acc: 0.250]\n",
      "140 [D loss: 0.713(R 0.639, F0.788)] [D acc: 0.547(R 0.750, F 0.344)] [G loss: 0.713] [G acc: 0.297]\n",
      "141 [D loss: 0.694(R 0.583, F0.805)] [D acc: 0.531(R 0.781, F 0.281)] [G loss: 0.694] [G acc: 0.125]\n",
      "142 [D loss: 0.652(R 0.573, F0.730)] [D acc: 0.617(R 0.781, F 0.453)] [G loss: 0.652] [G acc: 0.359]\n",
      "143 [D loss: 0.680(R 0.582, F0.778)] [D acc: 0.523(R 0.688, F 0.359)] [G loss: 0.680] [G acc: 0.188]\n",
      "144 [D loss: 0.642(R 0.626, F0.658)] [D acc: 0.633(R 0.766, F 0.500)] [G loss: 0.642] [G acc: 0.266]\n",
      "145 [D loss: 0.696(R 0.710, F0.682)] [D acc: 0.633(R 0.625, F 0.641)] [G loss: 0.696] [G acc: 0.062]\n",
      "146 [D loss: 0.729(R 0.629, F0.829)] [D acc: 0.484(R 0.656, F 0.312)] [G loss: 0.729] [G acc: 0.125]\n",
      "147 [D loss: 0.667(R 0.585, F0.750)] [D acc: 0.594(R 0.812, F 0.375)] [G loss: 0.667] [G acc: 0.094]\n",
      "148 [D loss: 0.701(R 0.611, F0.790)] [D acc: 0.500(R 0.672, F 0.328)] [G loss: 0.701] [G acc: 0.219]\n",
      "149 [D loss: 0.662(R 0.612, F0.712)] [D acc: 0.609(R 0.766, F 0.453)] [G loss: 0.662] [G acc: 0.203]\n",
      "150 [D loss: 0.706(R 0.629, F0.782)] [D acc: 0.445(R 0.641, F 0.250)] [G loss: 0.706] [G acc: 0.141]\n",
      "151 [D loss: 0.641(R 0.624, F0.658)] [D acc: 0.664(R 0.719, F 0.609)] [G loss: 0.641] [G acc: 0.188]\n",
      "152 [D loss: 0.739(R 0.662, F0.816)] [D acc: 0.469(R 0.641, F 0.297)] [G loss: 0.739] [G acc: 0.016]\n",
      "153 [D loss: 0.648(R 0.674, F0.622)] [D acc: 0.648(R 0.594, F 0.703)] [G loss: 0.648] [G acc: 0.156]\n",
      "154 [D loss: 0.519(R 0.627, F0.411)] [D acc: 0.805(R 0.781, F 0.828)] [G loss: 0.519] [G acc: 0.484]\n",
      "155 [D loss: 0.640(R 0.787, F0.494)] [D acc: 0.766(R 0.578, F 0.953)] [G loss: 0.640] [G acc: 0.016]\n",
      "156 [D loss: 0.881(R 0.433, F1.329)] [D acc: 0.469(R 0.906, F 0.031)] [G loss: 0.881] [G acc: 0.000]\n",
      "157 [D loss: 0.708(R 0.742, F0.673)] [D acc: 0.508(R 0.438, F 0.578)] [G loss: 0.708] [G acc: 0.203]\n",
      "158 [D loss: 0.681(R 0.589, F0.773)] [D acc: 0.562(R 0.781, F 0.344)] [G loss: 0.681] [G acc: 0.172]\n",
      "159 [D loss: 0.630(R 0.584, F0.676)] [D acc: 0.672(R 0.828, F 0.516)] [G loss: 0.630] [G acc: 0.141]\n",
      "160 [D loss: 0.705(R 0.647, F0.762)] [D acc: 0.555(R 0.734, F 0.375)] [G loss: 0.705] [G acc: 0.188]\n",
      "161 [D loss: 0.615(R 0.593, F0.636)] [D acc: 0.688(R 0.797, F 0.578)] [G loss: 0.615] [G acc: 0.219]\n",
      "162 [D loss: 0.747(R 0.689, F0.805)] [D acc: 0.531(R 0.703, F 0.359)] [G loss: 0.747] [G acc: 0.172]\n",
      "163 [D loss: 0.620(R 0.627, F0.613)] [D acc: 0.719(R 0.766, F 0.672)] [G loss: 0.620] [G acc: 0.359]\n",
      "164 [D loss: 0.593(R 0.560, F0.626)] [D acc: 0.641(R 0.797, F 0.484)] [G loss: 0.593] [G acc: 0.422]\n",
      "165 [D loss: 0.742(R 0.655, F0.829)] [D acc: 0.570(R 0.750, F 0.391)] [G loss: 0.742] [G acc: 0.094]\n",
      "166 [D loss: 0.674(R 0.613, F0.735)] [D acc: 0.555(R 0.734, F 0.375)] [G loss: 0.674] [G acc: 0.266]\n",
      "167 [D loss: 0.593(R 0.559, F0.626)] [D acc: 0.766(R 0.906, F 0.625)] [G loss: 0.593] [G acc: 0.375]\n",
      "168 [D loss: 0.651(R 0.590, F0.712)] [D acc: 0.609(R 0.703, F 0.516)] [G loss: 0.651] [G acc: 0.016]\n",
      "169 [D loss: 0.486(R 0.572, F0.401)] [D acc: 0.859(R 0.859, F 0.859)] [G loss: 0.486] [G acc: 0.375]\n",
      "170 [D loss: 0.338(R 0.565, F0.111)] [D acc: 0.883(R 0.766, F 1.000)] [G loss: 0.338] [G acc: 0.047]\n",
      "171 [D loss: 0.953(R 0.306, F1.600)] [D acc: 0.461(R 0.906, F 0.016)] [G loss: 0.953] [G acc: 0.016]\n",
      "172 [D loss: 0.715(R 0.673, F0.757)] [D acc: 0.492(R 0.641, F 0.344)] [G loss: 0.715] [G acc: 0.281]\n",
      "173 [D loss: 0.640(R 0.534, F0.745)] [D acc: 0.664(R 0.875, F 0.453)] [G loss: 0.640] [G acc: 0.156]\n",
      "174 [D loss: 0.676(R 0.575, F0.777)] [D acc: 0.523(R 0.734, F 0.312)] [G loss: 0.676] [G acc: 0.219]\n",
      "175 [D loss: 0.724(R 0.610, F0.838)] [D acc: 0.539(R 0.828, F 0.250)] [G loss: 0.724] [G acc: 0.234]\n",
      "176 [D loss: 0.665(R 0.598, F0.732)] [D acc: 0.547(R 0.766, F 0.328)] [G loss: 0.665] [G acc: 0.156]\n",
      "177 [D loss: 0.599(R 0.563, F0.635)] [D acc: 0.734(R 0.828, F 0.641)] [G loss: 0.599] [G acc: 0.312]\n",
      "178 [D loss: 0.736(R 0.611, F0.861)] [D acc: 0.453(R 0.703, F 0.203)] [G loss: 0.736] [G acc: 0.344]\n",
      "179 [D loss: 0.616(R 0.595, F0.637)] [D acc: 0.711(R 0.812, F 0.609)] [G loss: 0.616] [G acc: 0.391]\n",
      "180 [D loss: 0.558(R 0.574, F0.543)] [D acc: 0.742(R 0.781, F 0.703)] [G loss: 0.558] [G acc: 0.578]\n",
      "181 [D loss: 0.713(R 0.650, F0.776)] [D acc: 0.562(R 0.750, F 0.375)] [G loss: 0.713] [G acc: 0.578]\n",
      "182 [D loss: 0.706(R 0.579, F0.832)] [D acc: 0.555(R 0.875, F 0.234)] [G loss: 0.706] [G acc: 0.531]\n",
      "183 [D loss: 0.736(R 0.639, F0.833)] [D acc: 0.484(R 0.734, F 0.234)] [G loss: 0.736] [G acc: 0.547]\n",
      "184 [D loss: 0.674(R 0.608, F0.739)] [D acc: 0.641(R 0.828, F 0.453)] [G loss: 0.674] [G acc: 0.422]\n",
      "185 [D loss: 0.702(R 0.626, F0.779)] [D acc: 0.547(R 0.797, F 0.297)] [G loss: 0.702] [G acc: 0.312]\n",
      "186 [D loss: 0.668(R 0.599, F0.737)] [D acc: 0.578(R 0.781, F 0.375)] [G loss: 0.668] [G acc: 0.344]\n",
      "187 [D loss: 0.695(R 0.605, F0.785)] [D acc: 0.492(R 0.750, F 0.234)] [G loss: 0.695] [G acc: 0.266]\n",
      "188 [D loss: 0.681(R 0.617, F0.744)] [D acc: 0.555(R 0.766, F 0.344)] [G loss: 0.681] [G acc: 0.188]\n",
      "189 [D loss: 0.675(R 0.615, F0.734)] [D acc: 0.609(R 0.828, F 0.391)] [G loss: 0.675] [G acc: 0.328]\n",
      "190 [D loss: 0.636(R 0.550, F0.722)] [D acc: 0.695(R 0.891, F 0.500)] [G loss: 0.636] [G acc: 0.156]\n",
      "191 [D loss: 0.676(R 0.542, F0.809)] [D acc: 0.539(R 0.844, F 0.234)] [G loss: 0.676] [G acc: 0.203]\n",
      "192 [D loss: 0.665(R 0.565, F0.765)] [D acc: 0.523(R 0.766, F 0.281)] [G loss: 0.665] [G acc: 0.094]\n",
      "193 [D loss: 0.653(R 0.614, F0.692)] [D acc: 0.641(R 0.688, F 0.594)] [G loss: 0.653] [G acc: 0.078]\n",
      "194 [D loss: 0.621(R 0.655, F0.588)] [D acc: 0.703(R 0.609, F 0.797)] [G loss: 0.621] [G acc: 0.031]\n",
      "195 [D loss: 0.530(R 0.607, F0.453)] [D acc: 0.734(R 0.672, F 0.797)] [G loss: 0.530] [G acc: 0.297]\n",
      "196 [D loss: 0.492(R 0.631, F0.354)] [D acc: 0.859(R 0.766, F 0.953)] [G loss: 0.492] [G acc: 0.000]\n",
      "197 [D loss: 0.918(R 0.470, F1.366)] [D acc: 0.484(R 0.906, F 0.062)] [G loss: 0.918] [G acc: 0.109]\n",
      "198 [D loss: 0.684(R 0.618, F0.750)] [D acc: 0.562(R 0.688, F 0.438)] [G loss: 0.684] [G acc: 0.109]\n",
      "199 [D loss: 0.651(R 0.602, F0.700)] [D acc: 0.641(R 0.797, F 0.484)] [G loss: 0.651] [G acc: 0.078]\n",
      "INFO:tensorflow:Assets written to: ram://4d06f3a0-cbe3-4987-ad12-5199d3d0a71f/assets\n",
      "INFO:tensorflow:Assets written to: ram://811f1eca-0f0a-4ace-be96-50eb33e0541f/assets\n",
      "INFO:tensorflow:Assets written to: ram://3609bdab-46c7-4fa5-a78f-49f0d68a22e5/assets\n",
      "200 [D loss: 0.697(R 0.657, F0.738)] [D acc: 0.547(R 0.688, F 0.406)] [G loss: 0.697] [G acc: 0.141]\n",
      "201 [D loss: 0.663(R 0.630, F0.696)] [D acc: 0.641(R 0.719, F 0.562)] [G loss: 0.663] [G acc: 0.188]\n",
      "202 [D loss: 0.681(R 0.632, F0.729)] [D acc: 0.562(R 0.734, F 0.391)] [G loss: 0.681] [G acc: 0.125]\n",
      "203 [D loss: 0.646(R 0.584, F0.708)] [D acc: 0.648(R 0.734, F 0.562)] [G loss: 0.646] [G acc: 0.125]\n",
      "204 [D loss: 0.665(R 0.614, F0.717)] [D acc: 0.625(R 0.719, F 0.531)] [G loss: 0.665] [G acc: 0.219]\n",
      "205 [D loss: 0.597(R 0.553, F0.641)] [D acc: 0.688(R 0.781, F 0.594)] [G loss: 0.597] [G acc: 0.297]\n",
      "206 [D loss: 0.705(R 0.671, F0.739)] [D acc: 0.578(R 0.703, F 0.453)] [G loss: 0.705] [G acc: 0.281]\n",
      "207 [D loss: 0.676(R 0.580, F0.773)] [D acc: 0.562(R 0.766, F 0.359)] [G loss: 0.676] [G acc: 0.172]\n",
      "208 [D loss: 0.692(R 0.628, F0.756)] [D acc: 0.539(R 0.734, F 0.344)] [G loss: 0.692] [G acc: 0.203]\n",
      "209 [D loss: 0.676(R 0.612, F0.740)] [D acc: 0.602(R 0.719, F 0.484)] [G loss: 0.676] [G acc: 0.125]\n",
      "210 [D loss: 0.640(R 0.577, F0.703)] [D acc: 0.594(R 0.672, F 0.516)] [G loss: 0.640] [G acc: 0.141]\n",
      "211 [D loss: 0.683(R 0.606, F0.759)] [D acc: 0.578(R 0.766, F 0.391)] [G loss: 0.683] [G acc: 0.250]\n",
      "212 [D loss: 0.662(R 0.632, F0.691)] [D acc: 0.609(R 0.703, F 0.516)] [G loss: 0.662] [G acc: 0.109]\n",
      "213 [D loss: 0.700(R 0.630, F0.771)] [D acc: 0.492(R 0.672, F 0.312)] [G loss: 0.700] [G acc: 0.062]\n",
      "214 [D loss: 0.652(R 0.628, F0.676)] [D acc: 0.625(R 0.625, F 0.625)] [G loss: 0.652] [G acc: 0.141]\n",
      "215 [D loss: 0.674(R 0.642, F0.706)] [D acc: 0.594(R 0.625, F 0.562)] [G loss: 0.674] [G acc: 0.141]\n",
      "216 [D loss: 0.669(R 0.638, F0.700)] [D acc: 0.602(R 0.656, F 0.547)] [G loss: 0.669] [G acc: 0.141]\n",
      "217 [D loss: 0.648(R 0.624, F0.673)] [D acc: 0.648(R 0.672, F 0.625)] [G loss: 0.648] [G acc: 0.109]\n",
      "218 [D loss: 0.695(R 0.649, F0.741)] [D acc: 0.523(R 0.594, F 0.453)] [G loss: 0.695] [G acc: 0.109]\n",
      "219 [D loss: 0.661(R 0.615, F0.708)] [D acc: 0.594(R 0.688, F 0.500)] [G loss: 0.661] [G acc: 0.188]\n",
      "220 [D loss: 0.678(R 0.632, F0.724)] [D acc: 0.578(R 0.594, F 0.562)] [G loss: 0.678] [G acc: 0.078]\n",
      "221 [D loss: 0.664(R 0.637, F0.692)] [D acc: 0.562(R 0.594, F 0.531)] [G loss: 0.664] [G acc: 0.125]\n",
      "222 [D loss: 0.663(R 0.615, F0.711)] [D acc: 0.602(R 0.688, F 0.516)] [G loss: 0.663] [G acc: 0.031]\n",
      "223 [D loss: 0.653(R 0.619, F0.687)] [D acc: 0.594(R 0.625, F 0.562)] [G loss: 0.653] [G acc: 0.062]\n",
      "224 [D loss: 0.693(R 0.652, F0.735)] [D acc: 0.539(R 0.578, F 0.500)] [G loss: 0.693] [G acc: 0.172]\n",
      "225 [D loss: 0.681(R 0.609, F0.754)] [D acc: 0.531(R 0.672, F 0.391)] [G loss: 0.681] [G acc: 0.078]\n",
      "226 [D loss: 0.651(R 0.585, F0.716)] [D acc: 0.625(R 0.750, F 0.500)] [G loss: 0.651] [G acc: 0.156]\n",
      "227 [D loss: 0.673(R 0.645, F0.701)] [D acc: 0.562(R 0.609, F 0.516)] [G loss: 0.673] [G acc: 0.156]\n",
      "228 [D loss: 0.657(R 0.605, F0.709)] [D acc: 0.562(R 0.641, F 0.484)] [G loss: 0.657] [G acc: 0.172]\n",
      "229 [D loss: 0.710(R 0.676, F0.743)] [D acc: 0.461(R 0.484, F 0.438)] [G loss: 0.710] [G acc: 0.062]\n",
      "230 [D loss: 0.661(R 0.661, F0.661)] [D acc: 0.633(R 0.625, F 0.641)] [G loss: 0.661] [G acc: 0.094]\n",
      "231 [D loss: 0.660(R 0.639, F0.681)] [D acc: 0.648(R 0.656, F 0.641)] [G loss: 0.660] [G acc: 0.172]\n",
      "232 [D loss: 0.663(R 0.637, F0.689)] [D acc: 0.617(R 0.688, F 0.547)] [G loss: 0.663] [G acc: 0.141]\n",
      "233 [D loss: 0.662(R 0.643, F0.682)] [D acc: 0.570(R 0.594, F 0.547)] [G loss: 0.662] [G acc: 0.109]\n",
      "234 [D loss: 0.649(R 0.638, F0.661)] [D acc: 0.609(R 0.594, F 0.625)] [G loss: 0.649] [G acc: 0.047]\n",
      "235 [D loss: 0.697(R 0.605, F0.790)] [D acc: 0.609(R 0.781, F 0.438)] [G loss: 0.697] [G acc: 0.188]\n",
      "236 [D loss: 0.686(R 0.621, F0.751)] [D acc: 0.578(R 0.688, F 0.469)] [G loss: 0.686] [G acc: 0.031]\n",
      "237 [D loss: 0.663(R 0.666, F0.660)] [D acc: 0.609(R 0.578, F 0.641)] [G loss: 0.663] [G acc: 0.078]\n",
      "238 [D loss: 0.641(R 0.579, F0.704)] [D acc: 0.711(R 0.781, F 0.641)] [G loss: 0.641] [G acc: 0.109]\n",
      "239 [D loss: 0.682(R 0.702, F0.662)] [D acc: 0.555(R 0.484, F 0.625)] [G loss: 0.682] [G acc: 0.109]\n",
      "240 [D loss: 0.663(R 0.641, F0.685)] [D acc: 0.594(R 0.609, F 0.578)] [G loss: 0.663] [G acc: 0.094]\n",
      "241 [D loss: 0.622(R 0.610, F0.633)] [D acc: 0.672(R 0.609, F 0.734)] [G loss: 0.622] [G acc: 0.172]\n",
      "242 [D loss: 0.675(R 0.563, F0.788)] [D acc: 0.633(R 0.797, F 0.469)] [G loss: 0.675] [G acc: 0.094]\n",
      "243 [D loss: 0.688(R 0.783, F0.593)] [D acc: 0.617(R 0.438, F 0.797)] [G loss: 0.688] [G acc: 0.031]\n",
      "244 [D loss: 0.635(R 0.630, F0.641)] [D acc: 0.664(R 0.656, F 0.672)] [G loss: 0.635] [G acc: 0.047]\n",
      "245 [D loss: 0.686(R 0.628, F0.744)] [D acc: 0.555(R 0.625, F 0.484)] [G loss: 0.686] [G acc: 0.266]\n",
      "246 [D loss: 0.667(R 0.604, F0.730)] [D acc: 0.609(R 0.750, F 0.469)] [G loss: 0.667] [G acc: 0.203]\n",
      "247 [D loss: 0.675(R 0.601, F0.749)] [D acc: 0.578(R 0.719, F 0.438)] [G loss: 0.675] [G acc: 0.172]\n",
      "248 [D loss: 0.686(R 0.659, F0.713)] [D acc: 0.586(R 0.641, F 0.531)] [G loss: 0.686] [G acc: 0.094]\n",
      "249 [D loss: 0.657(R 0.620, F0.695)] [D acc: 0.688(R 0.719, F 0.656)] [G loss: 0.657] [G acc: 0.141]\n",
      "250 [D loss: 0.639(R 0.632, F0.647)] [D acc: 0.656(R 0.656, F 0.656)] [G loss: 0.639] [G acc: 0.141]\n",
      "251 [D loss: 0.634(R 0.611, F0.658)] [D acc: 0.656(R 0.625, F 0.688)] [G loss: 0.634] [G acc: 0.125]\n",
      "252 [D loss: 0.658(R 0.607, F0.709)] [D acc: 0.586(R 0.656, F 0.516)] [G loss: 0.658] [G acc: 0.188]\n",
      "253 [D loss: 0.639(R 0.586, F0.692)] [D acc: 0.609(R 0.703, F 0.516)] [G loss: 0.639] [G acc: 0.141]\n",
      "254 [D loss: 0.633(R 0.629, F0.636)] [D acc: 0.680(R 0.609, F 0.750)] [G loss: 0.633] [G acc: 0.141]\n",
      "255 [D loss: 0.684(R 0.493, F0.875)] [D acc: 0.570(R 0.844, F 0.297)] [G loss: 0.684] [G acc: 0.125]\n",
      "256 [D loss: 0.647(R 0.614, F0.680)] [D acc: 0.617(R 0.609, F 0.625)] [G loss: 0.647] [G acc: 0.203]\n",
      "257 [D loss: 0.696(R 0.645, F0.747)] [D acc: 0.516(R 0.578, F 0.453)] [G loss: 0.696] [G acc: 0.125]\n",
      "258 [D loss: 0.639(R 0.603, F0.674)] [D acc: 0.664(R 0.703, F 0.625)] [G loss: 0.639] [G acc: 0.094]\n",
      "259 [D loss: 0.634(R 0.645, F0.623)] [D acc: 0.664(R 0.578, F 0.750)] [G loss: 0.634] [G acc: 0.078]\n",
      "260 [D loss: 0.635(R 0.547, F0.722)] [D acc: 0.664(R 0.797, F 0.531)] [G loss: 0.635] [G acc: 0.062]\n",
      "261 [D loss: 0.649(R 0.606, F0.691)] [D acc: 0.578(R 0.594, F 0.562)] [G loss: 0.649] [G acc: 0.141]\n",
      "262 [D loss: 0.677(R 0.610, F0.745)] [D acc: 0.578(R 0.641, F 0.516)] [G loss: 0.677] [G acc: 0.047]\n",
      "263 [D loss: 0.630(R 0.633, F0.626)] [D acc: 0.734(R 0.672, F 0.797)] [G loss: 0.630] [G acc: 0.109]\n",
      "264 [D loss: 0.623(R 0.538, F0.707)] [D acc: 0.680(R 0.750, F 0.609)] [G loss: 0.623] [G acc: 0.125]\n",
      "265 [D loss: 0.618(R 0.676, F0.561)] [D acc: 0.758(R 0.594, F 0.922)] [G loss: 0.618] [G acc: 0.031]\n",
      "266 [D loss: 0.674(R 0.548, F0.800)] [D acc: 0.594(R 0.750, F 0.438)] [G loss: 0.674] [G acc: 0.109]\n",
      "267 [D loss: 0.624(R 0.573, F0.675)] [D acc: 0.664(R 0.734, F 0.594)] [G loss: 0.624] [G acc: 0.156]\n",
      "268 [D loss: 0.634(R 0.616, F0.652)] [D acc: 0.625(R 0.609, F 0.641)] [G loss: 0.634] [G acc: 0.016]\n",
      "269 [D loss: 0.616(R 0.525, F0.707)] [D acc: 0.656(R 0.797, F 0.516)] [G loss: 0.616] [G acc: 0.078]\n",
      "270 [D loss: 0.608(R 0.533, F0.682)] [D acc: 0.695(R 0.797, F 0.594)] [G loss: 0.608] [G acc: 0.078]\n",
      "271 [D loss: 0.619(R 0.576, F0.662)] [D acc: 0.641(R 0.672, F 0.609)] [G loss: 0.619] [G acc: 0.062]\n",
      "272 [D loss: 0.628(R 0.650, F0.605)] [D acc: 0.648(R 0.609, F 0.688)] [G loss: 0.628] [G acc: 0.047]\n",
      "273 [D loss: 0.641(R 0.545, F0.737)] [D acc: 0.648(R 0.750, F 0.547)] [G loss: 0.641] [G acc: 0.109]\n",
      "274 [D loss: 0.642(R 0.619, F0.665)] [D acc: 0.656(R 0.641, F 0.672)] [G loss: 0.642] [G acc: 0.047]\n",
      "275 [D loss: 0.598(R 0.581, F0.615)] [D acc: 0.695(R 0.719, F 0.672)] [G loss: 0.598] [G acc: 0.109]\n",
      "276 [D loss: 0.567(R 0.513, F0.622)] [D acc: 0.719(R 0.750, F 0.688)] [G loss: 0.567] [G acc: 0.031]\n",
      "277 [D loss: 0.663(R 0.612, F0.714)] [D acc: 0.594(R 0.625, F 0.562)] [G loss: 0.663] [G acc: 0.062]\n",
      "278 [D loss: 0.595(R 0.643, F0.547)] [D acc: 0.695(R 0.578, F 0.812)] [G loss: 0.595] [G acc: 0.047]\n",
      "279 [D loss: 0.672(R 0.530, F0.814)] [D acc: 0.609(R 0.766, F 0.453)] [G loss: 0.672] [G acc: 0.188]\n",
      "280 [D loss: 0.596(R 0.558, F0.634)] [D acc: 0.680(R 0.719, F 0.641)] [G loss: 0.596] [G acc: 0.109]\n",
      "281 [D loss: 0.586(R 0.546, F0.627)] [D acc: 0.688(R 0.688, F 0.688)] [G loss: 0.586] [G acc: 0.062]\n",
      "282 [D loss: 0.584(R 0.529, F0.640)] [D acc: 0.703(R 0.750, F 0.656)] [G loss: 0.584] [G acc: 0.141]\n",
      "283 [D loss: 0.610(R 0.549, F0.670)] [D acc: 0.664(R 0.750, F 0.578)] [G loss: 0.610] [G acc: 0.109]\n",
      "284 [D loss: 0.668(R 0.657, F0.679)] [D acc: 0.594(R 0.578, F 0.609)] [G loss: 0.668] [G acc: 0.172]\n",
      "285 [D loss: 0.615(R 0.544, F0.685)] [D acc: 0.633(R 0.719, F 0.547)] [G loss: 0.615] [G acc: 0.078]\n",
      "286 [D loss: 0.632(R 0.553, F0.712)] [D acc: 0.664(R 0.781, F 0.547)] [G loss: 0.632] [G acc: 0.125]\n",
      "287 [D loss: 0.618(R 0.652, F0.584)] [D acc: 0.680(R 0.609, F 0.750)] [G loss: 0.618] [G acc: 0.109]\n",
      "288 [D loss: 0.612(R 0.599, F0.625)] [D acc: 0.625(R 0.609, F 0.641)] [G loss: 0.612] [G acc: 0.078]\n",
      "289 [D loss: 0.580(R 0.547, F0.614)] [D acc: 0.695(R 0.703, F 0.688)] [G loss: 0.580] [G acc: 0.031]\n",
      "290 [D loss: 0.533(R 0.491, F0.574)] [D acc: 0.766(R 0.766, F 0.766)] [G loss: 0.533] [G acc: 0.141]\n",
      "291 [D loss: 0.641(R 0.515, F0.768)] [D acc: 0.594(R 0.688, F 0.500)] [G loss: 0.641] [G acc: 0.125]\n",
      "292 [D loss: 0.651(R 0.580, F0.721)] [D acc: 0.703(R 0.750, F 0.656)] [G loss: 0.651] [G acc: 0.031]\n",
      "293 [D loss: 0.598(R 0.502, F0.694)] [D acc: 0.680(R 0.750, F 0.609)] [G loss: 0.598] [G acc: 0.031]\n",
      "294 [D loss: 0.602(R 0.627, F0.577)] [D acc: 0.672(R 0.609, F 0.734)] [G loss: 0.602] [G acc: 0.094]\n",
      "295 [D loss: 0.632(R 0.613, F0.651)] [D acc: 0.625(R 0.641, F 0.609)] [G loss: 0.632] [G acc: 0.031]\n",
      "296 [D loss: 0.627(R 0.703, F0.551)] [D acc: 0.680(R 0.578, F 0.781)] [G loss: 0.627] [G acc: 0.016]\n",
      "297 [D loss: 0.607(R 0.517, F0.697)] [D acc: 0.664(R 0.766, F 0.562)] [G loss: 0.607] [G acc: 0.062]\n",
      "298 [D loss: 0.635(R 0.560, F0.711)] [D acc: 0.656(R 0.688, F 0.625)] [G loss: 0.635] [G acc: 0.062]\n",
      "299 [D loss: 0.631(R 0.656, F0.606)] [D acc: 0.664(R 0.625, F 0.703)] [G loss: 0.631] [G acc: 0.031]\n",
      "INFO:tensorflow:Assets written to: ram://959784f7-0015-47fb-b99b-44941898cbce/assets\n",
      "INFO:tensorflow:Assets written to: ram://26af9a90-3d5d-4784-8df0-f79ae42be0da/assets\n",
      "INFO:tensorflow:Assets written to: ram://7c1531f8-6730-49e3-ab00-76dd15ce5e51/assets\n",
      "300 [D loss: 0.568(R 0.456, F0.679)] [D acc: 0.688(R 0.766, F 0.609)] [G loss: 0.568] [G acc: 0.078]\n",
      "301 [D loss: 0.598(R 0.554, F0.641)] [D acc: 0.656(R 0.625, F 0.688)] [G loss: 0.598] [G acc: 0.062]\n",
      "302 [D loss: 0.532(R 0.593, F0.470)] [D acc: 0.805(R 0.656, F 0.953)] [G loss: 0.532] [G acc: 0.016]\n",
      "303 [D loss: 0.615(R 0.450, F0.780)] [D acc: 0.648(R 0.781, F 0.516)] [G loss: 0.615] [G acc: 0.062]\n",
      "304 [D loss: 0.602(R 0.570, F0.634)] [D acc: 0.656(R 0.641, F 0.672)] [G loss: 0.602] [G acc: 0.094]\n",
      "305 [D loss: 0.657(R 0.721, F0.594)] [D acc: 0.625(R 0.516, F 0.734)] [G loss: 0.657] [G acc: 0.031]\n",
      "306 [D loss: 0.638(R 0.507, F0.769)] [D acc: 0.633(R 0.719, F 0.547)] [G loss: 0.638] [G acc: 0.156]\n",
      "307 [D loss: 0.591(R 0.597, F0.584)] [D acc: 0.688(R 0.625, F 0.750)] [G loss: 0.591] [G acc: 0.031]\n",
      "308 [D loss: 0.617(R 0.575, F0.658)] [D acc: 0.648(R 0.641, F 0.656)] [G loss: 0.617] [G acc: 0.078]\n",
      "309 [D loss: 0.608(R 0.561, F0.655)] [D acc: 0.633(R 0.641, F 0.625)] [G loss: 0.608] [G acc: 0.125]\n",
      "310 [D loss: 0.604(R 0.671, F0.536)] [D acc: 0.672(R 0.516, F 0.828)] [G loss: 0.604] [G acc: 0.000]\n",
      "311 [D loss: 0.702(R 0.549, F0.856)] [D acc: 0.570(R 0.688, F 0.453)] [G loss: 0.702] [G acc: 0.094]\n",
      "312 [D loss: 0.556(R 0.562, F0.549)] [D acc: 0.750(R 0.719, F 0.781)] [G loss: 0.556] [G acc: 0.141]\n",
      "313 [D loss: 0.688(R 0.600, F0.777)] [D acc: 0.594(R 0.609, F 0.578)] [G loss: 0.688] [G acc: 0.094]\n",
      "314 [D loss: 0.587(R 0.571, F0.603)] [D acc: 0.695(R 0.703, F 0.688)] [G loss: 0.587] [G acc: 0.125]\n",
      "315 [D loss: 0.658(R 0.639, F0.677)] [D acc: 0.648(R 0.625, F 0.672)] [G loss: 0.658] [G acc: 0.078]\n",
      "316 [D loss: 0.673(R 0.684, F0.661)] [D acc: 0.570(R 0.531, F 0.609)] [G loss: 0.673] [G acc: 0.078]\n",
      "317 [D loss: 0.615(R 0.708, F0.522)] [D acc: 0.703(R 0.469, F 0.938)] [G loss: 0.615] [G acc: 0.031]\n",
      "318 [D loss: 0.599(R 0.528, F0.671)] [D acc: 0.695(R 0.750, F 0.641)] [G loss: 0.599] [G acc: 0.078]\n",
      "319 [D loss: 0.609(R 0.662, F0.556)] [D acc: 0.633(R 0.516, F 0.750)] [G loss: 0.609] [G acc: 0.109]\n",
      "320 [D loss: 0.557(R 0.585, F0.528)] [D acc: 0.773(R 0.672, F 0.875)] [G loss: 0.557] [G acc: 0.016]\n",
      "321 [D loss: 0.637(R 0.558, F0.716)] [D acc: 0.648(R 0.703, F 0.594)] [G loss: 0.637] [G acc: 0.016]\n",
      "322 [D loss: 0.616(R 0.569, F0.662)] [D acc: 0.648(R 0.672, F 0.625)] [G loss: 0.616] [G acc: 0.062]\n",
      "323 [D loss: 0.589(R 0.613, F0.566)] [D acc: 0.648(R 0.609, F 0.688)] [G loss: 0.589] [G acc: 0.078]\n",
      "324 [D loss: 0.576(R 0.600, F0.553)] [D acc: 0.758(R 0.719, F 0.797)] [G loss: 0.576] [G acc: 0.000]\n",
      "325 [D loss: 0.588(R 0.520, F0.656)] [D acc: 0.648(R 0.703, F 0.594)] [G loss: 0.588] [G acc: 0.078]\n",
      "326 [D loss: 0.604(R 0.616, F0.593)] [D acc: 0.680(R 0.578, F 0.781)] [G loss: 0.604] [G acc: 0.109]\n",
      "327 [D loss: 0.637(R 0.550, F0.723)] [D acc: 0.641(R 0.703, F 0.578)] [G loss: 0.637] [G acc: 0.062]\n",
      "328 [D loss: 0.626(R 0.612, F0.640)] [D acc: 0.625(R 0.672, F 0.578)] [G loss: 0.626] [G acc: 0.078]\n",
      "329 [D loss: 0.597(R 0.579, F0.615)] [D acc: 0.695(R 0.672, F 0.719)] [G loss: 0.597] [G acc: 0.047]\n",
      "330 [D loss: 0.538(R 0.528, F0.549)] [D acc: 0.742(R 0.672, F 0.812)] [G loss: 0.538] [G acc: 0.062]\n",
      "331 [D loss: 0.663(R 0.520, F0.806)] [D acc: 0.547(R 0.625, F 0.469)] [G loss: 0.663] [G acc: 0.062]\n",
      "332 [D loss: 0.587(R 0.591, F0.582)] [D acc: 0.672(R 0.625, F 0.719)] [G loss: 0.587] [G acc: 0.125]\n",
      "333 [D loss: 0.626(R 0.604, F0.648)] [D acc: 0.680(R 0.656, F 0.703)] [G loss: 0.626] [G acc: 0.094]\n",
      "334 [D loss: 0.539(R 0.599, F0.478)] [D acc: 0.797(R 0.672, F 0.922)] [G loss: 0.539] [G acc: 0.016]\n",
      "335 [D loss: 0.558(R 0.417, F0.699)] [D acc: 0.688(R 0.766, F 0.609)] [G loss: 0.558] [G acc: 0.047]\n",
      "336 [D loss: 0.618(R 0.527, F0.708)] [D acc: 0.633(R 0.688, F 0.578)] [G loss: 0.618] [G acc: 0.078]\n",
      "337 [D loss: 0.586(R 0.548, F0.624)] [D acc: 0.633(R 0.641, F 0.625)] [G loss: 0.586] [G acc: 0.094]\n",
      "338 [D loss: 0.623(R 0.621, F0.625)] [D acc: 0.625(R 0.594, F 0.656)] [G loss: 0.623] [G acc: 0.141]\n",
      "339 [D loss: 0.571(R 0.623, F0.519)] [D acc: 0.750(R 0.562, F 0.938)] [G loss: 0.571] [G acc: 0.000]\n",
      "340 [D loss: 0.684(R 0.540, F0.828)] [D acc: 0.586(R 0.672, F 0.500)] [G loss: 0.684] [G acc: 0.062]\n",
      "341 [D loss: 0.583(R 0.484, F0.682)] [D acc: 0.688(R 0.797, F 0.578)] [G loss: 0.583] [G acc: 0.062]\n",
      "342 [D loss: 0.635(R 0.697, F0.573)] [D acc: 0.688(R 0.562, F 0.812)] [G loss: 0.635] [G acc: 0.141]\n",
      "343 [D loss: 0.580(R 0.516, F0.644)] [D acc: 0.688(R 0.781, F 0.594)] [G loss: 0.580] [G acc: 0.078]\n",
      "344 [D loss: 0.505(R 0.572, F0.439)] [D acc: 0.812(R 0.688, F 0.938)] [G loss: 0.505] [G acc: 0.031]\n",
      "345 [D loss: 0.675(R 0.493, F0.857)] [D acc: 0.602(R 0.750, F 0.453)] [G loss: 0.675] [G acc: 0.078]\n",
      "346 [D loss: 0.611(R 0.606, F0.616)] [D acc: 0.672(R 0.641, F 0.703)] [G loss: 0.611] [G acc: 0.062]\n",
      "347 [D loss: 0.629(R 0.561, F0.697)] [D acc: 0.641(R 0.625, F 0.656)] [G loss: 0.629] [G acc: 0.062]\n",
      "348 [D loss: 0.540(R 0.588, F0.492)] [D acc: 0.742(R 0.594, F 0.891)] [G loss: 0.540] [G acc: 0.078]\n",
      "349 [D loss: 0.666(R 0.574, F0.759)] [D acc: 0.625(R 0.609, F 0.641)] [G loss: 0.666] [G acc: 0.078]\n",
      "350 [D loss: 0.521(R 0.477, F0.566)] [D acc: 0.805(R 0.797, F 0.812)] [G loss: 0.521] [G acc: 0.078]\n",
      "351 [D loss: 0.692(R 0.779, F0.605)] [D acc: 0.586(R 0.484, F 0.688)] [G loss: 0.692] [G acc: 0.172]\n",
      "352 [D loss: 0.588(R 0.484, F0.692)] [D acc: 0.742(R 0.844, F 0.641)] [G loss: 0.588] [G acc: 0.094]\n",
      "353 [D loss: 0.556(R 0.584, F0.528)] [D acc: 0.766(R 0.719, F 0.812)] [G loss: 0.556] [G acc: 0.078]\n",
      "354 [D loss: 0.726(R 0.646, F0.805)] [D acc: 0.539(R 0.562, F 0.516)] [G loss: 0.726] [G acc: 0.078]\n",
      "355 [D loss: 0.610(R 0.581, F0.638)] [D acc: 0.688(R 0.688, F 0.688)] [G loss: 0.610] [G acc: 0.094]\n",
      "356 [D loss: 0.630(R 0.651, F0.608)] [D acc: 0.641(R 0.578, F 0.703)] [G loss: 0.630] [G acc: 0.109]\n",
      "357 [D loss: 0.580(R 0.619, F0.540)] [D acc: 0.680(R 0.609, F 0.750)] [G loss: 0.580] [G acc: 0.078]\n",
      "358 [D loss: 0.550(R 0.508, F0.591)] [D acc: 0.758(R 0.781, F 0.734)] [G loss: 0.550] [G acc: 0.094]\n",
      "359 [D loss: 0.620(R 0.565, F0.676)] [D acc: 0.633(R 0.656, F 0.609)] [G loss: 0.620] [G acc: 0.094]\n",
      "360 [D loss: 0.572(R 0.696, F0.448)] [D acc: 0.734(R 0.516, F 0.953)] [G loss: 0.572] [G acc: 0.031]\n",
      "361 [D loss: 0.641(R 0.522, F0.760)] [D acc: 0.688(R 0.781, F 0.594)] [G loss: 0.641] [G acc: 0.047]\n",
      "362 [D loss: 0.629(R 0.575, F0.682)] [D acc: 0.680(R 0.656, F 0.703)] [G loss: 0.629] [G acc: 0.047]\n",
      "363 [D loss: 0.666(R 0.638, F0.694)] [D acc: 0.641(R 0.625, F 0.656)] [G loss: 0.666] [G acc: 0.094]\n",
      "364 [D loss: 0.641(R 0.632, F0.650)] [D acc: 0.633(R 0.672, F 0.594)] [G loss: 0.641] [G acc: 0.109]\n",
      "365 [D loss: 0.637(R 0.658, F0.616)] [D acc: 0.641(R 0.594, F 0.688)] [G loss: 0.637] [G acc: 0.172]\n",
      "366 [D loss: 0.562(R 0.567, F0.557)] [D acc: 0.766(R 0.719, F 0.812)] [G loss: 0.562] [G acc: 0.203]\n",
      "367 [D loss: 0.601(R 0.575, F0.627)] [D acc: 0.695(R 0.625, F 0.766)] [G loss: 0.601] [G acc: 0.125]\n",
      "368 [D loss: 0.596(R 0.500, F0.691)] [D acc: 0.680(R 0.719, F 0.641)] [G loss: 0.596] [G acc: 0.141]\n",
      "369 [D loss: 0.598(R 0.616, F0.581)] [D acc: 0.695(R 0.656, F 0.734)] [G loss: 0.598] [G acc: 0.031]\n",
      "370 [D loss: 0.627(R 0.621, F0.632)] [D acc: 0.656(R 0.609, F 0.703)] [G loss: 0.627] [G acc: 0.094]\n",
      "371 [D loss: 0.669(R 0.629, F0.709)] [D acc: 0.625(R 0.672, F 0.578)] [G loss: 0.669] [G acc: 0.094]\n",
      "372 [D loss: 0.623(R 0.667, F0.580)] [D acc: 0.664(R 0.562, F 0.766)] [G loss: 0.623] [G acc: 0.141]\n",
      "373 [D loss: 0.572(R 0.486, F0.658)] [D acc: 0.727(R 0.750, F 0.703)] [G loss: 0.572] [G acc: 0.141]\n",
      "374 [D loss: 0.580(R 0.500, F0.661)] [D acc: 0.719(R 0.781, F 0.656)] [G loss: 0.580] [G acc: 0.047]\n",
      "375 [D loss: 0.582(R 0.595, F0.569)] [D acc: 0.711(R 0.641, F 0.781)] [G loss: 0.582] [G acc: 0.141]\n",
      "376 [D loss: 0.607(R 0.487, F0.726)] [D acc: 0.648(R 0.734, F 0.562)] [G loss: 0.607] [G acc: 0.062]\n",
      "377 [D loss: 0.552(R 0.573, F0.531)] [D acc: 0.758(R 0.703, F 0.812)] [G loss: 0.552] [G acc: 0.109]\n",
      "378 [D loss: 0.573(R 0.504, F0.642)] [D acc: 0.734(R 0.734, F 0.734)] [G loss: 0.573] [G acc: 0.094]\n",
      "379 [D loss: 0.495(R 0.423, F0.567)] [D acc: 0.781(R 0.812, F 0.750)] [G loss: 0.495] [G acc: 0.156]\n",
      "380 [D loss: 0.580(R 0.417, F0.742)] [D acc: 0.711(R 0.781, F 0.641)] [G loss: 0.580] [G acc: 0.016]\n",
      "381 [D loss: 0.573(R 0.644, F0.501)] [D acc: 0.742(R 0.625, F 0.859)] [G loss: 0.573] [G acc: 0.078]\n",
      "382 [D loss: 0.586(R 0.478, F0.694)] [D acc: 0.688(R 0.766, F 0.609)] [G loss: 0.586] [G acc: 0.125]\n",
      "383 [D loss: 0.576(R 0.587, F0.564)] [D acc: 0.703(R 0.641, F 0.766)] [G loss: 0.576] [G acc: 0.031]\n",
      "384 [D loss: 0.668(R 0.616, F0.719)] [D acc: 0.609(R 0.656, F 0.562)] [G loss: 0.668] [G acc: 0.094]\n",
      "385 [D loss: 0.523(R 0.525, F0.521)] [D acc: 0.758(R 0.719, F 0.797)] [G loss: 0.523] [G acc: 0.047]\n",
      "386 [D loss: 0.649(R 0.556, F0.743)] [D acc: 0.609(R 0.672, F 0.547)] [G loss: 0.649] [G acc: 0.094]\n",
      "387 [D loss: 0.568(R 0.601, F0.536)] [D acc: 0.742(R 0.672, F 0.812)] [G loss: 0.568] [G acc: 0.047]\n",
      "388 [D loss: 0.621(R 0.562, F0.681)] [D acc: 0.695(R 0.703, F 0.688)] [G loss: 0.621] [G acc: 0.078]\n",
      "389 [D loss: 0.606(R 0.553, F0.658)] [D acc: 0.664(R 0.656, F 0.672)] [G loss: 0.606] [G acc: 0.078]\n",
      "390 [D loss: 0.681(R 0.607, F0.754)] [D acc: 0.617(R 0.609, F 0.625)] [G loss: 0.681] [G acc: 0.094]\n",
      "391 [D loss: 0.525(R 0.585, F0.464)] [D acc: 0.797(R 0.672, F 0.922)] [G loss: 0.525] [G acc: 0.141]\n",
      "392 [D loss: 0.524(R 0.479, F0.569)] [D acc: 0.711(R 0.719, F 0.703)] [G loss: 0.524] [G acc: 0.125]\n",
      "393 [D loss: 0.598(R 0.592, F0.604)] [D acc: 0.719(R 0.688, F 0.750)] [G loss: 0.598] [G acc: 0.156]\n",
      "394 [D loss: 0.608(R 0.541, F0.675)] [D acc: 0.672(R 0.703, F 0.641)] [G loss: 0.608] [G acc: 0.109]\n",
      "395 [D loss: 0.598(R 0.572, F0.624)] [D acc: 0.672(R 0.672, F 0.672)] [G loss: 0.598] [G acc: 0.109]\n",
      "396 [D loss: 0.624(R 0.599, F0.650)] [D acc: 0.695(R 0.719, F 0.672)] [G loss: 0.624] [G acc: 0.141]\n",
      "397 [D loss: 0.599(R 0.560, F0.639)] [D acc: 0.672(R 0.703, F 0.641)] [G loss: 0.599] [G acc: 0.141]\n",
      "398 [D loss: 0.562(R 0.565, F0.560)] [D acc: 0.773(R 0.750, F 0.797)] [G loss: 0.562] [G acc: 0.094]\n",
      "399 [D loss: 0.514(R 0.512, F0.516)] [D acc: 0.789(R 0.766, F 0.812)] [G loss: 0.514] [G acc: 0.094]\n",
      "INFO:tensorflow:Assets written to: ram://7ff84235-d6e8-48ee-8671-5be82412183b/assets\n",
      "INFO:tensorflow:Assets written to: ram://f53d2a7d-5df1-4c60-bc33-a0c64931b19c/assets\n",
      "INFO:tensorflow:Assets written to: ram://69998f6e-1f32-4c64-ae81-c69e14d514ea/assets\n",
      "400 [D loss: 0.610(R 0.504, F0.716)] [D acc: 0.664(R 0.781, F 0.547)] [G loss: 0.610] [G acc: 0.078]\n",
      "401 [D loss: 0.520(R 0.587, F0.453)] [D acc: 0.773(R 0.672, F 0.875)] [G loss: 0.520] [G acc: 0.078]\n",
      "402 [D loss: 0.564(R 0.574, F0.554)] [D acc: 0.719(R 0.719, F 0.719)] [G loss: 0.564] [G acc: 0.031]\n",
      "403 [D loss: 0.533(R 0.490, F0.577)] [D acc: 0.727(R 0.734, F 0.719)] [G loss: 0.533] [G acc: 0.000]\n",
      "404 [D loss: 0.633(R 0.674, F0.593)] [D acc: 0.656(R 0.609, F 0.703)] [G loss: 0.633] [G acc: 0.031]\n",
      "405 [D loss: 0.505(R 0.516, F0.493)] [D acc: 0.766(R 0.672, F 0.859)] [G loss: 0.505] [G acc: 0.047]\n",
      "406 [D loss: 0.453(R 0.368, F0.538)] [D acc: 0.766(R 0.828, F 0.703)] [G loss: 0.453] [G acc: 0.078]\n",
      "407 [D loss: 0.577(R 0.416, F0.739)] [D acc: 0.664(R 0.781, F 0.547)] [G loss: 0.577] [G acc: 0.188]\n",
      "408 [D loss: 0.546(R 0.498, F0.593)] [D acc: 0.719(R 0.719, F 0.719)] [G loss: 0.546] [G acc: 0.031]\n",
      "409 [D loss: 0.615(R 0.573, F0.658)] [D acc: 0.648(R 0.688, F 0.609)] [G loss: 0.615] [G acc: 0.172]\n",
      "410 [D loss: 0.573(R 0.538, F0.608)] [D acc: 0.711(R 0.672, F 0.750)] [G loss: 0.573] [G acc: 0.156]\n",
      "411 [D loss: 0.649(R 0.643, F0.655)] [D acc: 0.633(R 0.625, F 0.641)] [G loss: 0.649] [G acc: 0.141]\n",
      "412 [D loss: 0.675(R 0.679, F0.671)] [D acc: 0.570(R 0.531, F 0.609)] [G loss: 0.675] [G acc: 0.078]\n",
      "413 [D loss: 0.673(R 0.668, F0.678)] [D acc: 0.516(R 0.531, F 0.500)] [G loss: 0.673] [G acc: 0.062]\n",
      "414 [D loss: 0.583(R 0.637, F0.530)] [D acc: 0.664(R 0.578, F 0.750)] [G loss: 0.583] [G acc: 0.062]\n",
      "415 [D loss: 0.625(R 0.546, F0.703)] [D acc: 0.633(R 0.719, F 0.547)] [G loss: 0.625] [G acc: 0.078]\n",
      "416 [D loss: 0.622(R 0.674, F0.571)] [D acc: 0.641(R 0.547, F 0.734)] [G loss: 0.622] [G acc: 0.047]\n",
      "417 [D loss: 0.549(R 0.495, F0.603)] [D acc: 0.734(R 0.734, F 0.734)] [G loss: 0.549] [G acc: 0.094]\n",
      "418 [D loss: 0.563(R 0.578, F0.549)] [D acc: 0.727(R 0.688, F 0.766)] [G loss: 0.563] [G acc: 0.125]\n",
      "419 [D loss: 0.596(R 0.530, F0.662)] [D acc: 0.641(R 0.703, F 0.578)] [G loss: 0.596] [G acc: 0.047]\n",
      "420 [D loss: 0.631(R 0.603, F0.660)] [D acc: 0.641(R 0.578, F 0.703)] [G loss: 0.631] [G acc: 0.047]\n",
      "421 [D loss: 0.525(R 0.525, F0.524)] [D acc: 0.734(R 0.672, F 0.797)] [G loss: 0.525] [G acc: 0.062]\n",
      "422 [D loss: 0.562(R 0.561, F0.562)] [D acc: 0.734(R 0.688, F 0.781)] [G loss: 0.562] [G acc: 0.062]\n",
      "423 [D loss: 0.610(R 0.515, F0.705)] [D acc: 0.680(R 0.719, F 0.641)] [G loss: 0.610] [G acc: 0.078]\n",
      "424 [D loss: 0.518(R 0.655, F0.381)] [D acc: 0.750(R 0.578, F 0.922)] [G loss: 0.518] [G acc: 0.062]\n",
      "425 [D loss: 0.607(R 0.545, F0.669)] [D acc: 0.695(R 0.672, F 0.719)] [G loss: 0.607] [G acc: 0.062]\n",
      "426 [D loss: 0.626(R 0.558, F0.695)] [D acc: 0.680(R 0.703, F 0.656)] [G loss: 0.626] [G acc: 0.078]\n",
      "427 [D loss: 0.595(R 0.597, F0.592)] [D acc: 0.695(R 0.672, F 0.719)] [G loss: 0.595] [G acc: 0.094]\n",
      "428 [D loss: 0.533(R 0.502, F0.564)] [D acc: 0.711(R 0.703, F 0.719)] [G loss: 0.533] [G acc: 0.141]\n",
      "429 [D loss: 0.508(R 0.543, F0.472)] [D acc: 0.758(R 0.688, F 0.828)] [G loss: 0.508] [G acc: 0.109]\n",
      "430 [D loss: 0.569(R 0.713, F0.426)] [D acc: 0.750(R 0.562, F 0.938)] [G loss: 0.569] [G acc: 0.109]\n",
      "431 [D loss: 0.544(R 0.513, F0.575)] [D acc: 0.758(R 0.750, F 0.766)] [G loss: 0.544] [G acc: 0.125]\n",
      "432 [D loss: 0.567(R 0.408, F0.726)] [D acc: 0.672(R 0.781, F 0.562)] [G loss: 0.567] [G acc: 0.125]\n",
      "433 [D loss: 0.621(R 0.553, F0.688)] [D acc: 0.688(R 0.688, F 0.688)] [G loss: 0.621] [G acc: 0.062]\n",
      "434 [D loss: 0.518(R 0.502, F0.533)] [D acc: 0.758(R 0.734, F 0.781)] [G loss: 0.518] [G acc: 0.141]\n",
      "435 [D loss: 0.594(R 0.561, F0.627)] [D acc: 0.641(R 0.641, F 0.641)] [G loss: 0.594] [G acc: 0.094]\n",
      "436 [D loss: 0.647(R 0.664, F0.630)] [D acc: 0.625(R 0.578, F 0.672)] [G loss: 0.647] [G acc: 0.172]\n",
      "437 [D loss: 0.541(R 0.475, F0.606)] [D acc: 0.695(R 0.734, F 0.656)] [G loss: 0.541] [G acc: 0.172]\n",
      "438 [D loss: 0.590(R 0.574, F0.606)] [D acc: 0.680(R 0.609, F 0.750)] [G loss: 0.590] [G acc: 0.109]\n",
      "439 [D loss: 0.559(R 0.495, F0.623)] [D acc: 0.734(R 0.734, F 0.734)] [G loss: 0.559] [G acc: 0.203]\n",
      "440 [D loss: 0.570(R 0.539, F0.602)] [D acc: 0.680(R 0.703, F 0.656)] [G loss: 0.570] [G acc: 0.078]\n",
      "441 [D loss: 0.604(R 0.627, F0.581)] [D acc: 0.633(R 0.547, F 0.719)] [G loss: 0.604] [G acc: 0.094]\n",
      "442 [D loss: 0.584(R 0.578, F0.590)] [D acc: 0.703(R 0.656, F 0.750)] [G loss: 0.584] [G acc: 0.141]\n",
      "443 [D loss: 0.623(R 0.616, F0.630)] [D acc: 0.680(R 0.672, F 0.688)] [G loss: 0.623] [G acc: 0.156]\n",
      "444 [D loss: 0.607(R 0.533, F0.680)] [D acc: 0.688(R 0.734, F 0.641)] [G loss: 0.607] [G acc: 0.078]\n",
      "445 [D loss: 0.589(R 0.634, F0.543)] [D acc: 0.703(R 0.594, F 0.812)] [G loss: 0.589] [G acc: 0.109]\n",
      "446 [D loss: 0.566(R 0.531, F0.600)] [D acc: 0.750(R 0.750, F 0.750)] [G loss: 0.566] [G acc: 0.062]\n",
      "447 [D loss: 0.617(R 0.543, F0.691)] [D acc: 0.648(R 0.656, F 0.641)] [G loss: 0.617] [G acc: 0.109]\n",
      "448 [D loss: 0.560(R 0.600, F0.521)] [D acc: 0.695(R 0.594, F 0.797)] [G loss: 0.560] [G acc: 0.062]\n",
      "449 [D loss: 0.672(R 0.629, F0.715)] [D acc: 0.578(R 0.594, F 0.562)] [G loss: 0.672] [G acc: 0.156]\n",
      "450 [D loss: 0.632(R 0.669, F0.595)] [D acc: 0.656(R 0.562, F 0.750)] [G loss: 0.632] [G acc: 0.125]\n",
      "451 [D loss: 0.627(R 0.659, F0.594)] [D acc: 0.672(R 0.609, F 0.734)] [G loss: 0.627] [G acc: 0.109]\n",
      "452 [D loss: 0.600(R 0.604, F0.595)] [D acc: 0.625(R 0.578, F 0.672)] [G loss: 0.600] [G acc: 0.078]\n",
      "453 [D loss: 0.659(R 0.594, F0.724)] [D acc: 0.648(R 0.688, F 0.609)] [G loss: 0.659] [G acc: 0.156]\n",
      "454 [D loss: 0.612(R 0.657, F0.567)] [D acc: 0.680(R 0.547, F 0.812)] [G loss: 0.612] [G acc: 0.125]\n",
      "455 [D loss: 0.617(R 0.565, F0.669)] [D acc: 0.625(R 0.641, F 0.609)] [G loss: 0.617] [G acc: 0.109]\n",
      "456 [D loss: 0.645(R 0.656, F0.633)] [D acc: 0.664(R 0.641, F 0.688)] [G loss: 0.645] [G acc: 0.078]\n",
      "457 [D loss: 0.613(R 0.642, F0.584)] [D acc: 0.648(R 0.594, F 0.703)] [G loss: 0.613] [G acc: 0.062]\n",
      "458 [D loss: 0.553(R 0.637, F0.469)] [D acc: 0.781(R 0.656, F 0.906)] [G loss: 0.553] [G acc: 0.047]\n",
      "459 [D loss: 0.583(R 0.615, F0.551)] [D acc: 0.734(R 0.672, F 0.797)] [G loss: 0.583] [G acc: 0.125]\n",
      "460 [D loss: 0.568(R 0.444, F0.691)] [D acc: 0.742(R 0.781, F 0.703)] [G loss: 0.568] [G acc: 0.125]\n",
      "461 [D loss: 0.613(R 0.510, F0.716)] [D acc: 0.688(R 0.750, F 0.625)] [G loss: 0.613] [G acc: 0.094]\n",
      "462 [D loss: 0.491(R 0.524, F0.458)] [D acc: 0.773(R 0.656, F 0.891)] [G loss: 0.491] [G acc: 0.062]\n",
      "463 [D loss: 0.542(R 0.494, F0.590)] [D acc: 0.727(R 0.734, F 0.719)] [G loss: 0.542] [G acc: 0.156]\n",
      "464 [D loss: 0.590(R 0.543, F0.637)] [D acc: 0.688(R 0.672, F 0.703)] [G loss: 0.590] [G acc: 0.203]\n",
      "465 [D loss: 0.598(R 0.546, F0.650)] [D acc: 0.672(R 0.750, F 0.594)] [G loss: 0.598] [G acc: 0.078]\n",
      "466 [D loss: 0.534(R 0.637, F0.431)] [D acc: 0.758(R 0.609, F 0.906)] [G loss: 0.534] [G acc: 0.094]\n",
      "467 [D loss: 0.597(R 0.398, F0.796)] [D acc: 0.703(R 0.859, F 0.547)] [G loss: 0.597] [G acc: 0.078]\n",
      "468 [D loss: 0.576(R 0.488, F0.664)] [D acc: 0.711(R 0.734, F 0.688)] [G loss: 0.576] [G acc: 0.031]\n",
      "469 [D loss: 0.560(R 0.669, F0.450)] [D acc: 0.695(R 0.516, F 0.875)] [G loss: 0.560] [G acc: 0.109]\n",
      "470 [D loss: 0.522(R 0.471, F0.572)] [D acc: 0.711(R 0.750, F 0.672)] [G loss: 0.522] [G acc: 0.109]\n",
      "471 [D loss: 0.569(R 0.497, F0.640)] [D acc: 0.727(R 0.750, F 0.703)] [G loss: 0.569] [G acc: 0.094]\n",
      "472 [D loss: 0.552(R 0.675, F0.429)] [D acc: 0.719(R 0.531, F 0.906)] [G loss: 0.552] [G acc: 0.062]\n",
      "473 [D loss: 0.674(R 0.657, F0.690)] [D acc: 0.633(R 0.641, F 0.625)] [G loss: 0.674] [G acc: 0.188]\n",
      "474 [D loss: 0.574(R 0.450, F0.698)] [D acc: 0.719(R 0.812, F 0.625)] [G loss: 0.574] [G acc: 0.125]\n",
      "475 [D loss: 0.645(R 0.543, F0.746)] [D acc: 0.648(R 0.688, F 0.609)] [G loss: 0.645] [G acc: 0.047]\n",
      "476 [D loss: 0.515(R 0.489, F0.540)] [D acc: 0.766(R 0.797, F 0.734)] [G loss: 0.515] [G acc: 0.109]\n",
      "477 [D loss: 0.602(R 0.684, F0.519)] [D acc: 0.680(R 0.562, F 0.797)] [G loss: 0.602] [G acc: 0.109]\n",
      "478 [D loss: 0.486(R 0.413, F0.559)] [D acc: 0.758(R 0.797, F 0.719)] [G loss: 0.486] [G acc: 0.078]\n",
      "479 [D loss: 0.635(R 0.639, F0.632)] [D acc: 0.641(R 0.609, F 0.672)] [G loss: 0.635] [G acc: 0.109]\n",
      "480 [D loss: 0.594(R 0.612, F0.576)] [D acc: 0.656(R 0.609, F 0.703)] [G loss: 0.594] [G acc: 0.188]\n",
      "481 [D loss: 0.625(R 0.554, F0.697)] [D acc: 0.641(R 0.703, F 0.578)] [G loss: 0.625] [G acc: 0.125]\n",
      "482 [D loss: 0.561(R 0.593, F0.529)] [D acc: 0.758(R 0.672, F 0.844)] [G loss: 0.561] [G acc: 0.219]\n",
      "483 [D loss: 0.704(R 0.436, F0.971)] [D acc: 0.672(R 0.797, F 0.547)] [G loss: 0.704] [G acc: 0.109]\n",
      "484 [D loss: 0.637(R 0.722, F0.551)] [D acc: 0.641(R 0.531, F 0.750)] [G loss: 0.637] [G acc: 0.125]\n",
      "485 [D loss: 0.530(R 0.575, F0.484)] [D acc: 0.742(R 0.625, F 0.859)] [G loss: 0.530] [G acc: 0.141]\n",
      "486 [D loss: 0.538(R 0.496, F0.580)] [D acc: 0.758(R 0.766, F 0.750)] [G loss: 0.538] [G acc: 0.062]\n",
      "487 [D loss: 0.564(R 0.553, F0.575)] [D acc: 0.695(R 0.703, F 0.688)] [G loss: 0.564] [G acc: 0.109]\n",
      "488 [D loss: 0.626(R 0.602, F0.650)] [D acc: 0.672(R 0.625, F 0.719)] [G loss: 0.626] [G acc: 0.062]\n",
      "489 [D loss: 0.534(R 0.552, F0.516)] [D acc: 0.781(R 0.734, F 0.828)] [G loss: 0.534] [G acc: 0.094]\n",
      "490 [D loss: 0.622(R 0.625, F0.620)] [D acc: 0.625(R 0.578, F 0.672)] [G loss: 0.622] [G acc: 0.141]\n",
      "491 [D loss: 0.633(R 0.583, F0.682)] [D acc: 0.648(R 0.672, F 0.625)] [G loss: 0.633] [G acc: 0.094]\n",
      "492 [D loss: 0.521(R 0.602, F0.440)] [D acc: 0.773(R 0.656, F 0.891)] [G loss: 0.521] [G acc: 0.062]\n",
      "493 [D loss: 0.570(R 0.452, F0.688)] [D acc: 0.727(R 0.812, F 0.641)] [G loss: 0.570] [G acc: 0.078]\n",
      "494 [D loss: 0.576(R 0.634, F0.518)] [D acc: 0.695(R 0.625, F 0.766)] [G loss: 0.576] [G acc: 0.047]\n",
      "495 [D loss: 0.567(R 0.662, F0.472)] [D acc: 0.711(R 0.547, F 0.875)] [G loss: 0.567] [G acc: 0.031]\n",
      "496 [D loss: 0.605(R 0.453, F0.757)] [D acc: 0.695(R 0.797, F 0.594)] [G loss: 0.605] [G acc: 0.094]\n",
      "497 [D loss: 0.552(R 0.518, F0.586)] [D acc: 0.719(R 0.703, F 0.734)] [G loss: 0.552] [G acc: 0.078]\n",
      "498 [D loss: 0.591(R 0.605, F0.577)] [D acc: 0.688(R 0.625, F 0.750)] [G loss: 0.591] [G acc: 0.047]\n",
      "499 [D loss: 0.529(R 0.476, F0.583)] [D acc: 0.773(R 0.812, F 0.734)] [G loss: 0.529] [G acc: 0.078]\n",
      "INFO:tensorflow:Assets written to: ram://4a6708fb-14cd-486a-bad9-fba07169609d/assets\n",
      "INFO:tensorflow:Assets written to: ram://68e8b489-42bd-4f2b-8fd3-5f842d951292/assets\n",
      "INFO:tensorflow:Assets written to: ram://70b3449e-9da2-4e23-8073-e0391e6fe521/assets\n",
      "500 [D loss: 0.532(R 0.629, F0.436)] [D acc: 0.773(R 0.656, F 0.891)] [G loss: 0.532] [G acc: 0.109]\n",
      "501 [D loss: 0.537(R 0.441, F0.632)] [D acc: 0.711(R 0.766, F 0.656)] [G loss: 0.537] [G acc: 0.141]\n",
      "502 [D loss: 0.553(R 0.670, F0.436)] [D acc: 0.766(R 0.594, F 0.938)] [G loss: 0.553] [G acc: 0.047]\n",
      "503 [D loss: 0.552(R 0.428, F0.676)] [D acc: 0.719(R 0.781, F 0.656)] [G loss: 0.552] [G acc: 0.031]\n",
      "504 [D loss: 0.558(R 0.503, F0.614)] [D acc: 0.727(R 0.719, F 0.734)] [G loss: 0.558] [G acc: 0.141]\n",
      "505 [D loss: 0.580(R 0.631, F0.529)] [D acc: 0.672(R 0.594, F 0.750)] [G loss: 0.580] [G acc: 0.156]\n",
      "506 [D loss: 0.608(R 0.653, F0.564)] [D acc: 0.664(R 0.562, F 0.766)] [G loss: 0.608] [G acc: 0.141]\n",
      "507 [D loss: 0.587(R 0.543, F0.630)] [D acc: 0.695(R 0.656, F 0.734)] [G loss: 0.587] [G acc: 0.000]\n",
      "508 [D loss: 0.640(R 0.575, F0.705)] [D acc: 0.609(R 0.625, F 0.594)] [G loss: 0.640] [G acc: 0.078]\n",
      "509 [D loss: 0.541(R 0.593, F0.489)] [D acc: 0.750(R 0.625, F 0.875)] [G loss: 0.541] [G acc: 0.141]\n",
      "510 [D loss: 0.548(R 0.566, F0.531)] [D acc: 0.742(R 0.672, F 0.812)] [G loss: 0.548] [G acc: 0.062]\n",
      "511 [D loss: 0.607(R 0.524, F0.690)] [D acc: 0.625(R 0.672, F 0.578)] [G loss: 0.607] [G acc: 0.125]\n",
      "512 [D loss: 0.515(R 0.493, F0.536)] [D acc: 0.766(R 0.766, F 0.766)] [G loss: 0.515] [G acc: 0.109]\n",
      "513 [D loss: 0.562(R 0.553, F0.571)] [D acc: 0.695(R 0.625, F 0.766)] [G loss: 0.562] [G acc: 0.047]\n",
      "514 [D loss: 0.664(R 0.623, F0.706)] [D acc: 0.594(R 0.641, F 0.547)] [G loss: 0.664] [G acc: 0.156]\n",
      "515 [D loss: 0.628(R 0.605, F0.651)] [D acc: 0.633(R 0.609, F 0.656)] [G loss: 0.628] [G acc: 0.047]\n",
      "516 [D loss: 0.642(R 0.610, F0.673)] [D acc: 0.633(R 0.625, F 0.641)] [G loss: 0.642] [G acc: 0.125]\n",
      "517 [D loss: 0.595(R 0.641, F0.549)] [D acc: 0.648(R 0.547, F 0.750)] [G loss: 0.595] [G acc: 0.109]\n",
      "518 [D loss: 0.603(R 0.628, F0.577)] [D acc: 0.695(R 0.609, F 0.781)] [G loss: 0.603] [G acc: 0.125]\n",
      "519 [D loss: 0.590(R 0.532, F0.648)] [D acc: 0.648(R 0.703, F 0.594)] [G loss: 0.590] [G acc: 0.078]\n",
      "520 [D loss: 0.554(R 0.585, F0.524)] [D acc: 0.758(R 0.719, F 0.797)] [G loss: 0.554] [G acc: 0.094]\n",
      "521 [D loss: 0.564(R 0.509, F0.619)] [D acc: 0.672(R 0.656, F 0.688)] [G loss: 0.564] [G acc: 0.125]\n",
      "522 [D loss: 0.645(R 0.669, F0.620)] [D acc: 0.641(R 0.547, F 0.734)] [G loss: 0.645] [G acc: 0.141]\n",
      "523 [D loss: 0.607(R 0.542, F0.672)] [D acc: 0.703(R 0.750, F 0.656)] [G loss: 0.607] [G acc: 0.125]\n",
      "524 [D loss: 0.592(R 0.681, F0.504)] [D acc: 0.719(R 0.625, F 0.812)] [G loss: 0.592] [G acc: 0.094]\n",
      "525 [D loss: 0.572(R 0.547, F0.596)] [D acc: 0.750(R 0.703, F 0.797)] [G loss: 0.572] [G acc: 0.078]\n",
      "526 [D loss: 0.603(R 0.516, F0.689)] [D acc: 0.656(R 0.734, F 0.578)] [G loss: 0.603] [G acc: 0.062]\n",
      "527 [D loss: 0.611(R 0.623, F0.598)] [D acc: 0.648(R 0.562, F 0.734)] [G loss: 0.611] [G acc: 0.109]\n",
      "528 [D loss: 0.612(R 0.553, F0.671)] [D acc: 0.680(R 0.719, F 0.641)] [G loss: 0.612] [G acc: 0.094]\n",
      "529 [D loss: 0.606(R 0.573, F0.639)] [D acc: 0.695(R 0.672, F 0.719)] [G loss: 0.606] [G acc: 0.078]\n",
      "530 [D loss: 0.563(R 0.531, F0.594)] [D acc: 0.672(R 0.672, F 0.672)] [G loss: 0.563] [G acc: 0.188]\n",
      "531 [D loss: 0.585(R 0.555, F0.616)] [D acc: 0.711(R 0.672, F 0.750)] [G loss: 0.585] [G acc: 0.172]\n",
      "532 [D loss: 0.663(R 0.657, F0.668)] [D acc: 0.586(R 0.516, F 0.656)] [G loss: 0.663] [G acc: 0.094]\n",
      "533 [D loss: 0.620(R 0.598, F0.643)] [D acc: 0.633(R 0.641, F 0.625)] [G loss: 0.620] [G acc: 0.031]\n",
      "534 [D loss: 0.568(R 0.546, F0.589)] [D acc: 0.742(R 0.719, F 0.766)] [G loss: 0.568] [G acc: 0.094]\n",
      "535 [D loss: 0.611(R 0.630, F0.592)] [D acc: 0.695(R 0.688, F 0.703)] [G loss: 0.611] [G acc: 0.078]\n",
      "536 [D loss: 0.597(R 0.546, F0.648)] [D acc: 0.656(R 0.656, F 0.656)] [G loss: 0.597] [G acc: 0.062]\n",
      "537 [D loss: 0.623(R 0.617, F0.629)] [D acc: 0.641(R 0.594, F 0.688)] [G loss: 0.623] [G acc: 0.109]\n",
      "538 [D loss: 0.592(R 0.641, F0.544)] [D acc: 0.703(R 0.578, F 0.828)] [G loss: 0.592] [G acc: 0.078]\n",
      "539 [D loss: 0.552(R 0.602, F0.501)] [D acc: 0.766(R 0.672, F 0.859)] [G loss: 0.552] [G acc: 0.031]\n",
      "540 [D loss: 0.524(R 0.435, F0.613)] [D acc: 0.711(R 0.766, F 0.656)] [G loss: 0.524] [G acc: 0.094]\n",
      "541 [D loss: 0.644(R 0.666, F0.622)] [D acc: 0.648(R 0.562, F 0.734)] [G loss: 0.644] [G acc: 0.156]\n",
      "542 [D loss: 0.632(R 0.619, F0.645)] [D acc: 0.594(R 0.578, F 0.609)] [G loss: 0.632] [G acc: 0.047]\n",
      "543 [D loss: 0.442(R 0.472, F0.412)] [D acc: 0.836(R 0.719, F 0.953)] [G loss: 0.442] [G acc: 0.000]\n",
      "544 [D loss: 0.637(R 0.558, F0.716)] [D acc: 0.648(R 0.656, F 0.641)] [G loss: 0.637] [G acc: 0.125]\n",
      "545 [D loss: 0.569(R 0.454, F0.684)] [D acc: 0.695(R 0.750, F 0.641)] [G loss: 0.569] [G acc: 0.156]\n",
      "546 [D loss: 0.641(R 0.575, F0.708)] [D acc: 0.656(R 0.625, F 0.688)] [G loss: 0.641] [G acc: 0.156]\n",
      "547 [D loss: 0.682(R 0.692, F0.672)] [D acc: 0.586(R 0.516, F 0.656)] [G loss: 0.682] [G acc: 0.172]\n",
      "548 [D loss: 0.596(R 0.653, F0.540)] [D acc: 0.680(R 0.547, F 0.812)] [G loss: 0.596] [G acc: 0.062]\n",
      "549 [D loss: 0.611(R 0.622, F0.599)] [D acc: 0.656(R 0.578, F 0.734)] [G loss: 0.611] [G acc: 0.188]\n",
      "550 [D loss: 0.621(R 0.603, F0.640)] [D acc: 0.664(R 0.656, F 0.672)] [G loss: 0.621] [G acc: 0.109]\n",
      "551 [D loss: 0.592(R 0.551, F0.632)] [D acc: 0.695(R 0.672, F 0.719)] [G loss: 0.592] [G acc: 0.078]\n",
      "552 [D loss: 0.552(R 0.516, F0.589)] [D acc: 0.711(R 0.672, F 0.750)] [G loss: 0.552] [G acc: 0.141]\n",
      "553 [D loss: 0.567(R 0.616, F0.518)] [D acc: 0.742(R 0.656, F 0.828)] [G loss: 0.567] [G acc: 0.109]\n",
      "554 [D loss: 0.560(R 0.498, F0.622)] [D acc: 0.711(R 0.703, F 0.719)] [G loss: 0.560] [G acc: 0.156]\n",
      "555 [D loss: 0.617(R 0.608, F0.627)] [D acc: 0.609(R 0.578, F 0.641)] [G loss: 0.617] [G acc: 0.125]\n",
      "556 [D loss: 0.604(R 0.634, F0.573)] [D acc: 0.695(R 0.641, F 0.750)] [G loss: 0.604] [G acc: 0.062]\n",
      "557 [D loss: 0.604(R 0.627, F0.580)] [D acc: 0.648(R 0.594, F 0.703)] [G loss: 0.604] [G acc: 0.141]\n",
      "558 [D loss: 0.576(R 0.675, F0.477)] [D acc: 0.695(R 0.562, F 0.828)] [G loss: 0.576] [G acc: 0.078]\n",
      "559 [D loss: 0.584(R 0.555, F0.612)] [D acc: 0.719(R 0.688, F 0.750)] [G loss: 0.584] [G acc: 0.078]\n",
      "560 [D loss: 0.635(R 0.561, F0.708)] [D acc: 0.625(R 0.656, F 0.594)] [G loss: 0.635] [G acc: 0.125]\n",
      "561 [D loss: 0.694(R 0.646, F0.742)] [D acc: 0.609(R 0.562, F 0.656)] [G loss: 0.694] [G acc: 0.172]\n",
      "562 [D loss: 0.603(R 0.629, F0.576)] [D acc: 0.641(R 0.562, F 0.719)] [G loss: 0.603] [G acc: 0.094]\n",
      "563 [D loss: 0.547(R 0.544, F0.550)] [D acc: 0.766(R 0.734, F 0.797)] [G loss: 0.547] [G acc: 0.125]\n",
      "564 [D loss: 0.623(R 0.596, F0.650)] [D acc: 0.648(R 0.609, F 0.688)] [G loss: 0.623] [G acc: 0.047]\n",
      "565 [D loss: 0.551(R 0.618, F0.485)] [D acc: 0.766(R 0.656, F 0.875)] [G loss: 0.551] [G acc: 0.078]\n",
      "566 [D loss: 0.630(R 0.495, F0.764)] [D acc: 0.688(R 0.688, F 0.688)] [G loss: 0.630] [G acc: 0.109]\n",
      "567 [D loss: 0.613(R 0.638, F0.588)] [D acc: 0.633(R 0.578, F 0.688)] [G loss: 0.613] [G acc: 0.141]\n",
      "568 [D loss: 0.514(R 0.559, F0.469)] [D acc: 0.773(R 0.672, F 0.875)] [G loss: 0.514] [G acc: 0.125]\n",
      "569 [D loss: 0.582(R 0.537, F0.627)] [D acc: 0.703(R 0.672, F 0.734)] [G loss: 0.582] [G acc: 0.078]\n",
      "570 [D loss: 0.605(R 0.604, F0.606)] [D acc: 0.734(R 0.688, F 0.781)] [G loss: 0.605] [G acc: 0.125]\n",
      "571 [D loss: 0.593(R 0.672, F0.513)] [D acc: 0.672(R 0.578, F 0.766)] [G loss: 0.593] [G acc: 0.141]\n",
      "572 [D loss: 0.636(R 0.536, F0.737)] [D acc: 0.641(R 0.688, F 0.594)] [G loss: 0.636] [G acc: 0.109]\n",
      "573 [D loss: 0.631(R 0.554, F0.708)] [D acc: 0.656(R 0.703, F 0.609)] [G loss: 0.631] [G acc: 0.141]\n",
      "574 [D loss: 0.582(R 0.571, F0.594)] [D acc: 0.680(R 0.641, F 0.719)] [G loss: 0.582] [G acc: 0.062]\n",
      "575 [D loss: 0.628(R 0.647, F0.608)] [D acc: 0.641(R 0.578, F 0.703)] [G loss: 0.628] [G acc: 0.078]\n",
      "576 [D loss: 0.561(R 0.545, F0.577)] [D acc: 0.773(R 0.781, F 0.766)] [G loss: 0.561] [G acc: 0.109]\n",
      "577 [D loss: 0.556(R 0.580, F0.531)] [D acc: 0.742(R 0.672, F 0.812)] [G loss: 0.556] [G acc: 0.156]\n",
      "578 [D loss: 0.644(R 0.628, F0.660)] [D acc: 0.625(R 0.641, F 0.609)] [G loss: 0.644] [G acc: 0.094]\n",
      "579 [D loss: 0.646(R 0.604, F0.689)] [D acc: 0.672(R 0.625, F 0.719)] [G loss: 0.646] [G acc: 0.125]\n",
      "580 [D loss: 0.655(R 0.614, F0.695)] [D acc: 0.602(R 0.625, F 0.578)] [G loss: 0.655] [G acc: 0.188]\n",
      "581 [D loss: 0.578(R 0.548, F0.608)] [D acc: 0.688(R 0.688, F 0.688)] [G loss: 0.578] [G acc: 0.047]\n",
      "582 [D loss: 0.662(R 0.705, F0.618)] [D acc: 0.672(R 0.594, F 0.750)] [G loss: 0.662] [G acc: 0.125]\n",
      "583 [D loss: 0.600(R 0.558, F0.641)] [D acc: 0.703(R 0.719, F 0.688)] [G loss: 0.600] [G acc: 0.078]\n",
      "584 [D loss: 0.671(R 0.701, F0.641)] [D acc: 0.586(R 0.547, F 0.625)] [G loss: 0.671] [G acc: 0.062]\n",
      "585 [D loss: 0.611(R 0.627, F0.596)] [D acc: 0.688(R 0.625, F 0.750)] [G loss: 0.611] [G acc: 0.109]\n",
      "586 [D loss: 0.583(R 0.582, F0.584)] [D acc: 0.727(R 0.688, F 0.766)] [G loss: 0.583] [G acc: 0.078]\n",
      "587 [D loss: 0.625(R 0.657, F0.594)] [D acc: 0.625(R 0.500, F 0.750)] [G loss: 0.625] [G acc: 0.141]\n",
      "588 [D loss: 0.609(R 0.695, F0.523)] [D acc: 0.656(R 0.531, F 0.781)] [G loss: 0.609] [G acc: 0.062]\n",
      "589 [D loss: 0.582(R 0.520, F0.644)] [D acc: 0.727(R 0.734, F 0.719)] [G loss: 0.582] [G acc: 0.094]\n",
      "590 [D loss: 0.595(R 0.571, F0.620)] [D acc: 0.672(R 0.578, F 0.766)] [G loss: 0.595] [G acc: 0.203]\n",
      "591 [D loss: 0.571(R 0.563, F0.578)] [D acc: 0.734(R 0.719, F 0.750)] [G loss: 0.571] [G acc: 0.047]\n",
      "592 [D loss: 0.615(R 0.648, F0.582)] [D acc: 0.672(R 0.656, F 0.688)] [G loss: 0.615] [G acc: 0.094]\n",
      "593 [D loss: 0.622(R 0.610, F0.635)] [D acc: 0.672(R 0.609, F 0.734)] [G loss: 0.622] [G acc: 0.156]\n",
      "594 [D loss: 0.626(R 0.552, F0.699)] [D acc: 0.648(R 0.625, F 0.672)] [G loss: 0.626] [G acc: 0.188]\n",
      "595 [D loss: 0.563(R 0.589, F0.537)] [D acc: 0.719(R 0.625, F 0.812)] [G loss: 0.563] [G acc: 0.047]\n",
      "596 [D loss: 0.628(R 0.536, F0.720)] [D acc: 0.664(R 0.750, F 0.578)] [G loss: 0.628] [G acc: 0.188]\n",
      "597 [D loss: 0.514(R 0.542, F0.486)] [D acc: 0.781(R 0.734, F 0.828)] [G loss: 0.514] [G acc: 0.031]\n",
      "598 [D loss: 0.494(R 0.431, F0.557)] [D acc: 0.734(R 0.797, F 0.672)] [G loss: 0.494] [G acc: 0.125]\n",
      "599 [D loss: 0.584(R 0.553, F0.614)] [D acc: 0.672(R 0.656, F 0.688)] [G loss: 0.584] [G acc: 0.047]\n",
      "INFO:tensorflow:Assets written to: ram://4384ad46-36d2-4fa2-8615-ba0ec3586ffd/assets\n",
      "INFO:tensorflow:Assets written to: ram://ab30d4a8-f1f9-4659-8eca-f48a2420340d/assets\n",
      "INFO:tensorflow:Assets written to: ram://0fe08526-42a3-434c-bdaf-277484029aa1/assets\n",
      "600 [D loss: 0.534(R 0.546, F0.522)] [D acc: 0.758(R 0.734, F 0.781)] [G loss: 0.534] [G acc: 0.141]\n",
      "601 [D loss: 0.511(R 0.575, F0.448)] [D acc: 0.797(R 0.672, F 0.922)] [G loss: 0.511] [G acc: 0.047]\n",
      "602 [D loss: 0.612(R 0.533, F0.691)] [D acc: 0.680(R 0.703, F 0.656)] [G loss: 0.612] [G acc: 0.094]\n",
      "603 [D loss: 0.636(R 0.470, F0.802)] [D acc: 0.617(R 0.703, F 0.531)] [G loss: 0.636] [G acc: 0.109]\n",
      "604 [D loss: 0.530(R 0.647, F0.413)] [D acc: 0.758(R 0.625, F 0.891)] [G loss: 0.530] [G acc: 0.078]\n",
      "605 [D loss: 0.677(R 0.653, F0.700)] [D acc: 0.617(R 0.594, F 0.641)] [G loss: 0.677] [G acc: 0.125]\n",
      "606 [D loss: 0.602(R 0.587, F0.617)] [D acc: 0.688(R 0.703, F 0.672)] [G loss: 0.602] [G acc: 0.141]\n",
      "607 [D loss: 0.663(R 0.592, F0.735)] [D acc: 0.609(R 0.594, F 0.625)] [G loss: 0.663] [G acc: 0.156]\n",
      "608 [D loss: 0.624(R 0.639, F0.610)] [D acc: 0.617(R 0.594, F 0.641)] [G loss: 0.624] [G acc: 0.109]\n",
      "609 [D loss: 0.602(R 0.567, F0.637)] [D acc: 0.641(R 0.656, F 0.625)] [G loss: 0.602] [G acc: 0.031]\n",
      "610 [D loss: 0.578(R 0.597, F0.559)] [D acc: 0.703(R 0.672, F 0.734)] [G loss: 0.578] [G acc: 0.109]\n",
      "611 [D loss: 0.611(R 0.567, F0.655)] [D acc: 0.680(R 0.672, F 0.688)] [G loss: 0.611] [G acc: 0.094]\n",
      "612 [D loss: 0.528(R 0.529, F0.528)] [D acc: 0.781(R 0.750, F 0.812)] [G loss: 0.528] [G acc: 0.141]\n",
      "613 [D loss: 0.662(R 0.577, F0.746)] [D acc: 0.656(R 0.734, F 0.578)] [G loss: 0.662] [G acc: 0.078]\n",
      "614 [D loss: 0.553(R 0.625, F0.481)] [D acc: 0.750(R 0.609, F 0.891)] [G loss: 0.553] [G acc: 0.156]\n",
      "615 [D loss: 0.590(R 0.522, F0.659)] [D acc: 0.703(R 0.656, F 0.750)] [G loss: 0.590] [G acc: 0.094]\n",
      "616 [D loss: 0.615(R 0.560, F0.669)] [D acc: 0.688(R 0.703, F 0.672)] [G loss: 0.615] [G acc: 0.188]\n",
      "617 [D loss: 0.546(R 0.569, F0.523)] [D acc: 0.664(R 0.594, F 0.734)] [G loss: 0.546] [G acc: 0.109]\n",
      "618 [D loss: 0.620(R 0.582, F0.659)] [D acc: 0.664(R 0.641, F 0.688)] [G loss: 0.620] [G acc: 0.172]\n",
      "619 [D loss: 0.626(R 0.663, F0.589)] [D acc: 0.625(R 0.578, F 0.672)] [G loss: 0.626] [G acc: 0.172]\n",
      "620 [D loss: 0.556(R 0.520, F0.592)] [D acc: 0.688(R 0.672, F 0.703)] [G loss: 0.556] [G acc: 0.125]\n",
      "621 [D loss: 0.622(R 0.587, F0.657)] [D acc: 0.672(R 0.672, F 0.672)] [G loss: 0.622] [G acc: 0.141]\n",
      "622 [D loss: 0.581(R 0.657, F0.505)] [D acc: 0.750(R 0.625, F 0.875)] [G loss: 0.581] [G acc: 0.031]\n",
      "623 [D loss: 0.614(R 0.624, F0.605)] [D acc: 0.664(R 0.547, F 0.781)] [G loss: 0.614] [G acc: 0.094]\n",
      "624 [D loss: 0.536(R 0.541, F0.531)] [D acc: 0.703(R 0.641, F 0.766)] [G loss: 0.536] [G acc: 0.172]\n",
      "625 [D loss: 0.638(R 0.549, F0.727)] [D acc: 0.648(R 0.688, F 0.609)] [G loss: 0.638] [G acc: 0.156]\n",
      "626 [D loss: 0.589(R 0.503, F0.675)] [D acc: 0.695(R 0.719, F 0.672)] [G loss: 0.589] [G acc: 0.156]\n",
      "627 [D loss: 0.634(R 0.689, F0.580)] [D acc: 0.672(R 0.578, F 0.766)] [G loss: 0.634] [G acc: 0.156]\n",
      "628 [D loss: 0.603(R 0.634, F0.573)] [D acc: 0.641(R 0.562, F 0.719)] [G loss: 0.603] [G acc: 0.141]\n",
      "629 [D loss: 0.645(R 0.620, F0.669)] [D acc: 0.648(R 0.625, F 0.672)] [G loss: 0.645] [G acc: 0.156]\n",
      "630 [D loss: 0.519(R 0.525, F0.513)] [D acc: 0.750(R 0.703, F 0.797)] [G loss: 0.519] [G acc: 0.094]\n",
      "631 [D loss: 0.626(R 0.647, F0.605)] [D acc: 0.641(R 0.578, F 0.703)] [G loss: 0.626] [G acc: 0.125]\n",
      "632 [D loss: 0.610(R 0.589, F0.631)] [D acc: 0.695(R 0.688, F 0.703)] [G loss: 0.610] [G acc: 0.109]\n",
      "633 [D loss: 0.600(R 0.574, F0.626)] [D acc: 0.672(R 0.688, F 0.656)] [G loss: 0.600] [G acc: 0.109]\n",
      "634 [D loss: 0.587(R 0.608, F0.565)] [D acc: 0.680(R 0.609, F 0.750)] [G loss: 0.587] [G acc: 0.094]\n",
      "635 [D loss: 0.574(R 0.580, F0.568)] [D acc: 0.680(R 0.609, F 0.750)] [G loss: 0.574] [G acc: 0.078]\n",
      "636 [D loss: 0.580(R 0.483, F0.677)] [D acc: 0.727(R 0.781, F 0.672)] [G loss: 0.580] [G acc: 0.109]\n",
      "637 [D loss: 0.640(R 0.640, F0.640)] [D acc: 0.656(R 0.625, F 0.688)] [G loss: 0.640] [G acc: 0.141]\n",
      "638 [D loss: 0.642(R 0.665, F0.619)] [D acc: 0.672(R 0.562, F 0.781)] [G loss: 0.642] [G acc: 0.078]\n",
      "639 [D loss: 0.572(R 0.568, F0.575)] [D acc: 0.672(R 0.625, F 0.719)] [G loss: 0.572] [G acc: 0.078]\n",
      "640 [D loss: 0.688(R 0.693, F0.683)] [D acc: 0.594(R 0.500, F 0.688)] [G loss: 0.688] [G acc: 0.109]\n",
      "641 [D loss: 0.591(R 0.629, F0.554)] [D acc: 0.648(R 0.609, F 0.688)] [G loss: 0.591] [G acc: 0.094]\n",
      "642 [D loss: 0.589(R 0.587, F0.592)] [D acc: 0.688(R 0.656, F 0.719)] [G loss: 0.589] [G acc: 0.125]\n",
      "643 [D loss: 0.540(R 0.444, F0.636)] [D acc: 0.703(R 0.781, F 0.625)] [G loss: 0.540] [G acc: 0.109]\n",
      "644 [D loss: 0.652(R 0.656, F0.648)] [D acc: 0.633(R 0.594, F 0.672)] [G loss: 0.652] [G acc: 0.094]\n",
      "645 [D loss: 0.603(R 0.581, F0.626)] [D acc: 0.695(R 0.656, F 0.734)] [G loss: 0.603] [G acc: 0.125]\n",
      "646 [D loss: 0.624(R 0.653, F0.595)] [D acc: 0.641(R 0.500, F 0.781)] [G loss: 0.624] [G acc: 0.125]\n",
      "647 [D loss: 0.556(R 0.555, F0.558)] [D acc: 0.664(R 0.641, F 0.688)] [G loss: 0.556] [G acc: 0.031]\n",
      "648 [D loss: 0.537(R 0.513, F0.562)] [D acc: 0.750(R 0.703, F 0.797)] [G loss: 0.537] [G acc: 0.203]\n",
      "649 [D loss: 0.653(R 0.657, F0.649)] [D acc: 0.625(R 0.578, F 0.672)] [G loss: 0.653] [G acc: 0.078]\n",
      "650 [D loss: 0.488(R 0.532, F0.445)] [D acc: 0.820(R 0.703, F 0.938)] [G loss: 0.488] [G acc: 0.031]\n",
      "651 [D loss: 0.537(R 0.555, F0.519)] [D acc: 0.711(R 0.641, F 0.781)] [G loss: 0.537] [G acc: 0.031]\n",
      "652 [D loss: 0.703(R 0.621, F0.785)] [D acc: 0.617(R 0.641, F 0.594)] [G loss: 0.703] [G acc: 0.031]\n",
      "653 [D loss: 0.606(R 0.559, F0.653)] [D acc: 0.656(R 0.703, F 0.609)] [G loss: 0.606] [G acc: 0.125]\n",
      "654 [D loss: 0.605(R 0.653, F0.557)] [D acc: 0.625(R 0.516, F 0.734)] [G loss: 0.605] [G acc: 0.125]\n",
      "655 [D loss: 0.625(R 0.646, F0.604)] [D acc: 0.625(R 0.547, F 0.703)] [G loss: 0.625] [G acc: 0.109]\n",
      "656 [D loss: 0.597(R 0.536, F0.658)] [D acc: 0.680(R 0.688, F 0.672)] [G loss: 0.597] [G acc: 0.141]\n",
      "657 [D loss: 0.608(R 0.569, F0.648)] [D acc: 0.648(R 0.641, F 0.656)] [G loss: 0.608] [G acc: 0.156]\n",
      "658 [D loss: 0.528(R 0.520, F0.535)] [D acc: 0.758(R 0.750, F 0.766)] [G loss: 0.528] [G acc: 0.172]\n",
      "659 [D loss: 0.587(R 0.607, F0.567)] [D acc: 0.703(R 0.641, F 0.766)] [G loss: 0.587] [G acc: 0.234]\n",
      "660 [D loss: 0.632(R 0.620, F0.644)] [D acc: 0.656(R 0.609, F 0.703)] [G loss: 0.632] [G acc: 0.172]\n",
      "661 [D loss: 0.620(R 0.616, F0.624)] [D acc: 0.672(R 0.609, F 0.734)] [G loss: 0.620] [G acc: 0.094]\n",
      "662 [D loss: 0.614(R 0.563, F0.664)] [D acc: 0.680(R 0.656, F 0.703)] [G loss: 0.614] [G acc: 0.203]\n",
      "663 [D loss: 0.548(R 0.500, F0.596)] [D acc: 0.734(R 0.750, F 0.719)] [G loss: 0.548] [G acc: 0.156]\n",
      "664 [D loss: 0.583(R 0.536, F0.630)] [D acc: 0.664(R 0.672, F 0.656)] [G loss: 0.583] [G acc: 0.109]\n",
      "665 [D loss: 0.593(R 0.615, F0.570)] [D acc: 0.695(R 0.656, F 0.734)] [G loss: 0.593] [G acc: 0.141]\n",
      "666 [D loss: 0.620(R 0.671, F0.570)] [D acc: 0.695(R 0.531, F 0.859)] [G loss: 0.620] [G acc: 0.141]\n",
      "667 [D loss: 0.501(R 0.450, F0.552)] [D acc: 0.758(R 0.750, F 0.766)] [G loss: 0.501] [G acc: 0.141]\n",
      "668 [D loss: 0.555(R 0.519, F0.591)] [D acc: 0.695(R 0.656, F 0.734)] [G loss: 0.555] [G acc: 0.078]\n",
      "669 [D loss: 0.532(R 0.545, F0.519)] [D acc: 0.766(R 0.734, F 0.797)] [G loss: 0.532] [G acc: 0.094]\n",
      "670 [D loss: 0.686(R 0.651, F0.720)] [D acc: 0.609(R 0.562, F 0.656)] [G loss: 0.686] [G acc: 0.141]\n",
      "671 [D loss: 0.561(R 0.576, F0.545)] [D acc: 0.695(R 0.641, F 0.750)] [G loss: 0.561] [G acc: 0.094]\n",
      "672 [D loss: 0.555(R 0.500, F0.610)] [D acc: 0.703(R 0.734, F 0.672)] [G loss: 0.555] [G acc: 0.141]\n",
      "673 [D loss: 0.498(R 0.472, F0.523)] [D acc: 0.766(R 0.766, F 0.766)] [G loss: 0.498] [G acc: 0.078]\n",
      "674 [D loss: 0.623(R 0.665, F0.582)] [D acc: 0.625(R 0.500, F 0.750)] [G loss: 0.623] [G acc: 0.172]\n",
      "675 [D loss: 0.577(R 0.675, F0.480)] [D acc: 0.680(R 0.531, F 0.828)] [G loss: 0.577] [G acc: 0.094]\n",
      "676 [D loss: 0.590(R 0.554, F0.626)] [D acc: 0.711(R 0.719, F 0.703)] [G loss: 0.590] [G acc: 0.078]\n",
      "677 [D loss: 0.540(R 0.569, F0.512)] [D acc: 0.742(R 0.672, F 0.812)] [G loss: 0.540] [G acc: 0.078]\n",
      "678 [D loss: 0.594(R 0.619, F0.569)] [D acc: 0.672(R 0.625, F 0.719)] [G loss: 0.594] [G acc: 0.141]\n",
      "679 [D loss: 0.549(R 0.506, F0.593)] [D acc: 0.734(R 0.766, F 0.703)] [G loss: 0.549] [G acc: 0.047]\n",
      "680 [D loss: 0.553(R 0.653, F0.454)] [D acc: 0.711(R 0.547, F 0.875)] [G loss: 0.553] [G acc: 0.047]\n",
      "681 [D loss: 0.557(R 0.548, F0.565)] [D acc: 0.727(R 0.672, F 0.781)] [G loss: 0.557] [G acc: 0.031]\n",
      "682 [D loss: 0.680(R 0.644, F0.716)] [D acc: 0.609(R 0.625, F 0.594)] [G loss: 0.680] [G acc: 0.094]\n",
      "683 [D loss: 0.569(R 0.563, F0.575)] [D acc: 0.688(R 0.609, F 0.766)] [G loss: 0.569] [G acc: 0.125]\n",
      "684 [D loss: 0.609(R 0.533, F0.685)] [D acc: 0.719(R 0.688, F 0.750)] [G loss: 0.609] [G acc: 0.172]\n",
      "685 [D loss: 0.593(R 0.556, F0.630)] [D acc: 0.656(R 0.641, F 0.672)] [G loss: 0.593] [G acc: 0.234]\n",
      "686 [D loss: 0.647(R 0.638, F0.655)] [D acc: 0.602(R 0.578, F 0.625)] [G loss: 0.647] [G acc: 0.172]\n",
      "687 [D loss: 0.588(R 0.586, F0.590)] [D acc: 0.703(R 0.688, F 0.719)] [G loss: 0.588] [G acc: 0.188]\n",
      "688 [D loss: 0.597(R 0.557, F0.637)] [D acc: 0.672(R 0.641, F 0.703)] [G loss: 0.597] [G acc: 0.172]\n",
      "689 [D loss: 0.588(R 0.599, F0.576)] [D acc: 0.664(R 0.609, F 0.719)] [G loss: 0.588] [G acc: 0.156]\n",
      "690 [D loss: 0.634(R 0.644, F0.624)] [D acc: 0.672(R 0.625, F 0.719)] [G loss: 0.634] [G acc: 0.203]\n",
      "691 [D loss: 0.628(R 0.657, F0.598)] [D acc: 0.680(R 0.641, F 0.719)] [G loss: 0.628] [G acc: 0.125]\n",
      "692 [D loss: 0.615(R 0.618, F0.612)] [D acc: 0.680(R 0.562, F 0.797)] [G loss: 0.615] [G acc: 0.172]\n",
      "693 [D loss: 0.574(R 0.557, F0.591)] [D acc: 0.672(R 0.625, F 0.719)] [G loss: 0.574] [G acc: 0.109]\n",
      "694 [D loss: 0.595(R 0.597, F0.592)] [D acc: 0.672(R 0.609, F 0.734)] [G loss: 0.595] [G acc: 0.125]\n",
      "695 [D loss: 0.618(R 0.682, F0.554)] [D acc: 0.641(R 0.547, F 0.734)] [G loss: 0.618] [G acc: 0.156]\n",
      "696 [D loss: 0.661(R 0.686, F0.635)] [D acc: 0.641(R 0.578, F 0.703)] [G loss: 0.661] [G acc: 0.125]\n",
      "697 [D loss: 0.604(R 0.558, F0.651)] [D acc: 0.656(R 0.656, F 0.656)] [G loss: 0.604] [G acc: 0.188]\n",
      "698 [D loss: 0.570(R 0.586, F0.555)] [D acc: 0.656(R 0.594, F 0.719)] [G loss: 0.570] [G acc: 0.172]\n",
      "699 [D loss: 0.571(R 0.521, F0.620)] [D acc: 0.695(R 0.703, F 0.688)] [G loss: 0.571] [G acc: 0.062]\n",
      "INFO:tensorflow:Assets written to: ram://f164dea1-8089-4c0b-b091-c4a7226cd9d9/assets\n",
      "INFO:tensorflow:Assets written to: ram://0c99c9d1-1c10-4796-87f2-4ee2155a5e6a/assets\n",
      "INFO:tensorflow:Assets written to: ram://e930ee49-72da-4174-b45f-13d6a37d6130/assets\n",
      "700 [D loss: 0.587(R 0.597, F0.577)] [D acc: 0.656(R 0.578, F 0.734)] [G loss: 0.587] [G acc: 0.109]\n",
      "701 [D loss: 0.600(R 0.660, F0.541)] [D acc: 0.703(R 0.625, F 0.781)] [G loss: 0.600] [G acc: 0.094]\n",
      "702 [D loss: 0.606(R 0.625, F0.587)] [D acc: 0.688(R 0.578, F 0.797)] [G loss: 0.606] [G acc: 0.125]\n",
      "703 [D loss: 0.558(R 0.503, F0.614)] [D acc: 0.719(R 0.734, F 0.703)] [G loss: 0.558] [G acc: 0.125]\n",
      "704 [D loss: 0.619(R 0.614, F0.624)] [D acc: 0.656(R 0.609, F 0.703)] [G loss: 0.619] [G acc: 0.062]\n",
      "705 [D loss: 0.536(R 0.500, F0.573)] [D acc: 0.734(R 0.703, F 0.766)] [G loss: 0.536] [G acc: 0.156]\n",
      "706 [D loss: 0.578(R 0.564, F0.592)] [D acc: 0.695(R 0.656, F 0.734)] [G loss: 0.578] [G acc: 0.156]\n",
      "707 [D loss: 0.571(R 0.526, F0.617)] [D acc: 0.688(R 0.672, F 0.703)] [G loss: 0.571] [G acc: 0.125]\n",
      "708 [D loss: 0.651(R 0.686, F0.617)] [D acc: 0.633(R 0.547, F 0.719)] [G loss: 0.651] [G acc: 0.031]\n",
      "709 [D loss: 0.510(R 0.546, F0.473)] [D acc: 0.758(R 0.641, F 0.875)] [G loss: 0.510] [G acc: 0.203]\n",
      "710 [D loss: 0.593(R 0.560, F0.625)] [D acc: 0.680(R 0.672, F 0.688)] [G loss: 0.593] [G acc: 0.156]\n",
      "711 [D loss: 0.619(R 0.587, F0.652)] [D acc: 0.656(R 0.625, F 0.688)] [G loss: 0.619] [G acc: 0.094]\n",
      "712 [D loss: 0.554(R 0.577, F0.530)] [D acc: 0.727(R 0.688, F 0.766)] [G loss: 0.554] [G acc: 0.109]\n",
      "713 [D loss: 0.515(R 0.520, F0.510)] [D acc: 0.734(R 0.641, F 0.828)] [G loss: 0.515] [G acc: 0.234]\n",
      "714 [D loss: 0.586(R 0.566, F0.605)] [D acc: 0.641(R 0.562, F 0.719)] [G loss: 0.586] [G acc: 0.156]\n",
      "715 [D loss: 0.681(R 0.467, F0.894)] [D acc: 0.648(R 0.703, F 0.594)] [G loss: 0.681] [G acc: 0.016]\n",
      "716 [D loss: 0.591(R 0.564, F0.617)] [D acc: 0.672(R 0.641, F 0.703)] [G loss: 0.591] [G acc: 0.062]\n",
      "717 [D loss: 0.600(R 0.644, F0.557)] [D acc: 0.695(R 0.625, F 0.766)] [G loss: 0.600] [G acc: 0.078]\n",
      "718 [D loss: 0.571(R 0.564, F0.578)] [D acc: 0.703(R 0.641, F 0.766)] [G loss: 0.571] [G acc: 0.109]\n",
      "719 [D loss: 0.591(R 0.463, F0.720)] [D acc: 0.719(R 0.781, F 0.656)] [G loss: 0.591] [G acc: 0.125]\n",
      "720 [D loss: 0.700(R 0.611, F0.788)] [D acc: 0.586(R 0.609, F 0.562)] [G loss: 0.700] [G acc: 0.172]\n",
      "721 [D loss: 0.608(R 0.596, F0.621)] [D acc: 0.633(R 0.562, F 0.703)] [G loss: 0.608] [G acc: 0.094]\n",
      "722 [D loss: 0.589(R 0.671, F0.507)] [D acc: 0.680(R 0.531, F 0.828)] [G loss: 0.589] [G acc: 0.016]\n",
      "723 [D loss: 0.571(R 0.540, F0.603)] [D acc: 0.703(R 0.703, F 0.703)] [G loss: 0.571] [G acc: 0.094]\n",
      "724 [D loss: 0.676(R 0.606, F0.745)] [D acc: 0.570(R 0.562, F 0.578)] [G loss: 0.676] [G acc: 0.094]\n",
      "725 [D loss: 0.596(R 0.642, F0.549)] [D acc: 0.688(R 0.609, F 0.766)] [G loss: 0.596] [G acc: 0.141]\n",
      "726 [D loss: 0.548(R 0.549, F0.547)] [D acc: 0.719(R 0.672, F 0.766)] [G loss: 0.548] [G acc: 0.172]\n",
      "727 [D loss: 0.556(R 0.494, F0.618)] [D acc: 0.711(R 0.750, F 0.672)] [G loss: 0.556] [G acc: 0.078]\n",
      "728 [D loss: 0.549(R 0.586, F0.512)] [D acc: 0.711(R 0.609, F 0.812)] [G loss: 0.549] [G acc: 0.062]\n",
      "729 [D loss: 0.583(R 0.559, F0.608)] [D acc: 0.695(R 0.672, F 0.719)] [G loss: 0.583] [G acc: 0.094]\n",
      "730 [D loss: 0.568(R 0.627, F0.509)] [D acc: 0.734(R 0.656, F 0.812)] [G loss: 0.568] [G acc: 0.094]\n",
      "731 [D loss: 0.638(R 0.617, F0.658)] [D acc: 0.664(R 0.625, F 0.703)] [G loss: 0.638] [G acc: 0.031]\n",
      "732 [D loss: 0.621(R 0.594, F0.649)] [D acc: 0.672(R 0.688, F 0.656)] [G loss: 0.621] [G acc: 0.250]\n",
      "733 [D loss: 0.626(R 0.565, F0.688)] [D acc: 0.641(R 0.656, F 0.625)] [G loss: 0.626] [G acc: 0.141]\n",
      "734 [D loss: 0.550(R 0.569, F0.532)] [D acc: 0.766(R 0.703, F 0.828)] [G loss: 0.550] [G acc: 0.125]\n",
      "735 [D loss: 0.520(R 0.544, F0.497)] [D acc: 0.766(R 0.688, F 0.844)] [G loss: 0.520] [G acc: 0.109]\n",
      "736 [D loss: 0.664(R 0.557, F0.770)] [D acc: 0.617(R 0.672, F 0.562)] [G loss: 0.664] [G acc: 0.125]\n",
      "737 [D loss: 0.536(R 0.580, F0.493)] [D acc: 0.758(R 0.656, F 0.859)] [G loss: 0.536] [G acc: 0.125]\n",
      "738 [D loss: 0.597(R 0.539, F0.655)] [D acc: 0.648(R 0.641, F 0.656)] [G loss: 0.597] [G acc: 0.109]\n",
      "739 [D loss: 0.548(R 0.468, F0.627)] [D acc: 0.711(R 0.750, F 0.672)] [G loss: 0.548] [G acc: 0.078]\n",
      "740 [D loss: 0.615(R 0.691, F0.539)] [D acc: 0.672(R 0.562, F 0.781)] [G loss: 0.615] [G acc: 0.062]\n",
      "741 [D loss: 0.540(R 0.500, F0.580)] [D acc: 0.797(R 0.781, F 0.812)] [G loss: 0.540] [G acc: 0.156]\n",
      "742 [D loss: 0.633(R 0.645, F0.621)] [D acc: 0.609(R 0.547, F 0.672)] [G loss: 0.633] [G acc: 0.234]\n",
      "743 [D loss: 0.575(R 0.589, F0.562)] [D acc: 0.703(R 0.656, F 0.750)] [G loss: 0.575] [G acc: 0.094]\n",
      "744 [D loss: 0.563(R 0.512, F0.614)] [D acc: 0.711(R 0.703, F 0.719)] [G loss: 0.563] [G acc: 0.078]\n",
      "745 [D loss: 0.577(R 0.611, F0.544)] [D acc: 0.703(R 0.609, F 0.797)] [G loss: 0.577] [G acc: 0.172]\n",
      "746 [D loss: 0.535(R 0.534, F0.536)] [D acc: 0.797(R 0.719, F 0.875)] [G loss: 0.535] [G acc: 0.172]\n",
      "747 [D loss: 0.630(R 0.587, F0.672)] [D acc: 0.617(R 0.641, F 0.594)] [G loss: 0.630] [G acc: 0.078]\n",
      "748 [D loss: 0.547(R 0.639, F0.456)] [D acc: 0.734(R 0.609, F 0.859)] [G loss: 0.547] [G acc: 0.047]\n",
      "749 [D loss: 0.633(R 0.518, F0.748)] [D acc: 0.672(R 0.734, F 0.609)] [G loss: 0.633] [G acc: 0.094]\n",
      "750 [D loss: 0.550(R 0.526, F0.574)] [D acc: 0.711(R 0.719, F 0.703)] [G loss: 0.550] [G acc: 0.125]\n",
      "751 [D loss: 0.623(R 0.551, F0.695)] [D acc: 0.625(R 0.672, F 0.578)] [G loss: 0.623] [G acc: 0.156]\n",
      "752 [D loss: 0.552(R 0.519, F0.585)] [D acc: 0.742(R 0.734, F 0.750)] [G loss: 0.552] [G acc: 0.141]\n",
      "753 [D loss: 0.578(R 0.585, F0.571)] [D acc: 0.641(R 0.609, F 0.672)] [G loss: 0.578] [G acc: 0.219]\n",
      "754 [D loss: 0.602(R 0.536, F0.667)] [D acc: 0.672(R 0.703, F 0.641)] [G loss: 0.602] [G acc: 0.172]\n",
      "755 [D loss: 0.719(R 0.621, F0.816)] [D acc: 0.570(R 0.594, F 0.547)] [G loss: 0.719] [G acc: 0.109]\n",
      "756 [D loss: 0.605(R 0.621, F0.590)] [D acc: 0.672(R 0.609, F 0.734)] [G loss: 0.605] [G acc: 0.078]\n",
      "757 [D loss: 0.606(R 0.581, F0.630)] [D acc: 0.688(R 0.688, F 0.688)] [G loss: 0.606] [G acc: 0.156]\n",
      "758 [D loss: 0.606(R 0.598, F0.614)] [D acc: 0.688(R 0.641, F 0.734)] [G loss: 0.606] [G acc: 0.125]\n",
      "759 [D loss: 0.662(R 0.665, F0.660)] [D acc: 0.672(R 0.672, F 0.672)] [G loss: 0.662] [G acc: 0.172]\n",
      "760 [D loss: 0.603(R 0.646, F0.559)] [D acc: 0.664(R 0.562, F 0.766)] [G loss: 0.603] [G acc: 0.125]\n",
      "761 [D loss: 0.587(R 0.603, F0.571)] [D acc: 0.625(R 0.547, F 0.703)] [G loss: 0.587] [G acc: 0.109]\n",
      "762 [D loss: 0.593(R 0.609, F0.577)] [D acc: 0.695(R 0.656, F 0.734)] [G loss: 0.593] [G acc: 0.172]\n",
      "763 [D loss: 0.622(R 0.665, F0.579)] [D acc: 0.656(R 0.531, F 0.781)] [G loss: 0.622] [G acc: 0.062]\n",
      "764 [D loss: 0.548(R 0.579, F0.517)] [D acc: 0.727(R 0.656, F 0.797)] [G loss: 0.548] [G acc: 0.109]\n",
      "765 [D loss: 0.544(R 0.550, F0.538)] [D acc: 0.742(R 0.672, F 0.812)] [G loss: 0.544] [G acc: 0.156]\n",
      "766 [D loss: 0.559(R 0.480, F0.637)] [D acc: 0.750(R 0.797, F 0.703)] [G loss: 0.559] [G acc: 0.156]\n",
      "767 [D loss: 0.622(R 0.618, F0.626)] [D acc: 0.664(R 0.594, F 0.734)] [G loss: 0.622] [G acc: 0.141]\n",
      "768 [D loss: 0.533(R 0.520, F0.545)] [D acc: 0.734(R 0.734, F 0.734)] [G loss: 0.533] [G acc: 0.266]\n",
      "769 [D loss: 0.526(R 0.451, F0.601)] [D acc: 0.727(R 0.719, F 0.734)] [G loss: 0.526] [G acc: 0.125]\n",
      "770 [D loss: 0.633(R 0.602, F0.664)] [D acc: 0.656(R 0.656, F 0.656)] [G loss: 0.633] [G acc: 0.047]\n",
      "771 [D loss: 0.587(R 0.677, F0.497)] [D acc: 0.695(R 0.516, F 0.875)] [G loss: 0.587] [G acc: 0.094]\n",
      "772 [D loss: 0.572(R 0.584, F0.560)] [D acc: 0.734(R 0.688, F 0.781)] [G loss: 0.572] [G acc: 0.141]\n",
      "773 [D loss: 0.616(R 0.571, F0.660)] [D acc: 0.641(R 0.594, F 0.688)] [G loss: 0.616] [G acc: 0.109]\n",
      "774 [D loss: 0.611(R 0.631, F0.591)] [D acc: 0.625(R 0.547, F 0.703)] [G loss: 0.611] [G acc: 0.109]\n",
      "775 [D loss: 0.625(R 0.658, F0.592)] [D acc: 0.641(R 0.547, F 0.734)] [G loss: 0.625] [G acc: 0.094]\n",
      "776 [D loss: 0.586(R 0.610, F0.562)] [D acc: 0.688(R 0.641, F 0.734)] [G loss: 0.586] [G acc: 0.078]\n",
      "777 [D loss: 0.592(R 0.679, F0.505)] [D acc: 0.703(R 0.531, F 0.875)] [G loss: 0.592] [G acc: 0.078]\n",
      "778 [D loss: 0.477(R 0.442, F0.513)] [D acc: 0.812(R 0.828, F 0.797)] [G loss: 0.477] [G acc: 0.047]\n",
      "779 [D loss: 0.497(R 0.479, F0.516)] [D acc: 0.781(R 0.766, F 0.797)] [G loss: 0.497] [G acc: 0.094]\n",
      "780 [D loss: 0.665(R 0.634, F0.697)] [D acc: 0.602(R 0.516, F 0.688)] [G loss: 0.665] [G acc: 0.094]\n",
      "781 [D loss: 0.547(R 0.551, F0.543)] [D acc: 0.711(R 0.672, F 0.750)] [G loss: 0.547] [G acc: 0.141]\n",
      "782 [D loss: 0.641(R 0.511, F0.770)] [D acc: 0.680(R 0.703, F 0.656)] [G loss: 0.641] [G acc: 0.062]\n",
      "783 [D loss: 0.580(R 0.610, F0.550)] [D acc: 0.711(R 0.625, F 0.797)] [G loss: 0.580] [G acc: 0.078]\n",
      "784 [D loss: 0.527(R 0.555, F0.499)] [D acc: 0.703(R 0.609, F 0.797)] [G loss: 0.527] [G acc: 0.141]\n",
      "785 [D loss: 0.575(R 0.551, F0.599)] [D acc: 0.680(R 0.625, F 0.734)] [G loss: 0.575] [G acc: 0.094]\n",
      "786 [D loss: 0.598(R 0.594, F0.602)] [D acc: 0.641(R 0.625, F 0.656)] [G loss: 0.598] [G acc: 0.094]\n",
      "787 [D loss: 0.646(R 0.682, F0.610)] [D acc: 0.602(R 0.500, F 0.703)] [G loss: 0.646] [G acc: 0.016]\n",
      "788 [D loss: 0.685(R 0.673, F0.697)] [D acc: 0.539(R 0.484, F 0.594)] [G loss: 0.685] [G acc: 0.125]\n",
      "789 [D loss: 0.578(R 0.550, F0.607)] [D acc: 0.703(R 0.703, F 0.703)] [G loss: 0.578] [G acc: 0.141]\n",
      "790 [D loss: 0.640(R 0.631, F0.650)] [D acc: 0.602(R 0.531, F 0.672)] [G loss: 0.640] [G acc: 0.094]\n",
      "791 [D loss: 0.652(R 0.626, F0.679)] [D acc: 0.664(R 0.609, F 0.719)] [G loss: 0.652] [G acc: 0.141]\n",
      "792 [D loss: 0.529(R 0.563, F0.495)] [D acc: 0.727(R 0.594, F 0.859)] [G loss: 0.529] [G acc: 0.172]\n",
      "793 [D loss: 0.562(R 0.526, F0.597)] [D acc: 0.742(R 0.703, F 0.781)] [G loss: 0.562] [G acc: 0.078]\n",
      "794 [D loss: 0.557(R 0.592, F0.522)] [D acc: 0.703(R 0.609, F 0.797)] [G loss: 0.557] [G acc: 0.047]\n",
      "795 [D loss: 0.666(R 0.557, F0.775)] [D acc: 0.602(R 0.672, F 0.531)] [G loss: 0.666] [G acc: 0.141]\n",
      "796 [D loss: 0.540(R 0.601, F0.480)] [D acc: 0.750(R 0.656, F 0.844)] [G loss: 0.540] [G acc: 0.031]\n",
      "797 [D loss: 0.591(R 0.456, F0.726)] [D acc: 0.711(R 0.766, F 0.656)] [G loss: 0.591] [G acc: 0.094]\n",
      "798 [D loss: 0.648(R 0.653, F0.643)] [D acc: 0.625(R 0.625, F 0.625)] [G loss: 0.648] [G acc: 0.078]\n",
      "799 [D loss: 0.551(R 0.523, F0.578)] [D acc: 0.719(R 0.703, F 0.734)] [G loss: 0.551] [G acc: 0.141]\n",
      "INFO:tensorflow:Assets written to: ram://37bc7afd-f7ce-4680-9d19-8a1016344343/assets\n",
      "INFO:tensorflow:Assets written to: ram://93e98eb2-45d8-45ef-a062-a9ccfe8edbd1/assets\n",
      "INFO:tensorflow:Assets written to: ram://ddf866cf-12a0-44b2-8351-3a9045cb88b8/assets\n",
      "800 [D loss: 0.635(R 0.648, F0.622)] [D acc: 0.633(R 0.609, F 0.656)] [G loss: 0.635] [G acc: 0.078]\n",
      "801 [D loss: 0.586(R 0.687, F0.485)] [D acc: 0.711(R 0.531, F 0.891)] [G loss: 0.586] [G acc: 0.047]\n",
      "802 [D loss: 0.573(R 0.524, F0.622)] [D acc: 0.688(R 0.672, F 0.703)] [G loss: 0.573] [G acc: 0.125]\n",
      "803 [D loss: 0.606(R 0.606, F0.607)] [D acc: 0.656(R 0.594, F 0.719)] [G loss: 0.606] [G acc: 0.188]\n",
      "804 [D loss: 0.634(R 0.595, F0.673)] [D acc: 0.641(R 0.609, F 0.672)] [G loss: 0.634] [G acc: 0.234]\n",
      "805 [D loss: 0.519(R 0.519, F0.518)] [D acc: 0.758(R 0.734, F 0.781)] [G loss: 0.519] [G acc: 0.109]\n",
      "806 [D loss: 0.613(R 0.630, F0.596)] [D acc: 0.672(R 0.594, F 0.750)] [G loss: 0.613] [G acc: 0.156]\n",
      "807 [D loss: 0.563(R 0.555, F0.572)] [D acc: 0.672(R 0.609, F 0.734)] [G loss: 0.563] [G acc: 0.062]\n",
      "808 [D loss: 0.621(R 0.648, F0.594)] [D acc: 0.688(R 0.594, F 0.781)] [G loss: 0.621] [G acc: 0.078]\n",
      "809 [D loss: 0.584(R 0.614, F0.554)] [D acc: 0.680(R 0.578, F 0.781)] [G loss: 0.584] [G acc: 0.156]\n",
      "810 [D loss: 0.577(R 0.575, F0.578)] [D acc: 0.711(R 0.656, F 0.766)] [G loss: 0.577] [G acc: 0.203]\n",
      "811 [D loss: 0.578(R 0.485, F0.671)] [D acc: 0.664(R 0.703, F 0.625)] [G loss: 0.578] [G acc: 0.156]\n",
      "812 [D loss: 0.633(R 0.650, F0.616)] [D acc: 0.664(R 0.609, F 0.719)] [G loss: 0.633] [G acc: 0.078]\n",
      "813 [D loss: 0.534(R 0.555, F0.512)] [D acc: 0.719(R 0.609, F 0.828)] [G loss: 0.534] [G acc: 0.156]\n",
      "814 [D loss: 0.554(R 0.536, F0.573)] [D acc: 0.695(R 0.672, F 0.719)] [G loss: 0.554] [G acc: 0.172]\n",
      "815 [D loss: 0.558(R 0.523, F0.592)] [D acc: 0.672(R 0.625, F 0.719)] [G loss: 0.558] [G acc: 0.156]\n",
      "816 [D loss: 0.676(R 0.694, F0.658)] [D acc: 0.625(R 0.562, F 0.688)] [G loss: 0.676] [G acc: 0.078]\n",
      "817 [D loss: 0.610(R 0.640, F0.580)] [D acc: 0.688(R 0.578, F 0.797)] [G loss: 0.610] [G acc: 0.078]\n",
      "818 [D loss: 0.558(R 0.550, F0.567)] [D acc: 0.742(R 0.750, F 0.734)] [G loss: 0.558] [G acc: 0.125]\n",
      "819 [D loss: 0.547(R 0.556, F0.538)] [D acc: 0.766(R 0.703, F 0.828)] [G loss: 0.547] [G acc: 0.156]\n",
      "820 [D loss: 0.647(R 0.660, F0.635)] [D acc: 0.656(R 0.531, F 0.781)] [G loss: 0.647] [G acc: 0.188]\n",
      "821 [D loss: 0.607(R 0.577, F0.636)] [D acc: 0.688(R 0.625, F 0.750)] [G loss: 0.607] [G acc: 0.188]\n",
      "822 [D loss: 0.555(R 0.544, F0.566)] [D acc: 0.727(R 0.719, F 0.734)] [G loss: 0.555] [G acc: 0.031]\n",
      "823 [D loss: 0.589(R 0.580, F0.599)] [D acc: 0.719(R 0.641, F 0.797)] [G loss: 0.589] [G acc: 0.094]\n",
      "824 [D loss: 0.594(R 0.556, F0.632)] [D acc: 0.680(R 0.719, F 0.641)] [G loss: 0.594] [G acc: 0.109]\n",
      "825 [D loss: 0.568(R 0.530, F0.605)] [D acc: 0.688(R 0.641, F 0.734)] [G loss: 0.568] [G acc: 0.156]\n",
      "826 [D loss: 0.594(R 0.579, F0.609)] [D acc: 0.695(R 0.641, F 0.750)] [G loss: 0.594] [G acc: 0.141]\n",
      "827 [D loss: 0.644(R 0.574, F0.713)] [D acc: 0.641(R 0.656, F 0.625)] [G loss: 0.644] [G acc: 0.125]\n",
      "828 [D loss: 0.585(R 0.588, F0.581)] [D acc: 0.719(R 0.625, F 0.812)] [G loss: 0.585] [G acc: 0.141]\n",
      "829 [D loss: 0.626(R 0.658, F0.595)] [D acc: 0.672(R 0.625, F 0.719)] [G loss: 0.626] [G acc: 0.156]\n",
      "830 [D loss: 0.585(R 0.491, F0.679)] [D acc: 0.695(R 0.719, F 0.672)] [G loss: 0.585] [G acc: 0.094]\n",
      "831 [D loss: 0.582(R 0.608, F0.556)] [D acc: 0.664(R 0.578, F 0.750)] [G loss: 0.582] [G acc: 0.219]\n",
      "832 [D loss: 0.608(R 0.566, F0.650)] [D acc: 0.695(R 0.672, F 0.719)] [G loss: 0.608] [G acc: 0.172]\n",
      "833 [D loss: 0.551(R 0.613, F0.490)] [D acc: 0.750(R 0.625, F 0.875)] [G loss: 0.551] [G acc: 0.094]\n",
      "834 [D loss: 0.656(R 0.645, F0.666)] [D acc: 0.680(R 0.625, F 0.734)] [G loss: 0.656] [G acc: 0.203]\n",
      "835 [D loss: 0.556(R 0.558, F0.554)] [D acc: 0.734(R 0.688, F 0.781)] [G loss: 0.556] [G acc: 0.203]\n",
      "836 [D loss: 0.533(R 0.500, F0.565)] [D acc: 0.703(R 0.719, F 0.688)] [G loss: 0.533] [G acc: 0.203]\n",
      "837 [D loss: 0.536(R 0.505, F0.567)] [D acc: 0.727(R 0.703, F 0.750)] [G loss: 0.536] [G acc: 0.062]\n",
      "838 [D loss: 0.631(R 0.595, F0.666)] [D acc: 0.617(R 0.609, F 0.625)] [G loss: 0.631] [G acc: 0.172]\n",
      "839 [D loss: 0.649(R 0.669, F0.628)] [D acc: 0.633(R 0.609, F 0.656)] [G loss: 0.649] [G acc: 0.109]\n",
      "840 [D loss: 0.648(R 0.665, F0.631)] [D acc: 0.578(R 0.516, F 0.641)] [G loss: 0.648] [G acc: 0.109]\n",
      "841 [D loss: 0.560(R 0.524, F0.596)] [D acc: 0.695(R 0.719, F 0.672)] [G loss: 0.560] [G acc: 0.141]\n",
      "842 [D loss: 0.604(R 0.652, F0.556)] [D acc: 0.641(R 0.578, F 0.703)] [G loss: 0.604] [G acc: 0.234]\n",
      "843 [D loss: 0.578(R 0.501, F0.656)] [D acc: 0.688(R 0.719, F 0.656)] [G loss: 0.578] [G acc: 0.141]\n",
      "844 [D loss: 0.651(R 0.722, F0.580)] [D acc: 0.648(R 0.562, F 0.734)] [G loss: 0.651] [G acc: 0.125]\n",
      "845 [D loss: 0.641(R 0.626, F0.656)] [D acc: 0.555(R 0.562, F 0.547)] [G loss: 0.641] [G acc: 0.141]\n",
      "846 [D loss: 0.614(R 0.605, F0.623)] [D acc: 0.648(R 0.578, F 0.719)] [G loss: 0.614] [G acc: 0.219]\n",
      "847 [D loss: 0.610(R 0.531, F0.689)] [D acc: 0.656(R 0.656, F 0.656)] [G loss: 0.610] [G acc: 0.234]\n",
      "848 [D loss: 0.536(R 0.550, F0.521)] [D acc: 0.766(R 0.703, F 0.828)] [G loss: 0.536] [G acc: 0.141]\n",
      "849 [D loss: 0.605(R 0.632, F0.578)] [D acc: 0.695(R 0.641, F 0.750)] [G loss: 0.605] [G acc: 0.141]\n",
      "850 [D loss: 0.623(R 0.653, F0.594)] [D acc: 0.688(R 0.578, F 0.797)] [G loss: 0.623] [G acc: 0.078]\n",
      "851 [D loss: 0.483(R 0.496, F0.469)] [D acc: 0.805(R 0.766, F 0.844)] [G loss: 0.483] [G acc: 0.094]\n",
      "852 [D loss: 0.530(R 0.539, F0.520)] [D acc: 0.758(R 0.734, F 0.781)] [G loss: 0.530] [G acc: 0.141]\n",
      "853 [D loss: 0.575(R 0.530, F0.619)] [D acc: 0.695(R 0.672, F 0.719)] [G loss: 0.575] [G acc: 0.125]\n",
      "854 [D loss: 0.551(R 0.488, F0.613)] [D acc: 0.742(R 0.781, F 0.703)] [G loss: 0.551] [G acc: 0.141]\n",
      "855 [D loss: 0.603(R 0.544, F0.663)] [D acc: 0.656(R 0.688, F 0.625)] [G loss: 0.603] [G acc: 0.172]\n",
      "856 [D loss: 0.645(R 0.816, F0.474)] [D acc: 0.680(R 0.422, F 0.938)] [G loss: 0.645] [G acc: 0.094]\n",
      "857 [D loss: 0.608(R 0.534, F0.681)] [D acc: 0.641(R 0.641, F 0.641)] [G loss: 0.608] [G acc: 0.109]\n",
      "858 [D loss: 0.618(R 0.622, F0.614)] [D acc: 0.625(R 0.531, F 0.719)] [G loss: 0.618] [G acc: 0.078]\n",
      "859 [D loss: 0.581(R 0.591, F0.570)] [D acc: 0.664(R 0.625, F 0.703)] [G loss: 0.581] [G acc: 0.109]\n",
      "860 [D loss: 0.630(R 0.633, F0.627)] [D acc: 0.648(R 0.578, F 0.719)] [G loss: 0.630] [G acc: 0.078]\n",
      "861 [D loss: 0.552(R 0.586, F0.518)] [D acc: 0.758(R 0.656, F 0.859)] [G loss: 0.552] [G acc: 0.141]\n",
      "862 [D loss: 0.624(R 0.634, F0.614)] [D acc: 0.609(R 0.484, F 0.734)] [G loss: 0.624] [G acc: 0.078]\n",
      "863 [D loss: 0.514(R 0.562, F0.465)] [D acc: 0.734(R 0.641, F 0.828)] [G loss: 0.514] [G acc: 0.094]\n",
      "864 [D loss: 0.594(R 0.574, F0.613)] [D acc: 0.688(R 0.641, F 0.734)] [G loss: 0.594] [G acc: 0.047]\n",
      "865 [D loss: 0.552(R 0.671, F0.434)] [D acc: 0.734(R 0.547, F 0.922)] [G loss: 0.552] [G acc: 0.109]\n",
      "866 [D loss: 0.510(R 0.473, F0.546)] [D acc: 0.797(R 0.781, F 0.812)] [G loss: 0.510] [G acc: 0.062]\n",
      "867 [D loss: 0.574(R 0.562, F0.585)] [D acc: 0.695(R 0.688, F 0.703)] [G loss: 0.574] [G acc: 0.188]\n",
      "868 [D loss: 0.621(R 0.614, F0.628)] [D acc: 0.633(R 0.641, F 0.625)] [G loss: 0.621] [G acc: 0.219]\n",
      "869 [D loss: 0.657(R 0.690, F0.624)] [D acc: 0.617(R 0.594, F 0.641)] [G loss: 0.657] [G acc: 0.109]\n",
      "870 [D loss: 0.607(R 0.551, F0.664)] [D acc: 0.711(R 0.719, F 0.703)] [G loss: 0.607] [G acc: 0.047]\n",
      "871 [D loss: 0.599(R 0.547, F0.652)] [D acc: 0.703(R 0.719, F 0.688)] [G loss: 0.599] [G acc: 0.125]\n",
      "872 [D loss: 0.569(R 0.539, F0.598)] [D acc: 0.688(R 0.625, F 0.750)] [G loss: 0.569] [G acc: 0.141]\n",
      "873 [D loss: 0.611(R 0.655, F0.568)] [D acc: 0.633(R 0.484, F 0.781)] [G loss: 0.611] [G acc: 0.125]\n",
      "874 [D loss: 0.524(R 0.559, F0.489)] [D acc: 0.758(R 0.688, F 0.828)] [G loss: 0.524] [G acc: 0.109]\n",
      "875 [D loss: 0.596(R 0.585, F0.607)] [D acc: 0.641(R 0.609, F 0.672)] [G loss: 0.596] [G acc: 0.125]\n",
      "876 [D loss: 0.606(R 0.534, F0.677)] [D acc: 0.672(R 0.703, F 0.641)] [G loss: 0.606] [G acc: 0.219]\n",
      "877 [D loss: 0.639(R 0.577, F0.702)] [D acc: 0.664(R 0.641, F 0.688)] [G loss: 0.639] [G acc: 0.094]\n",
      "878 [D loss: 0.658(R 0.633, F0.684)] [D acc: 0.578(R 0.484, F 0.672)] [G loss: 0.658] [G acc: 0.062]\n",
      "879 [D loss: 0.549(R 0.556, F0.542)] [D acc: 0.734(R 0.672, F 0.797)] [G loss: 0.549] [G acc: 0.156]\n",
      "880 [D loss: 0.488(R 0.510, F0.467)] [D acc: 0.797(R 0.734, F 0.859)] [G loss: 0.488] [G acc: 0.219]\n",
      "881 [D loss: 0.557(R 0.524, F0.590)] [D acc: 0.734(R 0.656, F 0.812)] [G loss: 0.557] [G acc: 0.172]\n",
      "882 [D loss: 0.500(R 0.476, F0.524)] [D acc: 0.750(R 0.750, F 0.750)] [G loss: 0.500] [G acc: 0.125]\n",
      "883 [D loss: 0.511(R 0.600, F0.422)] [D acc: 0.781(R 0.672, F 0.891)] [G loss: 0.511] [G acc: 0.109]\n",
      "884 [D loss: 0.534(R 0.495, F0.574)] [D acc: 0.727(R 0.734, F 0.719)] [G loss: 0.534] [G acc: 0.203]\n",
      "885 [D loss: 0.676(R 0.754, F0.598)] [D acc: 0.609(R 0.516, F 0.703)] [G loss: 0.676] [G acc: 0.156]\n",
      "886 [D loss: 0.601(R 0.509, F0.694)] [D acc: 0.688(R 0.656, F 0.719)] [G loss: 0.601] [G acc: 0.188]\n",
      "887 [D loss: 0.513(R 0.518, F0.507)] [D acc: 0.781(R 0.719, F 0.844)] [G loss: 0.513] [G acc: 0.141]\n",
      "888 [D loss: 0.635(R 0.603, F0.668)] [D acc: 0.688(R 0.672, F 0.703)] [G loss: 0.635] [G acc: 0.219]\n",
      "889 [D loss: 0.584(R 0.553, F0.615)] [D acc: 0.648(R 0.609, F 0.688)] [G loss: 0.584] [G acc: 0.047]\n",
      "890 [D loss: 0.598(R 0.670, F0.526)] [D acc: 0.609(R 0.484, F 0.734)] [G loss: 0.598] [G acc: 0.094]\n",
      "891 [D loss: 0.528(R 0.545, F0.511)] [D acc: 0.734(R 0.672, F 0.797)] [G loss: 0.528] [G acc: 0.062]\n",
      "892 [D loss: 0.610(R 0.495, F0.726)] [D acc: 0.680(R 0.734, F 0.625)] [G loss: 0.610] [G acc: 0.141]\n",
      "893 [D loss: 0.549(R 0.495, F0.603)] [D acc: 0.703(R 0.719, F 0.688)] [G loss: 0.549] [G acc: 0.125]\n",
      "894 [D loss: 0.636(R 0.555, F0.718)] [D acc: 0.633(R 0.672, F 0.594)] [G loss: 0.636] [G acc: 0.172]\n",
      "895 [D loss: 0.662(R 0.608, F0.716)] [D acc: 0.703(R 0.688, F 0.719)] [G loss: 0.662] [G acc: 0.172]\n",
      "896 [D loss: 0.563(R 0.594, F0.533)] [D acc: 0.719(R 0.625, F 0.812)] [G loss: 0.563] [G acc: 0.141]\n",
      "897 [D loss: 0.552(R 0.591, F0.512)] [D acc: 0.711(R 0.641, F 0.781)] [G loss: 0.552] [G acc: 0.188]\n",
      "898 [D loss: 0.549(R 0.548, F0.550)] [D acc: 0.758(R 0.703, F 0.812)] [G loss: 0.549] [G acc: 0.109]\n",
      "899 [D loss: 0.631(R 0.633, F0.630)] [D acc: 0.648(R 0.609, F 0.688)] [G loss: 0.631] [G acc: 0.203]\n",
      "INFO:tensorflow:Assets written to: ram://cbf2ea15-4ce8-4932-bebd-616ef3daedd5/assets\n",
      "INFO:tensorflow:Assets written to: ram://dd3625ee-beb7-4d2a-bb26-53cf087f3cfe/assets\n",
      "INFO:tensorflow:Assets written to: ram://4628d1c4-ad2b-457a-9f74-88b2bb99356f/assets\n",
      "900 [D loss: 0.612(R 0.624, F0.600)] [D acc: 0.680(R 0.594, F 0.766)] [G loss: 0.612] [G acc: 0.156]\n",
      "901 [D loss: 0.553(R 0.527, F0.580)] [D acc: 0.695(R 0.688, F 0.703)] [G loss: 0.553] [G acc: 0.188]\n",
      "902 [D loss: 0.572(R 0.526, F0.618)] [D acc: 0.664(R 0.609, F 0.719)] [G loss: 0.572] [G acc: 0.109]\n",
      "903 [D loss: 0.610(R 0.576, F0.645)] [D acc: 0.680(R 0.641, F 0.719)] [G loss: 0.610] [G acc: 0.250]\n",
      "904 [D loss: 0.552(R 0.630, F0.474)] [D acc: 0.711(R 0.625, F 0.797)] [G loss: 0.552] [G acc: 0.125]\n",
      "905 [D loss: 0.613(R 0.593, F0.632)] [D acc: 0.625(R 0.609, F 0.641)] [G loss: 0.613] [G acc: 0.188]\n",
      "906 [D loss: 0.591(R 0.585, F0.596)] [D acc: 0.672(R 0.641, F 0.703)] [G loss: 0.591] [G acc: 0.078]\n",
      "907 [D loss: 0.617(R 0.534, F0.700)] [D acc: 0.625(R 0.672, F 0.578)] [G loss: 0.617] [G acc: 0.219]\n",
      "908 [D loss: 0.642(R 0.651, F0.632)] [D acc: 0.633(R 0.578, F 0.688)] [G loss: 0.642] [G acc: 0.172]\n",
      "909 [D loss: 0.578(R 0.566, F0.590)] [D acc: 0.664(R 0.641, F 0.688)] [G loss: 0.578] [G acc: 0.156]\n",
      "910 [D loss: 0.608(R 0.598, F0.619)] [D acc: 0.641(R 0.562, F 0.719)] [G loss: 0.608] [G acc: 0.141]\n",
      "911 [D loss: 0.534(R 0.519, F0.549)] [D acc: 0.758(R 0.688, F 0.828)] [G loss: 0.534] [G acc: 0.078]\n",
      "912 [D loss: 0.594(R 0.573, F0.615)] [D acc: 0.680(R 0.625, F 0.734)] [G loss: 0.594] [G acc: 0.125]\n",
      "913 [D loss: 0.451(R 0.387, F0.515)] [D acc: 0.781(R 0.828, F 0.734)] [G loss: 0.451] [G acc: 0.031]\n",
      "914 [D loss: 0.519(R 0.590, F0.449)] [D acc: 0.781(R 0.703, F 0.859)] [G loss: 0.519] [G acc: 0.078]\n",
      "915 [D loss: 0.549(R 0.529, F0.568)] [D acc: 0.734(R 0.703, F 0.766)] [G loss: 0.549] [G acc: 0.156]\n",
      "916 [D loss: 0.575(R 0.552, F0.599)] [D acc: 0.742(R 0.703, F 0.781)] [G loss: 0.575] [G acc: 0.188]\n",
      "917 [D loss: 0.611(R 0.720, F0.501)] [D acc: 0.703(R 0.562, F 0.844)] [G loss: 0.611] [G acc: 0.203]\n",
      "918 [D loss: 0.639(R 0.654, F0.623)] [D acc: 0.641(R 0.547, F 0.734)] [G loss: 0.639] [G acc: 0.062]\n",
      "919 [D loss: 0.598(R 0.622, F0.573)] [D acc: 0.742(R 0.688, F 0.797)] [G loss: 0.598] [G acc: 0.078]\n",
      "920 [D loss: 0.607(R 0.509, F0.704)] [D acc: 0.711(R 0.719, F 0.703)] [G loss: 0.607] [G acc: 0.156]\n",
      "921 [D loss: 0.576(R 0.548, F0.605)] [D acc: 0.688(R 0.703, F 0.672)] [G loss: 0.576] [G acc: 0.156]\n",
      "922 [D loss: 0.614(R 0.675, F0.553)] [D acc: 0.633(R 0.484, F 0.781)] [G loss: 0.614] [G acc: 0.094]\n",
      "923 [D loss: 0.552(R 0.623, F0.482)] [D acc: 0.797(R 0.672, F 0.922)] [G loss: 0.552] [G acc: 0.047]\n",
      "924 [D loss: 0.612(R 0.615, F0.609)] [D acc: 0.711(R 0.625, F 0.797)] [G loss: 0.612] [G acc: 0.094]\n",
      "925 [D loss: 0.547(R 0.557, F0.537)] [D acc: 0.727(R 0.641, F 0.812)] [G loss: 0.547] [G acc: 0.172]\n",
      "926 [D loss: 0.569(R 0.479, F0.659)] [D acc: 0.727(R 0.734, F 0.719)] [G loss: 0.569] [G acc: 0.094]\n",
      "927 [D loss: 0.547(R 0.532, F0.563)] [D acc: 0.742(R 0.734, F 0.750)] [G loss: 0.547] [G acc: 0.062]\n",
      "928 [D loss: 0.617(R 0.624, F0.610)] [D acc: 0.633(R 0.609, F 0.656)] [G loss: 0.617] [G acc: 0.078]\n",
      "929 [D loss: 0.565(R 0.632, F0.498)] [D acc: 0.703(R 0.578, F 0.828)] [G loss: 0.565] [G acc: 0.156]\n",
      "930 [D loss: 0.668(R 0.591, F0.745)] [D acc: 0.578(R 0.594, F 0.562)] [G loss: 0.668] [G acc: 0.125]\n",
      "931 [D loss: 0.516(R 0.550, F0.481)] [D acc: 0.805(R 0.703, F 0.906)] [G loss: 0.516] [G acc: 0.109]\n",
      "932 [D loss: 0.619(R 0.622, F0.616)] [D acc: 0.656(R 0.641, F 0.672)] [G loss: 0.619] [G acc: 0.078]\n",
      "933 [D loss: 0.573(R 0.510, F0.637)] [D acc: 0.734(R 0.750, F 0.719)] [G loss: 0.573] [G acc: 0.172]\n",
      "934 [D loss: 0.563(R 0.471, F0.655)] [D acc: 0.688(R 0.766, F 0.609)] [G loss: 0.563] [G acc: 0.125]\n",
      "935 [D loss: 0.544(R 0.544, F0.543)] [D acc: 0.711(R 0.641, F 0.781)] [G loss: 0.544] [G acc: 0.109]\n",
      "936 [D loss: 0.610(R 0.659, F0.562)] [D acc: 0.680(R 0.594, F 0.766)] [G loss: 0.610] [G acc: 0.156]\n",
      "937 [D loss: 0.582(R 0.561, F0.602)] [D acc: 0.633(R 0.609, F 0.656)] [G loss: 0.582] [G acc: 0.062]\n",
      "938 [D loss: 0.639(R 0.639, F0.639)] [D acc: 0.648(R 0.578, F 0.719)] [G loss: 0.639] [G acc: 0.141]\n",
      "939 [D loss: 0.646(R 0.648, F0.644)] [D acc: 0.648(R 0.594, F 0.703)] [G loss: 0.646] [G acc: 0.172]\n",
      "940 [D loss: 0.615(R 0.637, F0.593)] [D acc: 0.641(R 0.562, F 0.719)] [G loss: 0.615] [G acc: 0.047]\n",
      "941 [D loss: 0.600(R 0.710, F0.490)] [D acc: 0.719(R 0.594, F 0.844)] [G loss: 0.600] [G acc: 0.078]\n",
      "942 [D loss: 0.524(R 0.532, F0.515)] [D acc: 0.719(R 0.656, F 0.781)] [G loss: 0.524] [G acc: 0.141]\n",
      "943 [D loss: 0.587(R 0.566, F0.608)] [D acc: 0.695(R 0.625, F 0.766)] [G loss: 0.587] [G acc: 0.172]\n",
      "944 [D loss: 0.649(R 0.497, F0.801)] [D acc: 0.680(R 0.734, F 0.625)] [G loss: 0.649] [G acc: 0.125]\n",
      "945 [D loss: 0.509(R 0.502, F0.516)] [D acc: 0.742(R 0.719, F 0.766)] [G loss: 0.509] [G acc: 0.094]\n",
      "946 [D loss: 0.594(R 0.638, F0.550)] [D acc: 0.664(R 0.594, F 0.734)] [G loss: 0.594] [G acc: 0.125]\n",
      "947 [D loss: 0.627(R 0.663, F0.592)] [D acc: 0.656(R 0.547, F 0.766)] [G loss: 0.627] [G acc: 0.078]\n",
      "948 [D loss: 0.558(R 0.574, F0.542)] [D acc: 0.719(R 0.625, F 0.812)] [G loss: 0.558] [G acc: 0.141]\n",
      "949 [D loss: 0.599(R 0.607, F0.591)] [D acc: 0.664(R 0.562, F 0.766)] [G loss: 0.599] [G acc: 0.062]\n",
      "950 [D loss: 0.579(R 0.579, F0.578)] [D acc: 0.727(R 0.688, F 0.766)] [G loss: 0.579] [G acc: 0.078]\n",
      "951 [D loss: 0.549(R 0.554, F0.544)] [D acc: 0.711(R 0.641, F 0.781)] [G loss: 0.549] [G acc: 0.109]\n",
      "952 [D loss: 0.563(R 0.602, F0.524)] [D acc: 0.773(R 0.719, F 0.828)] [G loss: 0.563] [G acc: 0.203]\n",
      "953 [D loss: 0.516(R 0.487, F0.545)] [D acc: 0.727(R 0.688, F 0.766)] [G loss: 0.516] [G acc: 0.188]\n",
      "954 [D loss: 0.595(R 0.559, F0.631)] [D acc: 0.648(R 0.656, F 0.641)] [G loss: 0.595] [G acc: 0.203]\n",
      "955 [D loss: 0.527(R 0.519, F0.534)] [D acc: 0.750(R 0.672, F 0.828)] [G loss: 0.527] [G acc: 0.219]\n",
      "956 [D loss: 0.610(R 0.608, F0.613)] [D acc: 0.680(R 0.641, F 0.719)] [G loss: 0.610] [G acc: 0.234]\n",
      "957 [D loss: 0.601(R 0.526, F0.676)] [D acc: 0.672(R 0.656, F 0.688)] [G loss: 0.601] [G acc: 0.172]\n",
      "958 [D loss: 0.552(R 0.475, F0.630)] [D acc: 0.734(R 0.734, F 0.734)] [G loss: 0.552] [G acc: 0.188]\n",
      "959 [D loss: 0.623(R 0.574, F0.673)] [D acc: 0.625(R 0.641, F 0.609)] [G loss: 0.623] [G acc: 0.078]\n",
      "960 [D loss: 0.668(R 0.827, F0.509)] [D acc: 0.617(R 0.391, F 0.844)] [G loss: 0.668] [G acc: 0.172]\n",
      "961 [D loss: 0.609(R 0.579, F0.638)] [D acc: 0.688(R 0.609, F 0.766)] [G loss: 0.609] [G acc: 0.203]\n",
      "962 [D loss: 0.552(R 0.539, F0.566)] [D acc: 0.695(R 0.672, F 0.719)] [G loss: 0.552] [G acc: 0.109]\n",
      "963 [D loss: 0.574(R 0.639, F0.509)] [D acc: 0.750(R 0.641, F 0.859)] [G loss: 0.574] [G acc: 0.156]\n",
      "964 [D loss: 0.573(R 0.562, F0.584)] [D acc: 0.688(R 0.656, F 0.719)] [G loss: 0.573] [G acc: 0.234]\n",
      "965 [D loss: 0.640(R 0.680, F0.601)] [D acc: 0.617(R 0.531, F 0.703)] [G loss: 0.640] [G acc: 0.109]\n",
      "966 [D loss: 0.574(R 0.570, F0.579)] [D acc: 0.750(R 0.688, F 0.812)] [G loss: 0.574] [G acc: 0.172]\n",
      "967 [D loss: 0.567(R 0.577, F0.558)] [D acc: 0.680(R 0.656, F 0.703)] [G loss: 0.567] [G acc: 0.078]\n",
      "968 [D loss: 0.555(R 0.532, F0.578)] [D acc: 0.703(R 0.703, F 0.703)] [G loss: 0.555] [G acc: 0.219]\n",
      "969 [D loss: 0.624(R 0.500, F0.749)] [D acc: 0.664(R 0.719, F 0.609)] [G loss: 0.624] [G acc: 0.188]\n",
      "970 [D loss: 0.572(R 0.576, F0.568)] [D acc: 0.734(R 0.719, F 0.750)] [G loss: 0.572] [G acc: 0.156]\n",
      "971 [D loss: 0.547(R 0.602, F0.492)] [D acc: 0.719(R 0.594, F 0.844)] [G loss: 0.547] [G acc: 0.188]\n",
      "972 [D loss: 0.545(R 0.536, F0.554)] [D acc: 0.758(R 0.719, F 0.797)] [G loss: 0.545] [G acc: 0.125]\n",
      "973 [D loss: 0.651(R 0.677, F0.625)] [D acc: 0.625(R 0.500, F 0.750)] [G loss: 0.651] [G acc: 0.062]\n",
      "974 [D loss: 0.574(R 0.639, F0.510)] [D acc: 0.719(R 0.641, F 0.797)] [G loss: 0.574] [G acc: 0.078]\n",
      "975 [D loss: 0.593(R 0.648, F0.539)] [D acc: 0.695(R 0.578, F 0.812)] [G loss: 0.593] [G acc: 0.094]\n",
      "976 [D loss: 0.595(R 0.557, F0.633)] [D acc: 0.711(R 0.688, F 0.734)] [G loss: 0.595] [G acc: 0.234]\n",
      "977 [D loss: 0.633(R 0.594, F0.672)] [D acc: 0.695(R 0.688, F 0.703)] [G loss: 0.633] [G acc: 0.109]\n",
      "978 [D loss: 0.545(R 0.593, F0.497)] [D acc: 0.727(R 0.625, F 0.828)] [G loss: 0.545] [G acc: 0.141]\n",
      "979 [D loss: 0.725(R 0.691, F0.758)] [D acc: 0.633(R 0.547, F 0.719)] [G loss: 0.725] [G acc: 0.078]\n",
      "980 [D loss: 0.608(R 0.610, F0.606)] [D acc: 0.703(R 0.672, F 0.734)] [G loss: 0.608] [G acc: 0.109]\n",
      "981 [D loss: 0.591(R 0.634, F0.548)] [D acc: 0.680(R 0.547, F 0.812)] [G loss: 0.591] [G acc: 0.109]\n",
      "982 [D loss: 0.587(R 0.612, F0.562)] [D acc: 0.703(R 0.656, F 0.750)] [G loss: 0.587] [G acc: 0.094]\n",
      "983 [D loss: 0.565(R 0.508, F0.621)] [D acc: 0.719(R 0.734, F 0.703)] [G loss: 0.565] [G acc: 0.125]\n",
      "984 [D loss: 0.538(R 0.505, F0.571)] [D acc: 0.742(R 0.797, F 0.688)] [G loss: 0.538] [G acc: 0.109]\n",
      "985 [D loss: 0.614(R 0.495, F0.734)] [D acc: 0.656(R 0.688, F 0.625)] [G loss: 0.614] [G acc: 0.078]\n",
      "986 [D loss: 0.695(R 0.738, F0.652)] [D acc: 0.602(R 0.547, F 0.656)] [G loss: 0.695] [G acc: 0.125]\n",
      "987 [D loss: 0.556(R 0.597, F0.515)] [D acc: 0.750(R 0.656, F 0.844)] [G loss: 0.556] [G acc: 0.094]\n",
      "988 [D loss: 0.583(R 0.565, F0.601)] [D acc: 0.695(R 0.641, F 0.750)] [G loss: 0.583] [G acc: 0.172]\n",
      "989 [D loss: 0.628(R 0.629, F0.627)] [D acc: 0.594(R 0.531, F 0.656)] [G loss: 0.628] [G acc: 0.078]\n",
      "990 [D loss: 0.611(R 0.567, F0.654)] [D acc: 0.656(R 0.672, F 0.641)] [G loss: 0.611] [G acc: 0.062]\n",
      "991 [D loss: 0.564(R 0.642, F0.487)] [D acc: 0.695(R 0.562, F 0.828)] [G loss: 0.564] [G acc: 0.062]\n",
      "992 [D loss: 0.692(R 0.640, F0.743)] [D acc: 0.594(R 0.578, F 0.609)] [G loss: 0.692] [G acc: 0.078]\n",
      "993 [D loss: 0.600(R 0.614, F0.586)] [D acc: 0.672(R 0.609, F 0.734)] [G loss: 0.600] [G acc: 0.047]\n",
      "994 [D loss: 0.520(R 0.583, F0.457)] [D acc: 0.742(R 0.625, F 0.859)] [G loss: 0.520] [G acc: 0.062]\n",
      "995 [D loss: 0.613(R 0.587, F0.639)] [D acc: 0.672(R 0.625, F 0.719)] [G loss: 0.613] [G acc: 0.141]\n",
      "996 [D loss: 0.515(R 0.525, F0.506)] [D acc: 0.781(R 0.703, F 0.859)] [G loss: 0.515] [G acc: 0.094]\n",
      "997 [D loss: 0.587(R 0.623, F0.550)] [D acc: 0.688(R 0.641, F 0.734)] [G loss: 0.587] [G acc: 0.125]\n",
      "998 [D loss: 0.581(R 0.500, F0.662)] [D acc: 0.711(R 0.688, F 0.734)] [G loss: 0.581] [G acc: 0.250]\n",
      "999 [D loss: 0.634(R 0.576, F0.692)] [D acc: 0.664(R 0.672, F 0.656)] [G loss: 0.634] [G acc: 0.188]\n",
      "INFO:tensorflow:Assets written to: ram://5db78d38-c116-4518-b0f6-351eb0d4b9e3/assets\n",
      "INFO:tensorflow:Assets written to: ram://18cc96c8-24b1-44f1-8c85-66fbd0e2e777/assets\n",
      "INFO:tensorflow:Assets written to: ram://b9264e2c-ff5e-46bc-87ea-4192caed6b9b/assets\n",
      "1000 [D loss: 0.569(R 0.597, F0.541)] [D acc: 0.695(R 0.672, F 0.719)] [G loss: 0.569] [G acc: 0.141]\n",
      "1001 [D loss: 0.529(R 0.551, F0.506)] [D acc: 0.766(R 0.688, F 0.844)] [G loss: 0.529] [G acc: 0.062]\n",
      "1002 [D loss: 0.625(R 0.619, F0.632)] [D acc: 0.672(R 0.578, F 0.766)] [G loss: 0.625] [G acc: 0.219]\n",
      "1003 [D loss: 0.615(R 0.598, F0.633)] [D acc: 0.641(R 0.609, F 0.672)] [G loss: 0.615] [G acc: 0.141]\n",
      "1004 [D loss: 0.534(R 0.437, F0.632)] [D acc: 0.719(R 0.766, F 0.672)] [G loss: 0.534] [G acc: 0.109]\n",
      "1005 [D loss: 0.649(R 0.628, F0.670)] [D acc: 0.664(R 0.672, F 0.656)] [G loss: 0.649] [G acc: 0.141]\n",
      "1006 [D loss: 0.620(R 0.633, F0.608)] [D acc: 0.688(R 0.625, F 0.750)] [G loss: 0.620] [G acc: 0.156]\n",
      "1007 [D loss: 0.610(R 0.636, F0.585)] [D acc: 0.703(R 0.609, F 0.797)] [G loss: 0.610] [G acc: 0.094]\n",
      "1008 [D loss: 0.522(R 0.485, F0.559)] [D acc: 0.781(R 0.766, F 0.797)] [G loss: 0.522] [G acc: 0.078]\n",
      "1009 [D loss: 0.543(R 0.575, F0.511)] [D acc: 0.695(R 0.609, F 0.781)] [G loss: 0.543] [G acc: 0.188]\n",
      "1010 [D loss: 0.574(R 0.514, F0.634)] [D acc: 0.695(R 0.734, F 0.656)] [G loss: 0.574] [G acc: 0.094]\n",
      "1011 [D loss: 0.606(R 0.717, F0.496)] [D acc: 0.688(R 0.500, F 0.875)] [G loss: 0.606] [G acc: 0.078]\n",
      "1012 [D loss: 0.518(R 0.528, F0.507)] [D acc: 0.781(R 0.766, F 0.797)] [G loss: 0.518] [G acc: 0.141]\n",
      "1013 [D loss: 0.618(R 0.500, F0.735)] [D acc: 0.664(R 0.719, F 0.609)] [G loss: 0.618] [G acc: 0.109]\n",
      "1014 [D loss: 0.622(R 0.639, F0.605)] [D acc: 0.656(R 0.609, F 0.703)] [G loss: 0.622] [G acc: 0.078]\n",
      "1015 [D loss: 0.577(R 0.554, F0.601)] [D acc: 0.664(R 0.688, F 0.641)] [G loss: 0.577] [G acc: 0.094]\n",
      "1016 [D loss: 0.536(R 0.590, F0.481)] [D acc: 0.742(R 0.672, F 0.812)] [G loss: 0.536] [G acc: 0.188]\n",
      "1017 [D loss: 0.579(R 0.564, F0.593)] [D acc: 0.688(R 0.688, F 0.688)] [G loss: 0.579] [G acc: 0.234]\n",
      "1018 [D loss: 0.639(R 0.586, F0.692)] [D acc: 0.680(R 0.672, F 0.688)] [G loss: 0.639] [G acc: 0.094]\n",
      "1019 [D loss: 0.547(R 0.530, F0.565)] [D acc: 0.680(R 0.656, F 0.703)] [G loss: 0.547] [G acc: 0.078]\n",
      "1020 [D loss: 0.591(R 0.616, F0.566)] [D acc: 0.695(R 0.625, F 0.766)] [G loss: 0.591] [G acc: 0.188]\n",
      "1021 [D loss: 0.594(R 0.593, F0.594)] [D acc: 0.672(R 0.625, F 0.719)] [G loss: 0.594] [G acc: 0.156]\n",
      "1022 [D loss: 0.612(R 0.611, F0.613)] [D acc: 0.695(R 0.625, F 0.766)] [G loss: 0.612] [G acc: 0.188]\n",
      "1023 [D loss: 0.612(R 0.594, F0.631)] [D acc: 0.617(R 0.562, F 0.672)] [G loss: 0.612] [G acc: 0.062]\n",
      "1024 [D loss: 0.544(R 0.614, F0.475)] [D acc: 0.742(R 0.625, F 0.859)] [G loss: 0.544] [G acc: 0.031]\n",
      "1025 [D loss: 0.457(R 0.499, F0.416)] [D acc: 0.812(R 0.703, F 0.922)] [G loss: 0.457] [G acc: 0.141]\n",
      "1026 [D loss: 0.600(R 0.521, F0.678)] [D acc: 0.664(R 0.672, F 0.656)] [G loss: 0.600] [G acc: 0.141]\n",
      "1027 [D loss: 0.608(R 0.539, F0.678)] [D acc: 0.680(R 0.719, F 0.641)] [G loss: 0.608] [G acc: 0.156]\n",
      "1028 [D loss: 0.601(R 0.565, F0.638)] [D acc: 0.711(R 0.688, F 0.734)] [G loss: 0.601] [G acc: 0.141]\n",
      "1029 [D loss: 0.614(R 0.693, F0.534)] [D acc: 0.664(R 0.547, F 0.781)] [G loss: 0.614] [G acc: 0.188]\n",
      "1030 [D loss: 0.553(R 0.519, F0.587)] [D acc: 0.672(R 0.656, F 0.688)] [G loss: 0.553] [G acc: 0.156]\n",
      "1031 [D loss: 0.624(R 0.549, F0.698)] [D acc: 0.703(R 0.719, F 0.688)] [G loss: 0.624] [G acc: 0.094]\n",
      "1032 [D loss: 0.638(R 0.651, F0.626)] [D acc: 0.586(R 0.547, F 0.625)] [G loss: 0.638] [G acc: 0.141]\n",
      "1033 [D loss: 0.616(R 0.627, F0.606)] [D acc: 0.641(R 0.562, F 0.719)] [G loss: 0.616] [G acc: 0.062]\n",
      "1034 [D loss: 0.625(R 0.714, F0.535)] [D acc: 0.672(R 0.516, F 0.828)] [G loss: 0.625] [G acc: 0.094]\n",
      "1035 [D loss: 0.521(R 0.470, F0.573)] [D acc: 0.703(R 0.719, F 0.688)] [G loss: 0.521] [G acc: 0.094]\n",
      "1036 [D loss: 0.589(R 0.615, F0.562)] [D acc: 0.648(R 0.562, F 0.734)] [G loss: 0.589] [G acc: 0.141]\n",
      "1037 [D loss: 0.653(R 0.563, F0.742)] [D acc: 0.664(R 0.703, F 0.625)] [G loss: 0.653] [G acc: 0.188]\n",
      "1038 [D loss: 0.544(R 0.565, F0.523)] [D acc: 0.688(R 0.578, F 0.797)] [G loss: 0.544] [G acc: 0.047]\n",
      "1039 [D loss: 0.550(R 0.543, F0.557)] [D acc: 0.711(R 0.625, F 0.797)] [G loss: 0.550] [G acc: 0.156]\n",
      "1040 [D loss: 0.500(R 0.472, F0.528)] [D acc: 0.750(R 0.719, F 0.781)] [G loss: 0.500] [G acc: 0.141]\n",
      "1041 [D loss: 0.611(R 0.569, F0.653)] [D acc: 0.648(R 0.641, F 0.656)] [G loss: 0.611] [G acc: 0.109]\n",
      "1042 [D loss: 0.590(R 0.644, F0.536)] [D acc: 0.711(R 0.625, F 0.797)] [G loss: 0.590] [G acc: 0.031]\n",
      "1043 [D loss: 0.677(R 0.597, F0.757)] [D acc: 0.664(R 0.656, F 0.672)] [G loss: 0.677] [G acc: 0.094]\n",
      "1044 [D loss: 0.563(R 0.601, F0.526)] [D acc: 0.648(R 0.594, F 0.703)] [G loss: 0.563] [G acc: 0.141]\n",
      "1045 [D loss: 0.528(R 0.567, F0.488)] [D acc: 0.742(R 0.672, F 0.812)] [G loss: 0.528] [G acc: 0.141]\n",
      "1046 [D loss: 0.549(R 0.566, F0.533)] [D acc: 0.742(R 0.672, F 0.812)] [G loss: 0.549] [G acc: 0.109]\n",
      "1047 [D loss: 0.596(R 0.574, F0.619)] [D acc: 0.688(R 0.672, F 0.703)] [G loss: 0.596] [G acc: 0.141]\n",
      "1048 [D loss: 0.631(R 0.638, F0.624)] [D acc: 0.703(R 0.656, F 0.750)] [G loss: 0.631] [G acc: 0.203]\n",
      "1049 [D loss: 0.566(R 0.623, F0.508)] [D acc: 0.664(R 0.547, F 0.781)] [G loss: 0.566] [G acc: 0.109]\n",
      "1050 [D loss: 0.644(R 0.545, F0.742)] [D acc: 0.688(R 0.719, F 0.656)] [G loss: 0.644] [G acc: 0.141]\n",
      "1051 [D loss: 0.550(R 0.493, F0.606)] [D acc: 0.750(R 0.750, F 0.750)] [G loss: 0.550] [G acc: 0.141]\n",
      "1052 [D loss: 0.546(R 0.568, F0.524)] [D acc: 0.727(R 0.703, F 0.750)] [G loss: 0.546] [G acc: 0.172]\n",
      "1053 [D loss: 0.590(R 0.627, F0.553)] [D acc: 0.695(R 0.609, F 0.781)] [G loss: 0.590] [G acc: 0.156]\n",
      "1054 [D loss: 0.633(R 0.612, F0.655)] [D acc: 0.656(R 0.641, F 0.672)] [G loss: 0.633] [G acc: 0.172]\n",
      "1055 [D loss: 0.575(R 0.554, F0.595)] [D acc: 0.695(R 0.688, F 0.703)] [G loss: 0.575] [G acc: 0.125]\n",
      "1056 [D loss: 0.574(R 0.576, F0.572)] [D acc: 0.719(R 0.625, F 0.812)] [G loss: 0.574] [G acc: 0.234]\n",
      "1057 [D loss: 0.606(R 0.524, F0.688)] [D acc: 0.625(R 0.641, F 0.609)] [G loss: 0.606] [G acc: 0.141]\n",
      "1058 [D loss: 0.527(R 0.578, F0.476)] [D acc: 0.742(R 0.641, F 0.844)] [G loss: 0.527] [G acc: 0.281]\n",
      "1059 [D loss: 0.576(R 0.541, F0.611)] [D acc: 0.688(R 0.688, F 0.688)] [G loss: 0.576] [G acc: 0.109]\n",
      "1060 [D loss: 0.589(R 0.557, F0.620)] [D acc: 0.664(R 0.609, F 0.719)] [G loss: 0.589] [G acc: 0.125]\n",
      "1061 [D loss: 0.564(R 0.590, F0.537)] [D acc: 0.711(R 0.703, F 0.719)] [G loss: 0.564] [G acc: 0.203]\n",
      "1062 [D loss: 0.509(R 0.567, F0.451)] [D acc: 0.766(R 0.656, F 0.875)] [G loss: 0.509] [G acc: 0.203]\n",
      "1063 [D loss: 0.595(R 0.588, F0.603)] [D acc: 0.648(R 0.656, F 0.641)] [G loss: 0.595] [G acc: 0.125]\n",
      "1064 [D loss: 0.614(R 0.567, F0.662)] [D acc: 0.688(R 0.672, F 0.703)] [G loss: 0.614] [G acc: 0.047]\n",
      "1065 [D loss: 0.620(R 0.690, F0.550)] [D acc: 0.664(R 0.500, F 0.828)] [G loss: 0.620] [G acc: 0.094]\n",
      "1066 [D loss: 0.609(R 0.594, F0.625)] [D acc: 0.648(R 0.594, F 0.703)] [G loss: 0.609] [G acc: 0.141]\n",
      "1067 [D loss: 0.588(R 0.602, F0.575)] [D acc: 0.688(R 0.625, F 0.750)] [G loss: 0.588] [G acc: 0.156]\n",
      "1068 [D loss: 0.591(R 0.543, F0.639)] [D acc: 0.695(R 0.703, F 0.688)] [G loss: 0.591] [G acc: 0.109]\n",
      "1069 [D loss: 0.595(R 0.495, F0.696)] [D acc: 0.719(R 0.766, F 0.672)] [G loss: 0.595] [G acc: 0.172]\n",
      "1070 [D loss: 0.548(R 0.551, F0.545)] [D acc: 0.719(R 0.672, F 0.766)] [G loss: 0.548] [G acc: 0.125]\n",
      "1071 [D loss: 0.599(R 0.707, F0.492)] [D acc: 0.680(R 0.547, F 0.812)] [G loss: 0.599] [G acc: 0.156]\n",
      "1072 [D loss: 0.616(R 0.541, F0.691)] [D acc: 0.695(R 0.719, F 0.672)] [G loss: 0.616] [G acc: 0.156]\n",
      "1073 [D loss: 0.608(R 0.651, F0.565)] [D acc: 0.664(R 0.594, F 0.734)] [G loss: 0.608] [G acc: 0.078]\n",
      "1074 [D loss: 0.592(R 0.610, F0.574)] [D acc: 0.625(R 0.547, F 0.703)] [G loss: 0.592] [G acc: 0.031]\n",
      "1075 [D loss: 0.551(R 0.542, F0.561)] [D acc: 0.734(R 0.750, F 0.719)] [G loss: 0.551] [G acc: 0.062]\n",
      "1076 [D loss: 0.538(R 0.607, F0.469)] [D acc: 0.703(R 0.594, F 0.812)] [G loss: 0.538] [G acc: 0.109]\n",
      "1077 [D loss: 0.619(R 0.650, F0.588)] [D acc: 0.641(R 0.594, F 0.688)] [G loss: 0.619] [G acc: 0.094]\n",
      "1078 [D loss: 0.594(R 0.528, F0.659)] [D acc: 0.703(R 0.672, F 0.734)] [G loss: 0.594] [G acc: 0.156]\n",
      "1079 [D loss: 0.590(R 0.570, F0.611)] [D acc: 0.633(R 0.594, F 0.672)] [G loss: 0.590] [G acc: 0.141]\n",
      "1080 [D loss: 0.559(R 0.590, F0.528)] [D acc: 0.727(R 0.641, F 0.812)] [G loss: 0.559] [G acc: 0.203]\n",
      "1081 [D loss: 0.562(R 0.538, F0.586)] [D acc: 0.719(R 0.688, F 0.750)] [G loss: 0.562] [G acc: 0.109]\n",
      "1082 [D loss: 0.554(R 0.608, F0.500)] [D acc: 0.734(R 0.688, F 0.781)] [G loss: 0.554] [G acc: 0.047]\n",
      "1083 [D loss: 0.608(R 0.690, F0.525)] [D acc: 0.703(R 0.609, F 0.797)] [G loss: 0.608] [G acc: 0.062]\n",
      "1084 [D loss: 0.616(R 0.638, F0.595)] [D acc: 0.680(R 0.594, F 0.766)] [G loss: 0.616] [G acc: 0.156]\n",
      "1085 [D loss: 0.611(R 0.612, F0.610)] [D acc: 0.648(R 0.625, F 0.672)] [G loss: 0.611] [G acc: 0.062]\n",
      "1086 [D loss: 0.559(R 0.542, F0.577)] [D acc: 0.703(R 0.641, F 0.766)] [G loss: 0.559] [G acc: 0.234]\n",
      "1087 [D loss: 0.642(R 0.655, F0.629)] [D acc: 0.648(R 0.609, F 0.688)] [G loss: 0.642] [G acc: 0.109]\n",
      "1088 [D loss: 0.653(R 0.607, F0.700)] [D acc: 0.641(R 0.625, F 0.656)] [G loss: 0.653] [G acc: 0.109]\n",
      "1089 [D loss: 0.604(R 0.707, F0.501)] [D acc: 0.703(R 0.500, F 0.906)] [G loss: 0.604] [G acc: 0.047]\n",
      "1090 [D loss: 0.579(R 0.500, F0.659)] [D acc: 0.719(R 0.688, F 0.750)] [G loss: 0.579] [G acc: 0.078]\n",
      "1091 [D loss: 0.556(R 0.599, F0.513)] [D acc: 0.672(R 0.594, F 0.750)] [G loss: 0.556] [G acc: 0.031]\n",
      "1092 [D loss: 0.634(R 0.637, F0.630)] [D acc: 0.641(R 0.562, F 0.719)] [G loss: 0.634] [G acc: 0.109]\n",
      "1093 [D loss: 0.557(R 0.564, F0.551)] [D acc: 0.727(R 0.656, F 0.797)] [G loss: 0.557] [G acc: 0.219]\n",
      "1094 [D loss: 0.557(R 0.514, F0.600)] [D acc: 0.719(R 0.719, F 0.719)] [G loss: 0.557] [G acc: 0.188]\n",
      "1095 [D loss: 0.493(R 0.445, F0.540)] [D acc: 0.781(R 0.797, F 0.766)] [G loss: 0.493] [G acc: 0.094]\n",
      "1096 [D loss: 0.606(R 0.632, F0.580)] [D acc: 0.680(R 0.594, F 0.766)] [G loss: 0.606] [G acc: 0.109]\n",
      "1097 [D loss: 0.578(R 0.554, F0.602)] [D acc: 0.727(R 0.672, F 0.781)] [G loss: 0.578] [G acc: 0.062]\n",
      "1098 [D loss: 0.572(R 0.540, F0.604)] [D acc: 0.727(R 0.688, F 0.766)] [G loss: 0.572] [G acc: 0.047]\n",
      "1099 [D loss: 0.627(R 0.562, F0.691)] [D acc: 0.656(R 0.656, F 0.656)] [G loss: 0.627] [G acc: 0.156]\n",
      "INFO:tensorflow:Assets written to: ram://96d34056-1b7f-4ca9-9740-70ec58eaa060/assets\n",
      "INFO:tensorflow:Assets written to: ram://c1d5d01f-ad80-437a-8ce0-c6622583f3e1/assets\n",
      "INFO:tensorflow:Assets written to: ram://dca25d8a-471b-4799-9148-03d9ab96bf00/assets\n",
      "1100 [D loss: 0.547(R 0.547, F0.547)] [D acc: 0.672(R 0.641, F 0.703)] [G loss: 0.547] [G acc: 0.188]\n",
      "1101 [D loss: 0.596(R 0.541, F0.651)] [D acc: 0.641(R 0.656, F 0.625)] [G loss: 0.596] [G acc: 0.125]\n",
      "1102 [D loss: 0.577(R 0.633, F0.521)] [D acc: 0.695(R 0.609, F 0.781)] [G loss: 0.577] [G acc: 0.141]\n",
      "1103 [D loss: 0.492(R 0.493, F0.491)] [D acc: 0.789(R 0.719, F 0.859)] [G loss: 0.492] [G acc: 0.094]\n",
      "1104 [D loss: 0.651(R 0.533, F0.769)] [D acc: 0.656(R 0.688, F 0.625)] [G loss: 0.651] [G acc: 0.141]\n",
      "1105 [D loss: 0.536(R 0.536, F0.536)] [D acc: 0.695(R 0.672, F 0.719)] [G loss: 0.536] [G acc: 0.172]\n",
      "1106 [D loss: 0.543(R 0.576, F0.511)] [D acc: 0.727(R 0.656, F 0.797)] [G loss: 0.543] [G acc: 0.156]\n",
      "1107 [D loss: 0.611(R 0.634, F0.588)] [D acc: 0.711(R 0.672, F 0.750)] [G loss: 0.611] [G acc: 0.141]\n",
      "1108 [D loss: 0.662(R 0.671, F0.653)] [D acc: 0.602(R 0.609, F 0.594)] [G loss: 0.662] [G acc: 0.141]\n",
      "1109 [D loss: 0.591(R 0.628, F0.553)] [D acc: 0.680(R 0.594, F 0.766)] [G loss: 0.591] [G acc: 0.094]\n",
      "1110 [D loss: 0.607(R 0.617, F0.597)] [D acc: 0.727(R 0.625, F 0.828)] [G loss: 0.607] [G acc: 0.188]\n",
      "1111 [D loss: 0.519(R 0.453, F0.584)] [D acc: 0.758(R 0.812, F 0.703)] [G loss: 0.519] [G acc: 0.078]\n",
      "1112 [D loss: 0.575(R 0.570, F0.580)] [D acc: 0.727(R 0.641, F 0.812)] [G loss: 0.575] [G acc: 0.172]\n",
      "1113 [D loss: 0.665(R 0.686, F0.643)] [D acc: 0.602(R 0.547, F 0.656)] [G loss: 0.665] [G acc: 0.109]\n",
      "1114 [D loss: 0.594(R 0.594, F0.594)] [D acc: 0.672(R 0.656, F 0.688)] [G loss: 0.594] [G acc: 0.094]\n",
      "1115 [D loss: 0.604(R 0.596, F0.612)] [D acc: 0.688(R 0.578, F 0.797)] [G loss: 0.604] [G acc: 0.078]\n",
      "1116 [D loss: 0.567(R 0.564, F0.570)] [D acc: 0.742(R 0.719, F 0.766)] [G loss: 0.567] [G acc: 0.094]\n",
      "1117 [D loss: 0.529(R 0.513, F0.546)] [D acc: 0.734(R 0.672, F 0.797)] [G loss: 0.529] [G acc: 0.141]\n",
      "1118 [D loss: 0.598(R 0.571, F0.625)] [D acc: 0.672(R 0.672, F 0.672)] [G loss: 0.598] [G acc: 0.156]\n",
      "1119 [D loss: 0.521(R 0.482, F0.560)] [D acc: 0.758(R 0.750, F 0.766)] [G loss: 0.521] [G acc: 0.062]\n",
      "1120 [D loss: 0.661(R 0.647, F0.676)] [D acc: 0.594(R 0.578, F 0.609)] [G loss: 0.661] [G acc: 0.219]\n",
      "1121 [D loss: 0.624(R 0.625, F0.623)] [D acc: 0.656(R 0.625, F 0.688)] [G loss: 0.624] [G acc: 0.109]\n",
      "1122 [D loss: 0.562(R 0.569, F0.556)] [D acc: 0.703(R 0.609, F 0.797)] [G loss: 0.562] [G acc: 0.094]\n",
      "1123 [D loss: 0.613(R 0.657, F0.569)] [D acc: 0.664(R 0.562, F 0.766)] [G loss: 0.613] [G acc: 0.172]\n",
      "1124 [D loss: 0.523(R 0.554, F0.492)] [D acc: 0.766(R 0.703, F 0.828)] [G loss: 0.523] [G acc: 0.094]\n",
      "1125 [D loss: 0.633(R 0.563, F0.702)] [D acc: 0.688(R 0.641, F 0.734)] [G loss: 0.633] [G acc: 0.125]\n",
      "1126 [D loss: 0.558(R 0.596, F0.521)] [D acc: 0.727(R 0.656, F 0.797)] [G loss: 0.558] [G acc: 0.047]\n",
      "1127 [D loss: 0.572(R 0.576, F0.569)] [D acc: 0.711(R 0.672, F 0.750)] [G loss: 0.572] [G acc: 0.047]\n",
      "1128 [D loss: 0.573(R 0.615, F0.530)] [D acc: 0.703(R 0.625, F 0.781)] [G loss: 0.573] [G acc: 0.125]\n",
      "1129 [D loss: 0.611(R 0.653, F0.569)] [D acc: 0.648(R 0.516, F 0.781)] [G loss: 0.611] [G acc: 0.172]\n",
      "1130 [D loss: 0.562(R 0.597, F0.527)] [D acc: 0.703(R 0.594, F 0.812)] [G loss: 0.562] [G acc: 0.094]\n",
      "1131 [D loss: 0.603(R 0.517, F0.690)] [D acc: 0.648(R 0.656, F 0.641)] [G loss: 0.603] [G acc: 0.094]\n",
      "1132 [D loss: 0.606(R 0.619, F0.592)] [D acc: 0.688(R 0.609, F 0.766)] [G loss: 0.606] [G acc: 0.141]\n",
      "1133 [D loss: 0.577(R 0.544, F0.611)] [D acc: 0.711(R 0.719, F 0.703)] [G loss: 0.577] [G acc: 0.172]\n",
      "1134 [D loss: 0.591(R 0.590, F0.591)] [D acc: 0.719(R 0.672, F 0.766)] [G loss: 0.591] [G acc: 0.125]\n",
      "1135 [D loss: 0.630(R 0.630, F0.630)] [D acc: 0.641(R 0.609, F 0.672)] [G loss: 0.630] [G acc: 0.125]\n",
      "1136 [D loss: 0.631(R 0.589, F0.674)] [D acc: 0.680(R 0.688, F 0.672)] [G loss: 0.631] [G acc: 0.172]\n",
      "1137 [D loss: 0.558(R 0.581, F0.534)] [D acc: 0.711(R 0.656, F 0.766)] [G loss: 0.558] [G acc: 0.141]\n",
      "1138 [D loss: 0.547(R 0.540, F0.553)] [D acc: 0.695(R 0.625, F 0.766)] [G loss: 0.547] [G acc: 0.188]\n",
      "1139 [D loss: 0.567(R 0.501, F0.633)] [D acc: 0.727(R 0.719, F 0.734)] [G loss: 0.567] [G acc: 0.094]\n",
      "1140 [D loss: 0.504(R 0.530, F0.479)] [D acc: 0.773(R 0.688, F 0.859)] [G loss: 0.504] [G acc: 0.062]\n",
      "1141 [D loss: 0.552(R 0.550, F0.555)] [D acc: 0.680(R 0.656, F 0.703)] [G loss: 0.552] [G acc: 0.094]\n",
      "1142 [D loss: 0.549(R 0.615, F0.483)] [D acc: 0.695(R 0.625, F 0.766)] [G loss: 0.549] [G acc: 0.094]\n",
      "1143 [D loss: 0.653(R 0.666, F0.641)] [D acc: 0.602(R 0.500, F 0.703)] [G loss: 0.653] [G acc: 0.188]\n",
      "1144 [D loss: 0.549(R 0.490, F0.608)] [D acc: 0.719(R 0.688, F 0.750)] [G loss: 0.549] [G acc: 0.156]\n",
      "1145 [D loss: 0.584(R 0.527, F0.642)] [D acc: 0.648(R 0.656, F 0.641)] [G loss: 0.584] [G acc: 0.016]\n",
      "1146 [D loss: 0.679(R 0.716, F0.643)] [D acc: 0.578(R 0.531, F 0.625)] [G loss: 0.679] [G acc: 0.172]\n",
      "1147 [D loss: 0.528(R 0.483, F0.574)] [D acc: 0.742(R 0.719, F 0.766)] [G loss: 0.528] [G acc: 0.078]\n",
      "1148 [D loss: 0.573(R 0.546, F0.600)] [D acc: 0.680(R 0.672, F 0.688)] [G loss: 0.573] [G acc: 0.172]\n",
      "1149 [D loss: 0.578(R 0.613, F0.543)] [D acc: 0.688(R 0.578, F 0.797)] [G loss: 0.578] [G acc: 0.125]\n",
      "1150 [D loss: 0.516(R 0.515, F0.517)] [D acc: 0.734(R 0.656, F 0.812)] [G loss: 0.516] [G acc: 0.062]\n",
      "1151 [D loss: 0.609(R 0.651, F0.566)] [D acc: 0.617(R 0.531, F 0.703)] [G loss: 0.609] [G acc: 0.094]\n",
      "1152 [D loss: 0.589(R 0.662, F0.515)] [D acc: 0.703(R 0.578, F 0.828)] [G loss: 0.589] [G acc: 0.047]\n",
      "1153 [D loss: 0.520(R 0.500, F0.541)] [D acc: 0.773(R 0.719, F 0.828)] [G loss: 0.520] [G acc: 0.094]\n",
      "1154 [D loss: 0.515(R 0.562, F0.467)] [D acc: 0.812(R 0.719, F 0.906)] [G loss: 0.515] [G acc: 0.156]\n",
      "1155 [D loss: 0.560(R 0.508, F0.613)] [D acc: 0.688(R 0.625, F 0.750)] [G loss: 0.560] [G acc: 0.094]\n",
      "1156 [D loss: 0.552(R 0.481, F0.623)] [D acc: 0.711(R 0.734, F 0.688)] [G loss: 0.552] [G acc: 0.141]\n",
      "1157 [D loss: 0.624(R 0.573, F0.674)] [D acc: 0.633(R 0.609, F 0.656)] [G loss: 0.624] [G acc: 0.156]\n",
      "1158 [D loss: 0.553(R 0.588, F0.518)] [D acc: 0.750(R 0.609, F 0.891)] [G loss: 0.553] [G acc: 0.094]\n",
      "1159 [D loss: 0.547(R 0.437, F0.656)] [D acc: 0.773(R 0.797, F 0.750)] [G loss: 0.547] [G acc: 0.062]\n",
      "1160 [D loss: 0.587(R 0.551, F0.622)] [D acc: 0.742(R 0.703, F 0.781)] [G loss: 0.587] [G acc: 0.125]\n",
      "1161 [D loss: 0.557(R 0.586, F0.527)] [D acc: 0.688(R 0.578, F 0.797)] [G loss: 0.557] [G acc: 0.156]\n",
      "1162 [D loss: 0.597(R 0.558, F0.635)] [D acc: 0.688(R 0.625, F 0.750)] [G loss: 0.597] [G acc: 0.156]\n",
      "1163 [D loss: 0.512(R 0.518, F0.506)] [D acc: 0.766(R 0.688, F 0.844)] [G loss: 0.512] [G acc: 0.062]\n",
      "1164 [D loss: 0.569(R 0.552, F0.585)] [D acc: 0.703(R 0.688, F 0.719)] [G loss: 0.569] [G acc: 0.141]\n",
      "1165 [D loss: 0.465(R 0.337, F0.592)] [D acc: 0.781(R 0.797, F 0.766)] [G loss: 0.465] [G acc: 0.062]\n",
      "1166 [D loss: 0.509(R 0.526, F0.493)] [D acc: 0.766(R 0.719, F 0.812)] [G loss: 0.509] [G acc: 0.031]\n",
      "1167 [D loss: 0.625(R 0.648, F0.602)] [D acc: 0.656(R 0.562, F 0.750)] [G loss: 0.625] [G acc: 0.125]\n",
      "1168 [D loss: 0.505(R 0.491, F0.520)] [D acc: 0.727(R 0.734, F 0.719)] [G loss: 0.505] [G acc: 0.125]\n",
      "1169 [D loss: 0.558(R 0.464, F0.653)] [D acc: 0.773(R 0.766, F 0.781)] [G loss: 0.558] [G acc: 0.109]\n",
      "1170 [D loss: 0.546(R 0.576, F0.515)] [D acc: 0.750(R 0.703, F 0.797)] [G loss: 0.546] [G acc: 0.078]\n",
      "1171 [D loss: 0.595(R 0.595, F0.595)] [D acc: 0.688(R 0.641, F 0.734)] [G loss: 0.595] [G acc: 0.109]\n",
      "1172 [D loss: 0.505(R 0.503, F0.506)] [D acc: 0.758(R 0.719, F 0.797)] [G loss: 0.505] [G acc: 0.141]\n",
      "1173 [D loss: 0.569(R 0.449, F0.689)] [D acc: 0.680(R 0.703, F 0.656)] [G loss: 0.569] [G acc: 0.078]\n",
      "1174 [D loss: 0.557(R 0.610, F0.503)] [D acc: 0.750(R 0.625, F 0.875)] [G loss: 0.557] [G acc: 0.062]\n",
      "1175 [D loss: 0.583(R 0.645, F0.521)] [D acc: 0.688(R 0.594, F 0.781)] [G loss: 0.583] [G acc: 0.141]\n",
      "1176 [D loss: 0.589(R 0.597, F0.582)] [D acc: 0.672(R 0.625, F 0.719)] [G loss: 0.589] [G acc: 0.156]\n",
      "1177 [D loss: 0.527(R 0.512, F0.542)] [D acc: 0.734(R 0.719, F 0.750)] [G loss: 0.527] [G acc: 0.109]\n",
      "1178 [D loss: 0.527(R 0.523, F0.531)] [D acc: 0.711(R 0.625, F 0.797)] [G loss: 0.527] [G acc: 0.078]\n",
      "1179 [D loss: 0.522(R 0.446, F0.597)] [D acc: 0.703(R 0.688, F 0.719)] [G loss: 0.522] [G acc: 0.156]\n",
      "1180 [D loss: 0.492(R 0.513, F0.471)] [D acc: 0.758(R 0.703, F 0.812)] [G loss: 0.492] [G acc: 0.188]\n",
      "1181 [D loss: 0.587(R 0.622, F0.553)] [D acc: 0.672(R 0.594, F 0.750)] [G loss: 0.587] [G acc: 0.156]\n",
      "1182 [D loss: 0.509(R 0.482, F0.536)] [D acc: 0.758(R 0.750, F 0.766)] [G loss: 0.509] [G acc: 0.125]\n",
      "1183 [D loss: 0.618(R 0.566, F0.670)] [D acc: 0.703(R 0.656, F 0.750)] [G loss: 0.618] [G acc: 0.062]\n",
      "1184 [D loss: 0.657(R 0.702, F0.613)] [D acc: 0.633(R 0.500, F 0.766)] [G loss: 0.657] [G acc: 0.047]\n",
      "1185 [D loss: 0.536(R 0.550, F0.522)] [D acc: 0.703(R 0.641, F 0.766)] [G loss: 0.536] [G acc: 0.094]\n",
      "1186 [D loss: 0.483(R 0.499, F0.467)] [D acc: 0.758(R 0.688, F 0.828)] [G loss: 0.483] [G acc: 0.062]\n",
      "1187 [D loss: 0.574(R 0.582, F0.565)] [D acc: 0.688(R 0.625, F 0.750)] [G loss: 0.574] [G acc: 0.078]\n",
      "1188 [D loss: 0.525(R 0.546, F0.505)] [D acc: 0.727(R 0.656, F 0.797)] [G loss: 0.525] [G acc: 0.125]\n",
      "1189 [D loss: 0.552(R 0.554, F0.550)] [D acc: 0.719(R 0.656, F 0.781)] [G loss: 0.552] [G acc: 0.156]\n",
      "1190 [D loss: 0.556(R 0.519, F0.593)] [D acc: 0.727(R 0.766, F 0.688)] [G loss: 0.556] [G acc: 0.125]\n",
      "1191 [D loss: 0.518(R 0.514, F0.522)] [D acc: 0.734(R 0.703, F 0.766)] [G loss: 0.518] [G acc: 0.094]\n",
      "1192 [D loss: 0.558(R 0.473, F0.644)] [D acc: 0.711(R 0.703, F 0.719)] [G loss: 0.558] [G acc: 0.094]\n",
      "1193 [D loss: 0.585(R 0.649, F0.520)] [D acc: 0.656(R 0.531, F 0.781)] [G loss: 0.585] [G acc: 0.141]\n",
      "1194 [D loss: 0.650(R 0.632, F0.667)] [D acc: 0.664(R 0.672, F 0.656)] [G loss: 0.650] [G acc: 0.016]\n",
      "1195 [D loss: 0.555(R 0.653, F0.458)] [D acc: 0.703(R 0.578, F 0.828)] [G loss: 0.555] [G acc: 0.031]\n",
      "1196 [D loss: 0.566(R 0.606, F0.526)] [D acc: 0.672(R 0.547, F 0.797)] [G loss: 0.566] [G acc: 0.078]\n",
      "1197 [D loss: 0.538(R 0.511, F0.566)] [D acc: 0.758(R 0.703, F 0.812)] [G loss: 0.538] [G acc: 0.062]\n",
      "1198 [D loss: 0.559(R 0.518, F0.601)] [D acc: 0.727(R 0.734, F 0.719)] [G loss: 0.559] [G acc: 0.078]\n",
      "1199 [D loss: 0.604(R 0.584, F0.624)] [D acc: 0.719(R 0.641, F 0.797)] [G loss: 0.604] [G acc: 0.047]\n",
      "INFO:tensorflow:Assets written to: ram://c012a852-f670-4dd2-a507-95eb03ec70c8/assets\n",
      "INFO:tensorflow:Assets written to: ram://22180b99-c924-4a24-857e-f320968fd4f0/assets\n",
      "INFO:tensorflow:Assets written to: ram://60993de0-fbdd-4919-afc1-3557f2eb431e/assets\n",
      "1200 [D loss: 0.552(R 0.582, F0.522)] [D acc: 0.703(R 0.688, F 0.719)] [G loss: 0.552] [G acc: 0.203]\n",
      "1201 [D loss: 0.682(R 0.649, F0.715)] [D acc: 0.633(R 0.562, F 0.703)] [G loss: 0.682] [G acc: 0.094]\n",
      "1202 [D loss: 0.576(R 0.604, F0.549)] [D acc: 0.711(R 0.656, F 0.766)] [G loss: 0.576] [G acc: 0.047]\n",
      "1203 [D loss: 0.607(R 0.652, F0.562)] [D acc: 0.703(R 0.609, F 0.797)] [G loss: 0.607] [G acc: 0.156]\n",
      "1204 [D loss: 0.537(R 0.545, F0.529)] [D acc: 0.711(R 0.656, F 0.766)] [G loss: 0.537] [G acc: 0.141]\n",
      "1205 [D loss: 0.555(R 0.626, F0.484)] [D acc: 0.711(R 0.578, F 0.844)] [G loss: 0.555] [G acc: 0.094]\n",
      "1206 [D loss: 0.636(R 0.630, F0.641)] [D acc: 0.625(R 0.516, F 0.734)] [G loss: 0.636] [G acc: 0.141]\n",
      "1207 [D loss: 0.598(R 0.670, F0.527)] [D acc: 0.703(R 0.562, F 0.844)] [G loss: 0.598] [G acc: 0.156]\n",
      "1208 [D loss: 0.535(R 0.555, F0.515)] [D acc: 0.711(R 0.609, F 0.812)] [G loss: 0.535] [G acc: 0.094]\n",
      "1209 [D loss: 0.639(R 0.588, F0.691)] [D acc: 0.625(R 0.625, F 0.625)] [G loss: 0.639] [G acc: 0.109]\n",
      "1210 [D loss: 0.618(R 0.613, F0.624)] [D acc: 0.656(R 0.594, F 0.719)] [G loss: 0.618] [G acc: 0.078]\n",
      "1211 [D loss: 0.557(R 0.577, F0.538)] [D acc: 0.688(R 0.594, F 0.781)] [G loss: 0.557] [G acc: 0.125]\n",
      "1212 [D loss: 0.582(R 0.579, F0.584)] [D acc: 0.727(R 0.672, F 0.781)] [G loss: 0.582] [G acc: 0.125]\n",
      "1213 [D loss: 0.609(R 0.589, F0.629)] [D acc: 0.656(R 0.609, F 0.703)] [G loss: 0.609] [G acc: 0.031]\n",
      "1214 [D loss: 0.583(R 0.595, F0.571)] [D acc: 0.656(R 0.625, F 0.688)] [G loss: 0.583] [G acc: 0.078]\n",
      "1215 [D loss: 0.590(R 0.608, F0.572)] [D acc: 0.711(R 0.672, F 0.750)] [G loss: 0.590] [G acc: 0.078]\n",
      "1216 [D loss: 0.562(R 0.597, F0.527)] [D acc: 0.711(R 0.641, F 0.781)] [G loss: 0.562] [G acc: 0.109]\n",
      "1217 [D loss: 0.565(R 0.570, F0.559)] [D acc: 0.703(R 0.641, F 0.766)] [G loss: 0.565] [G acc: 0.078]\n",
      "1218 [D loss: 0.574(R 0.670, F0.477)] [D acc: 0.695(R 0.547, F 0.844)] [G loss: 0.574] [G acc: 0.047]\n",
      "1219 [D loss: 0.556(R 0.537, F0.576)] [D acc: 0.680(R 0.641, F 0.719)] [G loss: 0.556] [G acc: 0.141]\n",
      "1220 [D loss: 0.615(R 0.664, F0.567)] [D acc: 0.672(R 0.547, F 0.797)] [G loss: 0.615] [G acc: 0.297]\n",
      "1221 [D loss: 0.641(R 0.434, F0.848)] [D acc: 0.664(R 0.781, F 0.547)] [G loss: 0.641] [G acc: 0.078]\n",
      "1222 [D loss: 0.654(R 0.803, F0.505)] [D acc: 0.617(R 0.438, F 0.797)] [G loss: 0.654] [G acc: 0.172]\n",
      "1223 [D loss: 0.565(R 0.492, F0.637)] [D acc: 0.688(R 0.672, F 0.703)] [G loss: 0.565] [G acc: 0.156]\n",
      "1224 [D loss: 0.519(R 0.551, F0.486)] [D acc: 0.742(R 0.672, F 0.812)] [G loss: 0.519] [G acc: 0.094]\n",
      "1225 [D loss: 0.515(R 0.474, F0.555)] [D acc: 0.742(R 0.781, F 0.703)] [G loss: 0.515] [G acc: 0.125]\n",
      "1226 [D loss: 0.575(R 0.570, F0.580)] [D acc: 0.711(R 0.656, F 0.766)] [G loss: 0.575] [G acc: 0.062]\n",
      "1227 [D loss: 0.565(R 0.563, F0.567)] [D acc: 0.688(R 0.656, F 0.719)] [G loss: 0.565] [G acc: 0.078]\n",
      "1228 [D loss: 0.512(R 0.502, F0.522)] [D acc: 0.719(R 0.688, F 0.750)] [G loss: 0.512] [G acc: 0.188]\n",
      "1229 [D loss: 0.497(R 0.518, F0.476)] [D acc: 0.742(R 0.672, F 0.812)] [G loss: 0.497] [G acc: 0.156]\n",
      "1230 [D loss: 0.513(R 0.526, F0.501)] [D acc: 0.742(R 0.719, F 0.766)] [G loss: 0.513] [G acc: 0.094]\n",
      "1231 [D loss: 0.519(R 0.516, F0.522)] [D acc: 0.734(R 0.703, F 0.766)] [G loss: 0.519] [G acc: 0.047]\n",
      "1232 [D loss: 0.556(R 0.509, F0.603)] [D acc: 0.664(R 0.641, F 0.688)] [G loss: 0.556] [G acc: 0.109]\n",
      "1233 [D loss: 0.554(R 0.559, F0.548)] [D acc: 0.727(R 0.656, F 0.797)] [G loss: 0.554] [G acc: 0.094]\n",
      "1234 [D loss: 0.557(R 0.440, F0.675)] [D acc: 0.727(R 0.750, F 0.703)] [G loss: 0.557] [G acc: 0.094]\n",
      "1235 [D loss: 0.469(R 0.514, F0.424)] [D acc: 0.812(R 0.703, F 0.922)] [G loss: 0.469] [G acc: 0.031]\n",
      "1236 [D loss: 0.577(R 0.492, F0.662)] [D acc: 0.711(R 0.656, F 0.766)] [G loss: 0.577] [G acc: 0.062]\n",
      "1237 [D loss: 0.566(R 0.624, F0.508)] [D acc: 0.695(R 0.578, F 0.812)] [G loss: 0.566] [G acc: 0.125]\n",
      "1238 [D loss: 0.562(R 0.516, F0.609)] [D acc: 0.688(R 0.672, F 0.703)] [G loss: 0.562] [G acc: 0.109]\n",
      "1239 [D loss: 0.547(R 0.676, F0.419)] [D acc: 0.734(R 0.578, F 0.891)] [G loss: 0.547] [G acc: 0.125]\n",
      "1240 [D loss: 0.581(R 0.527, F0.635)] [D acc: 0.664(R 0.656, F 0.672)] [G loss: 0.581] [G acc: 0.172]\n",
      "1241 [D loss: 0.535(R 0.522, F0.547)] [D acc: 0.719(R 0.703, F 0.734)] [G loss: 0.535] [G acc: 0.031]\n",
      "1242 [D loss: 0.495(R 0.543, F0.447)] [D acc: 0.797(R 0.734, F 0.859)] [G loss: 0.495] [G acc: 0.109]\n",
      "1243 [D loss: 0.606(R 0.518, F0.694)] [D acc: 0.711(R 0.641, F 0.781)] [G loss: 0.606] [G acc: 0.094]\n",
      "1244 [D loss: 0.533(R 0.500, F0.566)] [D acc: 0.703(R 0.719, F 0.688)] [G loss: 0.533] [G acc: 0.109]\n",
      "1245 [D loss: 0.525(R 0.572, F0.477)] [D acc: 0.734(R 0.625, F 0.844)] [G loss: 0.525] [G acc: 0.156]\n",
      "1246 [D loss: 0.467(R 0.425, F0.509)] [D acc: 0.781(R 0.750, F 0.812)] [G loss: 0.467] [G acc: 0.062]\n",
      "1247 [D loss: 0.541(R 0.512, F0.571)] [D acc: 0.750(R 0.750, F 0.750)] [G loss: 0.541] [G acc: 0.047]\n",
      "1248 [D loss: 0.596(R 0.684, F0.508)] [D acc: 0.703(R 0.625, F 0.781)] [G loss: 0.596] [G acc: 0.047]\n",
      "1249 [D loss: 0.561(R 0.560, F0.562)] [D acc: 0.750(R 0.688, F 0.812)] [G loss: 0.561] [G acc: 0.047]\n",
      "1250 [D loss: 0.603(R 0.624, F0.583)] [D acc: 0.656(R 0.562, F 0.750)] [G loss: 0.603] [G acc: 0.156]\n",
      "1251 [D loss: 0.538(R 0.504, F0.573)] [D acc: 0.750(R 0.719, F 0.781)] [G loss: 0.538] [G acc: 0.141]\n",
      "1252 [D loss: 0.546(R 0.471, F0.620)] [D acc: 0.742(R 0.766, F 0.719)] [G loss: 0.546] [G acc: 0.031]\n",
      "1253 [D loss: 0.570(R 0.560, F0.581)] [D acc: 0.703(R 0.641, F 0.766)] [G loss: 0.570] [G acc: 0.062]\n",
      "1254 [D loss: 0.558(R 0.546, F0.569)] [D acc: 0.680(R 0.672, F 0.688)] [G loss: 0.558] [G acc: 0.078]\n",
      "1255 [D loss: 0.530(R 0.499, F0.562)] [D acc: 0.789(R 0.734, F 0.844)] [G loss: 0.530] [G acc: 0.000]\n",
      "1256 [D loss: 0.606(R 0.606, F0.606)] [D acc: 0.727(R 0.672, F 0.781)] [G loss: 0.606] [G acc: 0.094]\n",
      "1257 [D loss: 0.534(R 0.642, F0.426)] [D acc: 0.758(R 0.594, F 0.922)] [G loss: 0.534] [G acc: 0.062]\n",
      "1258 [D loss: 0.494(R 0.425, F0.564)] [D acc: 0.789(R 0.781, F 0.797)] [G loss: 0.494] [G acc: 0.109]\n",
      "1259 [D loss: 0.566(R 0.662, F0.469)] [D acc: 0.750(R 0.609, F 0.891)] [G loss: 0.566] [G acc: 0.109]\n",
      "1260 [D loss: 0.593(R 0.678, F0.508)] [D acc: 0.695(R 0.547, F 0.844)] [G loss: 0.593] [G acc: 0.109]\n",
      "1261 [D loss: 0.588(R 0.477, F0.699)] [D acc: 0.688(R 0.750, F 0.625)] [G loss: 0.588] [G acc: 0.125]\n",
      "1262 [D loss: 0.577(R 0.587, F0.567)] [D acc: 0.656(R 0.562, F 0.750)] [G loss: 0.577] [G acc: 0.172]\n",
      "1263 [D loss: 0.543(R 0.560, F0.526)] [D acc: 0.719(R 0.625, F 0.812)] [G loss: 0.543] [G acc: 0.141]\n",
      "1264 [D loss: 0.500(R 0.444, F0.556)] [D acc: 0.703(R 0.703, F 0.703)] [G loss: 0.500] [G acc: 0.125]\n",
      "1265 [D loss: 0.581(R 0.510, F0.653)] [D acc: 0.695(R 0.688, F 0.703)] [G loss: 0.581] [G acc: 0.094]\n",
      "1266 [D loss: 0.617(R 0.525, F0.709)] [D acc: 0.680(R 0.688, F 0.672)] [G loss: 0.617] [G acc: 0.078]\n",
      "1267 [D loss: 0.479(R 0.520, F0.437)] [D acc: 0.797(R 0.703, F 0.891)] [G loss: 0.479] [G acc: 0.109]\n",
      "1268 [D loss: 0.511(R 0.499, F0.523)] [D acc: 0.766(R 0.703, F 0.828)] [G loss: 0.511] [G acc: 0.109]\n",
      "1269 [D loss: 0.560(R 0.549, F0.570)] [D acc: 0.711(R 0.609, F 0.812)] [G loss: 0.560] [G acc: 0.031]\n",
      "1270 [D loss: 0.605(R 0.538, F0.673)] [D acc: 0.656(R 0.672, F 0.641)] [G loss: 0.605] [G acc: 0.094]\n",
      "1271 [D loss: 0.593(R 0.647, F0.538)] [D acc: 0.680(R 0.594, F 0.766)] [G loss: 0.593] [G acc: 0.141]\n",
      "1272 [D loss: 0.538(R 0.595, F0.481)] [D acc: 0.742(R 0.641, F 0.844)] [G loss: 0.538] [G acc: 0.141]\n",
      "1273 [D loss: 0.576(R 0.599, F0.554)] [D acc: 0.727(R 0.672, F 0.781)] [G loss: 0.576] [G acc: 0.125]\n",
      "1274 [D loss: 0.542(R 0.478, F0.605)] [D acc: 0.734(R 0.750, F 0.719)] [G loss: 0.542] [G acc: 0.078]\n",
      "1275 [D loss: 0.557(R 0.558, F0.555)] [D acc: 0.719(R 0.625, F 0.812)] [G loss: 0.557] [G acc: 0.109]\n",
      "1276 [D loss: 0.580(R 0.657, F0.504)] [D acc: 0.727(R 0.625, F 0.828)] [G loss: 0.580] [G acc: 0.047]\n",
      "1277 [D loss: 0.643(R 0.611, F0.675)] [D acc: 0.664(R 0.625, F 0.703)] [G loss: 0.643] [G acc: 0.062]\n",
      "1278 [D loss: 0.605(R 0.656, F0.555)] [D acc: 0.648(R 0.562, F 0.734)] [G loss: 0.605] [G acc: 0.188]\n",
      "1279 [D loss: 0.560(R 0.606, F0.514)] [D acc: 0.664(R 0.578, F 0.750)] [G loss: 0.560] [G acc: 0.031]\n",
      "1280 [D loss: 0.608(R 0.543, F0.672)] [D acc: 0.656(R 0.609, F 0.703)] [G loss: 0.608] [G acc: 0.078]\n",
      "1281 [D loss: 0.566(R 0.560, F0.572)] [D acc: 0.680(R 0.625, F 0.734)] [G loss: 0.566] [G acc: 0.109]\n",
      "1282 [D loss: 0.595(R 0.620, F0.569)] [D acc: 0.633(R 0.562, F 0.703)] [G loss: 0.595] [G acc: 0.125]\n",
      "1283 [D loss: 0.512(R 0.555, F0.469)] [D acc: 0.750(R 0.641, F 0.859)] [G loss: 0.512] [G acc: 0.094]\n",
      "1284 [D loss: 0.545(R 0.608, F0.483)] [D acc: 0.711(R 0.594, F 0.828)] [G loss: 0.545] [G acc: 0.062]\n",
      "1285 [D loss: 0.450(R 0.421, F0.479)] [D acc: 0.758(R 0.734, F 0.781)] [G loss: 0.450] [G acc: 0.125]\n",
      "1286 [D loss: 0.605(R 0.564, F0.647)] [D acc: 0.703(R 0.656, F 0.750)] [G loss: 0.605] [G acc: 0.031]\n",
      "1287 [D loss: 0.608(R 0.708, F0.508)] [D acc: 0.688(R 0.562, F 0.812)] [G loss: 0.608] [G acc: 0.109]\n",
      "1288 [D loss: 0.563(R 0.509, F0.617)] [D acc: 0.727(R 0.688, F 0.766)] [G loss: 0.563] [G acc: 0.094]\n",
      "1289 [D loss: 0.422(R 0.381, F0.463)] [D acc: 0.820(R 0.797, F 0.844)] [G loss: 0.422] [G acc: 0.078]\n",
      "1290 [D loss: 0.512(R 0.504, F0.520)] [D acc: 0.727(R 0.703, F 0.750)] [G loss: 0.512] [G acc: 0.047]\n",
      "1291 [D loss: 0.542(R 0.629, F0.455)] [D acc: 0.773(R 0.625, F 0.922)] [G loss: 0.542] [G acc: 0.125]\n",
      "1292 [D loss: 0.577(R 0.655, F0.499)] [D acc: 0.695(R 0.609, F 0.781)] [G loss: 0.577] [G acc: 0.125]\n",
      "1293 [D loss: 0.647(R 0.528, F0.766)] [D acc: 0.656(R 0.641, F 0.672)] [G loss: 0.647] [G acc: 0.125]\n",
      "1294 [D loss: 0.670(R 0.742, F0.597)] [D acc: 0.594(R 0.484, F 0.703)] [G loss: 0.670] [G acc: 0.156]\n",
      "1295 [D loss: 0.507(R 0.510, F0.505)] [D acc: 0.719(R 0.656, F 0.781)] [G loss: 0.507] [G acc: 0.094]\n",
      "1296 [D loss: 0.583(R 0.572, F0.595)] [D acc: 0.734(R 0.625, F 0.844)] [G loss: 0.583] [G acc: 0.188]\n",
      "1297 [D loss: 0.511(R 0.495, F0.527)] [D acc: 0.734(R 0.656, F 0.812)] [G loss: 0.511] [G acc: 0.094]\n",
      "1298 [D loss: 0.594(R 0.580, F0.607)] [D acc: 0.672(R 0.609, F 0.734)] [G loss: 0.594] [G acc: 0.078]\n",
      "1299 [D loss: 0.492(R 0.501, F0.483)] [D acc: 0.781(R 0.734, F 0.828)] [G loss: 0.492] [G acc: 0.125]\n",
      "INFO:tensorflow:Assets written to: ram://996ea5a1-4d56-466e-9874-34f1491be683/assets\n",
      "INFO:tensorflow:Assets written to: ram://cd50739c-03f0-4b74-a1bc-cef7eec49879/assets\n",
      "INFO:tensorflow:Assets written to: ram://492e60b3-7917-4530-8903-3e4e97a65f95/assets\n",
      "1300 [D loss: 0.591(R 0.566, F0.617)] [D acc: 0.680(R 0.625, F 0.734)] [G loss: 0.591] [G acc: 0.094]\n",
      "1301 [D loss: 0.670(R 0.730, F0.610)] [D acc: 0.602(R 0.484, F 0.719)] [G loss: 0.670] [G acc: 0.234]\n",
      "1302 [D loss: 0.515(R 0.438, F0.593)] [D acc: 0.727(R 0.750, F 0.703)] [G loss: 0.515] [G acc: 0.188]\n",
      "1303 [D loss: 0.660(R 0.646, F0.673)] [D acc: 0.641(R 0.594, F 0.688)] [G loss: 0.660] [G acc: 0.125]\n",
      "1304 [D loss: 0.560(R 0.490, F0.630)] [D acc: 0.703(R 0.719, F 0.688)] [G loss: 0.560] [G acc: 0.125]\n",
      "1305 [D loss: 0.541(R 0.552, F0.530)] [D acc: 0.742(R 0.641, F 0.844)] [G loss: 0.541] [G acc: 0.172]\n",
      "1306 [D loss: 0.582(R 0.563, F0.602)] [D acc: 0.703(R 0.703, F 0.703)] [G loss: 0.582] [G acc: 0.078]\n",
      "1307 [D loss: 0.621(R 0.536, F0.705)] [D acc: 0.664(R 0.609, F 0.719)] [G loss: 0.621] [G acc: 0.109]\n",
      "1308 [D loss: 0.565(R 0.658, F0.471)] [D acc: 0.727(R 0.625, F 0.828)] [G loss: 0.565] [G acc: 0.094]\n",
      "1309 [D loss: 0.530(R 0.571, F0.490)] [D acc: 0.734(R 0.656, F 0.812)] [G loss: 0.530] [G acc: 0.109]\n",
      "1310 [D loss: 0.573(R 0.477, F0.670)] [D acc: 0.711(R 0.703, F 0.719)] [G loss: 0.573] [G acc: 0.094]\n",
      "1311 [D loss: 0.534(R 0.599, F0.468)] [D acc: 0.734(R 0.594, F 0.875)] [G loss: 0.534] [G acc: 0.125]\n",
      "1312 [D loss: 0.614(R 0.590, F0.638)] [D acc: 0.664(R 0.625, F 0.703)] [G loss: 0.614] [G acc: 0.125]\n",
      "1313 [D loss: 0.521(R 0.556, F0.487)] [D acc: 0.742(R 0.703, F 0.781)] [G loss: 0.521] [G acc: 0.094]\n",
      "1314 [D loss: 0.587(R 0.670, F0.503)] [D acc: 0.719(R 0.578, F 0.859)] [G loss: 0.587] [G acc: 0.047]\n",
      "1315 [D loss: 0.607(R 0.552, F0.661)] [D acc: 0.641(R 0.641, F 0.641)] [G loss: 0.607] [G acc: 0.125]\n",
      "1316 [D loss: 0.585(R 0.603, F0.568)] [D acc: 0.672(R 0.656, F 0.688)] [G loss: 0.585] [G acc: 0.172]\n",
      "1317 [D loss: 0.513(R 0.506, F0.521)] [D acc: 0.789(R 0.703, F 0.875)] [G loss: 0.513] [G acc: 0.109]\n",
      "1318 [D loss: 0.765(R 0.526, F1.004)] [D acc: 0.672(R 0.688, F 0.656)] [G loss: 0.765] [G acc: 0.016]\n",
      "1319 [D loss: 0.589(R 0.728, F0.450)] [D acc: 0.672(R 0.500, F 0.844)] [G loss: 0.589] [G acc: 0.078]\n",
      "1320 [D loss: 0.531(R 0.623, F0.438)] [D acc: 0.734(R 0.562, F 0.906)] [G loss: 0.531] [G acc: 0.125]\n",
      "1321 [D loss: 0.545(R 0.493, F0.597)] [D acc: 0.750(R 0.750, F 0.750)] [G loss: 0.545] [G acc: 0.125]\n",
      "1322 [D loss: 0.589(R 0.622, F0.556)] [D acc: 0.648(R 0.547, F 0.750)] [G loss: 0.589] [G acc: 0.062]\n",
      "1323 [D loss: 0.552(R 0.516, F0.587)] [D acc: 0.727(R 0.703, F 0.750)] [G loss: 0.552] [G acc: 0.156]\n",
      "1324 [D loss: 0.625(R 0.612, F0.637)] [D acc: 0.656(R 0.594, F 0.719)] [G loss: 0.625] [G acc: 0.125]\n",
      "1325 [D loss: 0.554(R 0.493, F0.615)] [D acc: 0.703(R 0.719, F 0.688)] [G loss: 0.554] [G acc: 0.188]\n",
      "1326 [D loss: 0.557(R 0.577, F0.536)] [D acc: 0.703(R 0.656, F 0.750)] [G loss: 0.557] [G acc: 0.156]\n",
      "1327 [D loss: 0.603(R 0.570, F0.637)] [D acc: 0.648(R 0.672, F 0.625)] [G loss: 0.603] [G acc: 0.141]\n",
      "1328 [D loss: 0.527(R 0.578, F0.476)] [D acc: 0.766(R 0.703, F 0.828)] [G loss: 0.527] [G acc: 0.094]\n",
      "1329 [D loss: 0.577(R 0.539, F0.615)] [D acc: 0.719(R 0.672, F 0.766)] [G loss: 0.577] [G acc: 0.094]\n",
      "1330 [D loss: 0.445(R 0.515, F0.375)] [D acc: 0.859(R 0.766, F 0.953)] [G loss: 0.445] [G acc: 0.031]\n",
      "1331 [D loss: 0.618(R 0.638, F0.599)] [D acc: 0.633(R 0.562, F 0.703)] [G loss: 0.618] [G acc: 0.094]\n",
      "1332 [D loss: 0.611(R 0.519, F0.703)] [D acc: 0.672(R 0.703, F 0.641)] [G loss: 0.611] [G acc: 0.188]\n",
      "1333 [D loss: 0.583(R 0.604, F0.563)] [D acc: 0.656(R 0.625, F 0.688)] [G loss: 0.583] [G acc: 0.156]\n",
      "1334 [D loss: 0.561(R 0.496, F0.626)] [D acc: 0.719(R 0.750, F 0.688)] [G loss: 0.561] [G acc: 0.062]\n",
      "1335 [D loss: 0.459(R 0.488, F0.430)] [D acc: 0.797(R 0.703, F 0.891)] [G loss: 0.459] [G acc: 0.109]\n",
      "1336 [D loss: 0.562(R 0.475, F0.648)] [D acc: 0.703(R 0.719, F 0.688)] [G loss: 0.562] [G acc: 0.109]\n",
      "1337 [D loss: 0.550(R 0.541, F0.560)] [D acc: 0.711(R 0.672, F 0.750)] [G loss: 0.550] [G acc: 0.109]\n",
      "1338 [D loss: 0.538(R 0.572, F0.503)] [D acc: 0.703(R 0.641, F 0.766)] [G loss: 0.538] [G acc: 0.156]\n",
      "1339 [D loss: 0.495(R 0.525, F0.465)] [D acc: 0.766(R 0.688, F 0.844)] [G loss: 0.495] [G acc: 0.188]\n",
      "1340 [D loss: 0.605(R 0.485, F0.725)] [D acc: 0.664(R 0.719, F 0.609)] [G loss: 0.605] [G acc: 0.078]\n",
      "1341 [D loss: 0.481(R 0.487, F0.475)] [D acc: 0.742(R 0.688, F 0.797)] [G loss: 0.481] [G acc: 0.094]\n",
      "1342 [D loss: 0.638(R 0.611, F0.665)] [D acc: 0.633(R 0.609, F 0.656)] [G loss: 0.638] [G acc: 0.094]\n",
      "1343 [D loss: 0.609(R 0.639, F0.580)] [D acc: 0.672(R 0.578, F 0.766)] [G loss: 0.609] [G acc: 0.078]\n",
      "1344 [D loss: 0.516(R 0.501, F0.531)] [D acc: 0.781(R 0.750, F 0.812)] [G loss: 0.516] [G acc: 0.094]\n",
      "1345 [D loss: 0.574(R 0.573, F0.575)] [D acc: 0.680(R 0.594, F 0.766)] [G loss: 0.574] [G acc: 0.047]\n",
      "1346 [D loss: 0.554(R 0.616, F0.492)] [D acc: 0.719(R 0.641, F 0.797)] [G loss: 0.554] [G acc: 0.109]\n",
      "1347 [D loss: 0.539(R 0.531, F0.547)] [D acc: 0.695(R 0.672, F 0.719)] [G loss: 0.539] [G acc: 0.047]\n",
      "1348 [D loss: 0.559(R 0.553, F0.564)] [D acc: 0.688(R 0.672, F 0.703)] [G loss: 0.559] [G acc: 0.078]\n",
      "1349 [D loss: 0.555(R 0.655, F0.455)] [D acc: 0.695(R 0.531, F 0.859)] [G loss: 0.555] [G acc: 0.062]\n",
      "1350 [D loss: 0.570(R 0.566, F0.573)] [D acc: 0.703(R 0.656, F 0.750)] [G loss: 0.570] [G acc: 0.078]\n",
      "1351 [D loss: 0.576(R 0.510, F0.642)] [D acc: 0.703(R 0.688, F 0.719)] [G loss: 0.576] [G acc: 0.141]\n",
      "1352 [D loss: 0.546(R 0.602, F0.491)] [D acc: 0.695(R 0.609, F 0.781)] [G loss: 0.546] [G acc: 0.141]\n",
      "1353 [D loss: 0.518(R 0.536, F0.500)] [D acc: 0.719(R 0.625, F 0.812)] [G loss: 0.518] [G acc: 0.047]\n",
      "1354 [D loss: 0.563(R 0.533, F0.592)] [D acc: 0.711(R 0.641, F 0.781)] [G loss: 0.563] [G acc: 0.156]\n",
      "1355 [D loss: 0.582(R 0.568, F0.596)] [D acc: 0.711(R 0.641, F 0.781)] [G loss: 0.582] [G acc: 0.062]\n",
      "1356 [D loss: 0.558(R 0.532, F0.584)] [D acc: 0.727(R 0.703, F 0.750)] [G loss: 0.558] [G acc: 0.094]\n",
      "1357 [D loss: 0.517(R 0.560, F0.475)] [D acc: 0.766(R 0.703, F 0.828)] [G loss: 0.517] [G acc: 0.078]\n",
      "1358 [D loss: 0.613(R 0.634, F0.592)] [D acc: 0.719(R 0.656, F 0.781)] [G loss: 0.613] [G acc: 0.078]\n",
      "1359 [D loss: 0.549(R 0.610, F0.488)] [D acc: 0.719(R 0.656, F 0.781)] [G loss: 0.549] [G acc: 0.109]\n",
      "1360 [D loss: 0.471(R 0.412, F0.530)] [D acc: 0.805(R 0.766, F 0.844)] [G loss: 0.471] [G acc: 0.062]\n",
      "1361 [D loss: 0.581(R 0.637, F0.525)] [D acc: 0.664(R 0.547, F 0.781)] [G loss: 0.581] [G acc: 0.109]\n",
      "1362 [D loss: 0.488(R 0.480, F0.496)] [D acc: 0.773(R 0.734, F 0.812)] [G loss: 0.488] [G acc: 0.062]\n",
      "1363 [D loss: 0.544(R 0.542, F0.545)] [D acc: 0.719(R 0.672, F 0.766)] [G loss: 0.544] [G acc: 0.047]\n",
      "1364 [D loss: 0.537(R 0.607, F0.467)] [D acc: 0.727(R 0.656, F 0.797)] [G loss: 0.537] [G acc: 0.094]\n",
      "1365 [D loss: 0.596(R 0.519, F0.673)] [D acc: 0.766(R 0.719, F 0.812)] [G loss: 0.596] [G acc: 0.125]\n",
      "1366 [D loss: 0.571(R 0.512, F0.631)] [D acc: 0.703(R 0.719, F 0.688)] [G loss: 0.571] [G acc: 0.109]\n",
      "1367 [D loss: 0.607(R 0.671, F0.542)] [D acc: 0.641(R 0.500, F 0.781)] [G loss: 0.607] [G acc: 0.156]\n",
      "1368 [D loss: 0.553(R 0.488, F0.618)] [D acc: 0.688(R 0.703, F 0.672)] [G loss: 0.553] [G acc: 0.078]\n",
      "1369 [D loss: 0.577(R 0.615, F0.540)] [D acc: 0.742(R 0.672, F 0.812)] [G loss: 0.577] [G acc: 0.047]\n",
      "1370 [D loss: 0.612(R 0.625, F0.599)] [D acc: 0.664(R 0.609, F 0.719)] [G loss: 0.612] [G acc: 0.094]\n",
      "1371 [D loss: 0.490(R 0.531, F0.449)] [D acc: 0.750(R 0.656, F 0.844)] [G loss: 0.490] [G acc: 0.203]\n",
      "1372 [D loss: 0.610(R 0.592, F0.628)] [D acc: 0.664(R 0.625, F 0.703)] [G loss: 0.610] [G acc: 0.141]\n",
      "1373 [D loss: 0.597(R 0.632, F0.562)] [D acc: 0.672(R 0.609, F 0.734)] [G loss: 0.597] [G acc: 0.047]\n",
      "1374 [D loss: 0.590(R 0.605, F0.576)] [D acc: 0.656(R 0.578, F 0.734)] [G loss: 0.590] [G acc: 0.156]\n",
      "1375 [D loss: 0.504(R 0.494, F0.515)] [D acc: 0.789(R 0.734, F 0.844)] [G loss: 0.504] [G acc: 0.062]\n",
      "1376 [D loss: 0.599(R 0.628, F0.570)] [D acc: 0.695(R 0.625, F 0.766)] [G loss: 0.599] [G acc: 0.094]\n",
      "1377 [D loss: 0.593(R 0.679, F0.507)] [D acc: 0.703(R 0.578, F 0.828)] [G loss: 0.593] [G acc: 0.172]\n",
      "1378 [D loss: 0.689(R 0.522, F0.856)] [D acc: 0.641(R 0.688, F 0.594)] [G loss: 0.689] [G acc: 0.094]\n",
      "1379 [D loss: 0.581(R 0.613, F0.549)] [D acc: 0.648(R 0.562, F 0.734)] [G loss: 0.581] [G acc: 0.062]\n",
      "1380 [D loss: 0.567(R 0.583, F0.550)] [D acc: 0.711(R 0.625, F 0.797)] [G loss: 0.567] [G acc: 0.109]\n",
      "1381 [D loss: 0.617(R 0.600, F0.634)] [D acc: 0.633(R 0.578, F 0.688)] [G loss: 0.617] [G acc: 0.125]\n",
      "1382 [D loss: 0.671(R 0.841, F0.500)] [D acc: 0.570(R 0.375, F 0.766)] [G loss: 0.671] [G acc: 0.078]\n",
      "1383 [D loss: 0.488(R 0.454, F0.522)] [D acc: 0.797(R 0.797, F 0.797)] [G loss: 0.488] [G acc: 0.109]\n",
      "1384 [D loss: 0.581(R 0.481, F0.682)] [D acc: 0.750(R 0.703, F 0.797)] [G loss: 0.581] [G acc: 0.141]\n",
      "1385 [D loss: 0.682(R 0.824, F0.541)] [D acc: 0.602(R 0.469, F 0.734)] [G loss: 0.682] [G acc: 0.219]\n",
      "1386 [D loss: 0.503(R 0.493, F0.513)] [D acc: 0.758(R 0.734, F 0.781)] [G loss: 0.503] [G acc: 0.109]\n",
      "1387 [D loss: 0.530(R 0.569, F0.490)] [D acc: 0.711(R 0.578, F 0.844)] [G loss: 0.530] [G acc: 0.172]\n",
      "1388 [D loss: 0.578(R 0.474, F0.682)] [D acc: 0.672(R 0.688, F 0.656)] [G loss: 0.578] [G acc: 0.031]\n",
      "1389 [D loss: 0.548(R 0.616, F0.480)] [D acc: 0.750(R 0.625, F 0.875)] [G loss: 0.548] [G acc: 0.125]\n",
      "1390 [D loss: 0.539(R 0.582, F0.496)] [D acc: 0.680(R 0.594, F 0.766)] [G loss: 0.539] [G acc: 0.141]\n",
      "1391 [D loss: 0.483(R 0.510, F0.455)] [D acc: 0.750(R 0.688, F 0.812)] [G loss: 0.483] [G acc: 0.203]\n",
      "1392 [D loss: 0.542(R 0.481, F0.602)] [D acc: 0.742(R 0.766, F 0.719)] [G loss: 0.542] [G acc: 0.078]\n",
      "1393 [D loss: 0.567(R 0.706, F0.428)] [D acc: 0.672(R 0.516, F 0.828)] [G loss: 0.567] [G acc: 0.094]\n",
      "1394 [D loss: 0.649(R 0.768, F0.531)] [D acc: 0.695(R 0.531, F 0.859)] [G loss: 0.649] [G acc: 0.094]\n",
      "1395 [D loss: 0.472(R 0.445, F0.499)] [D acc: 0.773(R 0.766, F 0.781)] [G loss: 0.472] [G acc: 0.109]\n",
      "1396 [D loss: 0.632(R 0.625, F0.640)] [D acc: 0.680(R 0.641, F 0.719)] [G loss: 0.632] [G acc: 0.172]\n",
      "1397 [D loss: 0.599(R 0.574, F0.624)] [D acc: 0.703(R 0.719, F 0.688)] [G loss: 0.599] [G acc: 0.125]\n",
      "1398 [D loss: 0.512(R 0.497, F0.527)] [D acc: 0.773(R 0.734, F 0.812)] [G loss: 0.512] [G acc: 0.109]\n",
      "1399 [D loss: 0.618(R 0.456, F0.781)] [D acc: 0.719(R 0.766, F 0.672)] [G loss: 0.618] [G acc: 0.125]\n",
      "INFO:tensorflow:Assets written to: ram://78fd0533-06f5-4fef-91e9-b658045b6a42/assets\n",
      "INFO:tensorflow:Assets written to: ram://fb84493c-3282-47e5-894d-c89832059a53/assets\n",
      "INFO:tensorflow:Assets written to: ram://7274319d-c093-4e6e-abf3-3f0c98fc5a61/assets\n",
      "1400 [D loss: 0.596(R 0.610, F0.582)] [D acc: 0.695(R 0.625, F 0.766)] [G loss: 0.596] [G acc: 0.188]\n",
      "1401 [D loss: 0.498(R 0.495, F0.500)] [D acc: 0.766(R 0.703, F 0.828)] [G loss: 0.498] [G acc: 0.094]\n",
      "1402 [D loss: 0.577(R 0.631, F0.523)] [D acc: 0.664(R 0.594, F 0.734)] [G loss: 0.577] [G acc: 0.141]\n",
      "1403 [D loss: 0.458(R 0.463, F0.453)] [D acc: 0.797(R 0.734, F 0.859)] [G loss: 0.458] [G acc: 0.094]\n",
      "1404 [D loss: 0.462(R 0.476, F0.447)] [D acc: 0.820(R 0.781, F 0.859)] [G loss: 0.462] [G acc: 0.141]\n",
      "1405 [D loss: 0.558(R 0.558, F0.557)] [D acc: 0.688(R 0.625, F 0.750)] [G loss: 0.558] [G acc: 0.141]\n",
      "1406 [D loss: 0.614(R 0.636, F0.593)] [D acc: 0.695(R 0.625, F 0.766)] [G loss: 0.614] [G acc: 0.141]\n",
      "1407 [D loss: 0.500(R 0.463, F0.537)] [D acc: 0.750(R 0.734, F 0.766)] [G loss: 0.500] [G acc: 0.125]\n",
      "1408 [D loss: 0.540(R 0.571, F0.510)] [D acc: 0.766(R 0.750, F 0.781)] [G loss: 0.540] [G acc: 0.109]\n",
      "1409 [D loss: 0.497(R 0.490, F0.505)] [D acc: 0.805(R 0.750, F 0.859)] [G loss: 0.497] [G acc: 0.125]\n",
      "1410 [D loss: 0.648(R 0.597, F0.698)] [D acc: 0.602(R 0.594, F 0.609)] [G loss: 0.648] [G acc: 0.125]\n",
      "1411 [D loss: 0.548(R 0.640, F0.457)] [D acc: 0.773(R 0.641, F 0.906)] [G loss: 0.548] [G acc: 0.078]\n",
      "1412 [D loss: 0.559(R 0.588, F0.530)] [D acc: 0.703(R 0.703, F 0.703)] [G loss: 0.559] [G acc: 0.234]\n",
      "1413 [D loss: 0.566(R 0.647, F0.485)] [D acc: 0.703(R 0.609, F 0.797)] [G loss: 0.566] [G acc: 0.078]\n",
      "1414 [D loss: 0.499(R 0.492, F0.506)] [D acc: 0.742(R 0.719, F 0.766)] [G loss: 0.499] [G acc: 0.094]\n",
      "1415 [D loss: 0.534(R 0.463, F0.605)] [D acc: 0.688(R 0.672, F 0.703)] [G loss: 0.534] [G acc: 0.109]\n",
      "1416 [D loss: 0.619(R 0.650, F0.588)] [D acc: 0.664(R 0.547, F 0.781)] [G loss: 0.619] [G acc: 0.125]\n",
      "1417 [D loss: 0.537(R 0.476, F0.599)] [D acc: 0.727(R 0.703, F 0.750)] [G loss: 0.537] [G acc: 0.062]\n",
      "1418 [D loss: 0.552(R 0.578, F0.526)] [D acc: 0.711(R 0.641, F 0.781)] [G loss: 0.552] [G acc: 0.094]\n",
      "1419 [D loss: 0.530(R 0.536, F0.523)] [D acc: 0.773(R 0.719, F 0.828)] [G loss: 0.530] [G acc: 0.109]\n",
      "1420 [D loss: 0.428(R 0.405, F0.451)] [D acc: 0.828(R 0.844, F 0.812)] [G loss: 0.428] [G acc: 0.062]\n",
      "1421 [D loss: 0.548(R 0.438, F0.658)] [D acc: 0.711(R 0.734, F 0.688)] [G loss: 0.548] [G acc: 0.109]\n",
      "1422 [D loss: 0.552(R 0.476, F0.628)] [D acc: 0.758(R 0.734, F 0.781)] [G loss: 0.552] [G acc: 0.062]\n",
      "1423 [D loss: 0.559(R 0.655, F0.464)] [D acc: 0.664(R 0.484, F 0.844)] [G loss: 0.559] [G acc: 0.016]\n",
      "1424 [D loss: 0.630(R 0.636, F0.624)] [D acc: 0.688(R 0.562, F 0.812)] [G loss: 0.630] [G acc: 0.109]\n",
      "1425 [D loss: 0.503(R 0.542, F0.464)] [D acc: 0.773(R 0.719, F 0.828)] [G loss: 0.503] [G acc: 0.062]\n",
      "1426 [D loss: 0.543(R 0.591, F0.495)] [D acc: 0.742(R 0.672, F 0.812)] [G loss: 0.543] [G acc: 0.094]\n",
      "1427 [D loss: 0.564(R 0.579, F0.550)] [D acc: 0.711(R 0.641, F 0.781)] [G loss: 0.564] [G acc: 0.156]\n",
      "1428 [D loss: 0.555(R 0.568, F0.542)] [D acc: 0.758(R 0.688, F 0.828)] [G loss: 0.555] [G acc: 0.062]\n",
      "1429 [D loss: 0.548(R 0.525, F0.570)] [D acc: 0.688(R 0.656, F 0.719)] [G loss: 0.548] [G acc: 0.156]\n",
      "1430 [D loss: 0.637(R 0.687, F0.587)] [D acc: 0.625(R 0.484, F 0.766)] [G loss: 0.637] [G acc: 0.156]\n",
      "1431 [D loss: 0.562(R 0.526, F0.598)] [D acc: 0.695(R 0.688, F 0.703)] [G loss: 0.562] [G acc: 0.109]\n",
      "1432 [D loss: 0.556(R 0.596, F0.517)] [D acc: 0.703(R 0.594, F 0.812)] [G loss: 0.556] [G acc: 0.109]\n",
      "1433 [D loss: 0.529(R 0.521, F0.537)] [D acc: 0.766(R 0.703, F 0.828)] [G loss: 0.529] [G acc: 0.125]\n",
      "1434 [D loss: 0.503(R 0.520, F0.485)] [D acc: 0.781(R 0.734, F 0.828)] [G loss: 0.503] [G acc: 0.141]\n",
      "1435 [D loss: 0.573(R 0.539, F0.607)] [D acc: 0.703(R 0.672, F 0.734)] [G loss: 0.573] [G acc: 0.109]\n",
      "1436 [D loss: 0.527(R 0.604, F0.449)] [D acc: 0.719(R 0.609, F 0.828)] [G loss: 0.527] [G acc: 0.141]\n",
      "1437 [D loss: 0.582(R 0.617, F0.548)] [D acc: 0.656(R 0.578, F 0.734)] [G loss: 0.582] [G acc: 0.078]\n",
      "1438 [D loss: 0.598(R 0.548, F0.648)] [D acc: 0.633(R 0.609, F 0.656)] [G loss: 0.598] [G acc: 0.188]\n",
      "1439 [D loss: 0.553(R 0.513, F0.592)] [D acc: 0.711(R 0.672, F 0.750)] [G loss: 0.553] [G acc: 0.094]\n",
      "1440 [D loss: 0.520(R 0.453, F0.587)] [D acc: 0.773(R 0.750, F 0.797)] [G loss: 0.520] [G acc: 0.031]\n",
      "1441 [D loss: 0.515(R 0.589, F0.442)] [D acc: 0.750(R 0.625, F 0.875)] [G loss: 0.515] [G acc: 0.047]\n",
      "1442 [D loss: 0.481(R 0.516, F0.446)] [D acc: 0.781(R 0.672, F 0.891)] [G loss: 0.481] [G acc: 0.047]\n",
      "1443 [D loss: 0.556(R 0.596, F0.516)] [D acc: 0.711(R 0.672, F 0.750)] [G loss: 0.556] [G acc: 0.062]\n",
      "1444 [D loss: 0.567(R 0.550, F0.584)] [D acc: 0.680(R 0.625, F 0.734)] [G loss: 0.567] [G acc: 0.094]\n",
      "1445 [D loss: 0.608(R 0.600, F0.616)] [D acc: 0.664(R 0.656, F 0.672)] [G loss: 0.608] [G acc: 0.094]\n",
      "1446 [D loss: 0.487(R 0.503, F0.471)] [D acc: 0.711(R 0.625, F 0.797)] [G loss: 0.487] [G acc: 0.125]\n",
      "1447 [D loss: 0.585(R 0.563, F0.606)] [D acc: 0.727(R 0.719, F 0.734)] [G loss: 0.585] [G acc: 0.078]\n",
      "1448 [D loss: 0.552(R 0.564, F0.540)] [D acc: 0.703(R 0.656, F 0.750)] [G loss: 0.552] [G acc: 0.125]\n",
      "1449 [D loss: 0.591(R 0.621, F0.562)] [D acc: 0.742(R 0.625, F 0.859)] [G loss: 0.591] [G acc: 0.047]\n",
      "1450 [D loss: 0.548(R 0.595, F0.502)] [D acc: 0.750(R 0.672, F 0.828)] [G loss: 0.548] [G acc: 0.141]\n",
      "1451 [D loss: 0.538(R 0.482, F0.593)] [D acc: 0.750(R 0.750, F 0.750)] [G loss: 0.538] [G acc: 0.094]\n",
      "1452 [D loss: 0.548(R 0.583, F0.514)] [D acc: 0.750(R 0.672, F 0.828)] [G loss: 0.548] [G acc: 0.125]\n",
      "1453 [D loss: 0.499(R 0.473, F0.525)] [D acc: 0.766(R 0.750, F 0.781)] [G loss: 0.499] [G acc: 0.109]\n",
      "1454 [D loss: 0.494(R 0.469, F0.520)] [D acc: 0.766(R 0.734, F 0.797)] [G loss: 0.494] [G acc: 0.141]\n",
      "1455 [D loss: 0.574(R 0.505, F0.643)] [D acc: 0.680(R 0.703, F 0.656)] [G loss: 0.574] [G acc: 0.094]\n",
      "1456 [D loss: 0.543(R 0.595, F0.491)] [D acc: 0.688(R 0.578, F 0.797)] [G loss: 0.543] [G acc: 0.062]\n",
      "1457 [D loss: 0.520(R 0.566, F0.475)] [D acc: 0.703(R 0.625, F 0.781)] [G loss: 0.520] [G acc: 0.078]\n",
      "1458 [D loss: 0.467(R 0.452, F0.482)] [D acc: 0.766(R 0.734, F 0.797)] [G loss: 0.467] [G acc: 0.094]\n",
      "1459 [D loss: 0.593(R 0.629, F0.556)] [D acc: 0.703(R 0.672, F 0.734)] [G loss: 0.593] [G acc: 0.141]\n",
      "1460 [D loss: 0.566(R 0.646, F0.487)] [D acc: 0.719(R 0.578, F 0.859)] [G loss: 0.566] [G acc: 0.078]\n",
      "1461 [D loss: 0.495(R 0.572, F0.419)] [D acc: 0.773(R 0.672, F 0.875)] [G loss: 0.495] [G acc: 0.141]\n",
      "1462 [D loss: 0.675(R 0.665, F0.685)] [D acc: 0.648(R 0.594, F 0.703)] [G loss: 0.675] [G acc: 0.109]\n",
      "1463 [D loss: 0.527(R 0.485, F0.568)] [D acc: 0.719(R 0.703, F 0.734)] [G loss: 0.527] [G acc: 0.109]\n",
      "1464 [D loss: 0.491(R 0.483, F0.499)] [D acc: 0.766(R 0.688, F 0.844)] [G loss: 0.491] [G acc: 0.125]\n",
      "1465 [D loss: 0.423(R 0.389, F0.457)] [D acc: 0.773(R 0.719, F 0.828)] [G loss: 0.423] [G acc: 0.094]\n",
      "1466 [D loss: 0.494(R 0.614, F0.374)] [D acc: 0.758(R 0.641, F 0.875)] [G loss: 0.494] [G acc: 0.016]\n",
      "1467 [D loss: 0.610(R 0.549, F0.671)] [D acc: 0.742(R 0.750, F 0.734)] [G loss: 0.610] [G acc: 0.078]\n",
      "1468 [D loss: 0.514(R 0.539, F0.488)] [D acc: 0.742(R 0.672, F 0.812)] [G loss: 0.514] [G acc: 0.156]\n",
      "1469 [D loss: 0.493(R 0.472, F0.513)] [D acc: 0.789(R 0.750, F 0.828)] [G loss: 0.493] [G acc: 0.031]\n",
      "1470 [D loss: 0.508(R 0.550, F0.465)] [D acc: 0.789(R 0.672, F 0.906)] [G loss: 0.508] [G acc: 0.188]\n",
      "1471 [D loss: 0.563(R 0.539, F0.587)] [D acc: 0.703(R 0.609, F 0.797)] [G loss: 0.563] [G acc: 0.109]\n",
      "1472 [D loss: 0.622(R 0.523, F0.722)] [D acc: 0.703(R 0.656, F 0.750)] [G loss: 0.622] [G acc: 0.125]\n",
      "1473 [D loss: 0.512(R 0.575, F0.450)] [D acc: 0.750(R 0.641, F 0.859)] [G loss: 0.512] [G acc: 0.125]\n",
      "1474 [D loss: 0.610(R 0.661, F0.559)] [D acc: 0.703(R 0.625, F 0.781)] [G loss: 0.610] [G acc: 0.047]\n",
      "1475 [D loss: 0.503(R 0.566, F0.439)] [D acc: 0.727(R 0.594, F 0.859)] [G loss: 0.503] [G acc: 0.125]\n",
      "1476 [D loss: 0.636(R 0.534, F0.738)] [D acc: 0.656(R 0.656, F 0.656)] [G loss: 0.636] [G acc: 0.141]\n",
      "1477 [D loss: 0.607(R 0.699, F0.516)] [D acc: 0.656(R 0.547, F 0.766)] [G loss: 0.607] [G acc: 0.141]\n",
      "1478 [D loss: 0.535(R 0.547, F0.522)] [D acc: 0.750(R 0.719, F 0.781)] [G loss: 0.535] [G acc: 0.078]\n",
      "1479 [D loss: 0.589(R 0.589, F0.588)] [D acc: 0.672(R 0.672, F 0.672)] [G loss: 0.589] [G acc: 0.141]\n",
      "1480 [D loss: 0.615(R 0.465, F0.764)] [D acc: 0.703(R 0.750, F 0.656)] [G loss: 0.615] [G acc: 0.172]\n",
      "1481 [D loss: 0.581(R 0.674, F0.488)] [D acc: 0.703(R 0.594, F 0.812)] [G loss: 0.581] [G acc: 0.156]\n",
      "1482 [D loss: 0.555(R 0.514, F0.596)] [D acc: 0.703(R 0.672, F 0.734)] [G loss: 0.555] [G acc: 0.141]\n",
      "1483 [D loss: 0.545(R 0.539, F0.551)] [D acc: 0.727(R 0.656, F 0.797)] [G loss: 0.545] [G acc: 0.094]\n",
      "1484 [D loss: 0.654(R 0.730, F0.578)] [D acc: 0.633(R 0.547, F 0.719)] [G loss: 0.654] [G acc: 0.109]\n",
      "1485 [D loss: 0.548(R 0.612, F0.484)] [D acc: 0.719(R 0.609, F 0.828)] [G loss: 0.548] [G acc: 0.094]\n",
      "1486 [D loss: 0.565(R 0.651, F0.480)] [D acc: 0.695(R 0.547, F 0.844)] [G loss: 0.565] [G acc: 0.188]\n",
      "1487 [D loss: 0.531(R 0.505, F0.557)] [D acc: 0.719(R 0.703, F 0.734)] [G loss: 0.531] [G acc: 0.109]\n",
      "1488 [D loss: 0.561(R 0.563, F0.558)] [D acc: 0.727(R 0.641, F 0.812)] [G loss: 0.561] [G acc: 0.094]\n",
      "1489 [D loss: 0.540(R 0.536, F0.544)] [D acc: 0.711(R 0.703, F 0.719)] [G loss: 0.540] [G acc: 0.062]\n",
      "1490 [D loss: 0.575(R 0.600, F0.551)] [D acc: 0.711(R 0.672, F 0.750)] [G loss: 0.575] [G acc: 0.188]\n",
      "1491 [D loss: 0.524(R 0.492, F0.556)] [D acc: 0.773(R 0.734, F 0.812)] [G loss: 0.524] [G acc: 0.078]\n",
      "1492 [D loss: 0.508(R 0.477, F0.540)] [D acc: 0.773(R 0.734, F 0.812)] [G loss: 0.508] [G acc: 0.094]\n",
      "1493 [D loss: 0.605(R 0.616, F0.594)] [D acc: 0.641(R 0.625, F 0.656)] [G loss: 0.605] [G acc: 0.125]\n",
      "1494 [D loss: 0.531(R 0.586, F0.476)] [D acc: 0.734(R 0.656, F 0.812)] [G loss: 0.531] [G acc: 0.047]\n",
      "1495 [D loss: 0.479(R 0.344, F0.613)] [D acc: 0.773(R 0.828, F 0.719)] [G loss: 0.479] [G acc: 0.094]\n",
      "1496 [D loss: 0.470(R 0.454, F0.486)] [D acc: 0.805(R 0.781, F 0.828)] [G loss: 0.470] [G acc: 0.141]\n",
      "1497 [D loss: 0.461(R 0.498, F0.423)] [D acc: 0.773(R 0.703, F 0.844)] [G loss: 0.461] [G acc: 0.156]\n",
      "1498 [D loss: 0.560(R 0.614, F0.506)] [D acc: 0.680(R 0.594, F 0.766)] [G loss: 0.560] [G acc: 0.141]\n",
      "1499 [D loss: 0.656(R 0.622, F0.689)] [D acc: 0.656(R 0.625, F 0.688)] [G loss: 0.656] [G acc: 0.172]\n",
      "INFO:tensorflow:Assets written to: ram://d7ebc42e-1cf5-4f96-b62b-f08d6857a870/assets\n",
      "INFO:tensorflow:Assets written to: ram://85c1df7f-dc56-4b27-bb1e-149377f213f7/assets\n",
      "INFO:tensorflow:Assets written to: ram://7d17b9a9-1a2c-435a-b42b-c04d717ba206/assets\n",
      "1500 [D loss: 0.578(R 0.676, F0.481)] [D acc: 0.711(R 0.594, F 0.828)] [G loss: 0.578] [G acc: 0.109]\n",
      "1501 [D loss: 0.574(R 0.546, F0.602)] [D acc: 0.727(R 0.703, F 0.750)] [G loss: 0.574] [G acc: 0.172]\n",
      "1502 [D loss: 0.545(R 0.521, F0.568)] [D acc: 0.719(R 0.656, F 0.781)] [G loss: 0.545] [G acc: 0.109]\n",
      "1503 [D loss: 0.532(R 0.481, F0.583)] [D acc: 0.750(R 0.703, F 0.797)] [G loss: 0.532] [G acc: 0.047]\n",
      "1504 [D loss: 0.711(R 0.805, F0.616)] [D acc: 0.562(R 0.438, F 0.688)] [G loss: 0.711] [G acc: 0.188]\n",
      "1505 [D loss: 0.524(R 0.484, F0.564)] [D acc: 0.703(R 0.672, F 0.734)] [G loss: 0.524] [G acc: 0.094]\n",
      "1506 [D loss: 0.476(R 0.576, F0.376)] [D acc: 0.820(R 0.688, F 0.953)] [G loss: 0.476] [G acc: 0.109]\n",
      "1507 [D loss: 0.657(R 0.662, F0.652)] [D acc: 0.633(R 0.594, F 0.672)] [G loss: 0.657] [G acc: 0.125]\n",
      "1508 [D loss: 0.558(R 0.527, F0.590)] [D acc: 0.742(R 0.766, F 0.719)] [G loss: 0.558] [G acc: 0.141]\n",
      "1509 [D loss: 0.547(R 0.549, F0.544)] [D acc: 0.680(R 0.641, F 0.719)] [G loss: 0.547] [G acc: 0.125]\n",
      "1510 [D loss: 0.507(R 0.546, F0.467)] [D acc: 0.750(R 0.641, F 0.859)] [G loss: 0.507] [G acc: 0.047]\n",
      "1511 [D loss: 0.546(R 0.527, F0.565)] [D acc: 0.750(R 0.719, F 0.781)] [G loss: 0.546] [G acc: 0.078]\n",
      "1512 [D loss: 0.527(R 0.468, F0.585)] [D acc: 0.734(R 0.719, F 0.750)] [G loss: 0.527] [G acc: 0.141]\n",
      "1513 [D loss: 0.510(R 0.453, F0.567)] [D acc: 0.711(R 0.719, F 0.703)] [G loss: 0.510] [G acc: 0.094]\n",
      "1514 [D loss: 0.579(R 0.617, F0.541)] [D acc: 0.680(R 0.594, F 0.766)] [G loss: 0.579] [G acc: 0.078]\n",
      "1515 [D loss: 0.556(R 0.602, F0.511)] [D acc: 0.703(R 0.594, F 0.812)] [G loss: 0.556] [G acc: 0.078]\n",
      "1516 [D loss: 0.564(R 0.560, F0.569)] [D acc: 0.703(R 0.656, F 0.750)] [G loss: 0.564] [G acc: 0.109]\n",
      "1517 [D loss: 0.580(R 0.593, F0.566)] [D acc: 0.617(R 0.562, F 0.672)] [G loss: 0.580] [G acc: 0.094]\n",
      "1518 [D loss: 0.497(R 0.450, F0.544)] [D acc: 0.727(R 0.734, F 0.719)] [G loss: 0.497] [G acc: 0.109]\n",
      "1519 [D loss: 0.642(R 0.814, F0.469)] [D acc: 0.672(R 0.484, F 0.859)] [G loss: 0.642] [G acc: 0.062]\n",
      "1520 [D loss: 0.516(R 0.502, F0.530)] [D acc: 0.742(R 0.688, F 0.797)] [G loss: 0.516] [G acc: 0.047]\n",
      "1521 [D loss: 0.479(R 0.464, F0.494)] [D acc: 0.844(R 0.781, F 0.906)] [G loss: 0.479] [G acc: 0.062]\n",
      "1522 [D loss: 0.514(R 0.535, F0.492)] [D acc: 0.758(R 0.703, F 0.812)] [G loss: 0.514] [G acc: 0.094]\n",
      "1523 [D loss: 0.577(R 0.555, F0.598)] [D acc: 0.680(R 0.641, F 0.719)] [G loss: 0.577] [G acc: 0.047]\n",
      "1524 [D loss: 0.507(R 0.529, F0.484)] [D acc: 0.773(R 0.703, F 0.844)] [G loss: 0.507] [G acc: 0.047]\n",
      "1525 [D loss: 0.590(R 0.489, F0.691)] [D acc: 0.727(R 0.734, F 0.719)] [G loss: 0.590] [G acc: 0.188]\n",
      "1526 [D loss: 0.587(R 0.583, F0.590)] [D acc: 0.672(R 0.594, F 0.750)] [G loss: 0.587] [G acc: 0.109]\n",
      "1527 [D loss: 0.523(R 0.556, F0.489)] [D acc: 0.734(R 0.656, F 0.812)] [G loss: 0.523] [G acc: 0.094]\n",
      "1528 [D loss: 0.550(R 0.510, F0.590)] [D acc: 0.742(R 0.688, F 0.797)] [G loss: 0.550] [G acc: 0.125]\n",
      "1529 [D loss: 0.581(R 0.626, F0.536)] [D acc: 0.695(R 0.625, F 0.766)] [G loss: 0.581] [G acc: 0.016]\n",
      "1530 [D loss: 0.545(R 0.617, F0.472)] [D acc: 0.711(R 0.609, F 0.812)] [G loss: 0.545] [G acc: 0.109]\n",
      "1531 [D loss: 0.544(R 0.518, F0.570)] [D acc: 0.719(R 0.719, F 0.719)] [G loss: 0.544] [G acc: 0.156]\n",
      "1532 [D loss: 0.483(R 0.465, F0.501)] [D acc: 0.773(R 0.766, F 0.781)] [G loss: 0.483] [G acc: 0.156]\n",
      "1533 [D loss: 0.527(R 0.542, F0.512)] [D acc: 0.680(R 0.609, F 0.750)] [G loss: 0.527] [G acc: 0.172]\n",
      "1534 [D loss: 0.533(R 0.468, F0.597)] [D acc: 0.727(R 0.734, F 0.719)] [G loss: 0.533] [G acc: 0.109]\n",
      "1535 [D loss: 0.536(R 0.540, F0.532)] [D acc: 0.703(R 0.625, F 0.781)] [G loss: 0.536] [G acc: 0.016]\n",
      "1536 [D loss: 0.510(R 0.622, F0.397)] [D acc: 0.719(R 0.562, F 0.875)] [G loss: 0.510] [G acc: 0.078]\n",
      "1537 [D loss: 0.569(R 0.509, F0.628)] [D acc: 0.688(R 0.641, F 0.734)] [G loss: 0.569] [G acc: 0.094]\n",
      "1538 [D loss: 0.472(R 0.490, F0.454)] [D acc: 0.766(R 0.750, F 0.781)] [G loss: 0.472] [G acc: 0.109]\n",
      "1539 [D loss: 0.575(R 0.537, F0.613)] [D acc: 0.695(R 0.672, F 0.719)] [G loss: 0.575] [G acc: 0.125]\n",
      "1540 [D loss: 0.516(R 0.416, F0.617)] [D acc: 0.766(R 0.812, F 0.719)] [G loss: 0.516] [G acc: 0.078]\n",
      "1541 [D loss: 0.534(R 0.535, F0.532)] [D acc: 0.711(R 0.672, F 0.750)] [G loss: 0.534] [G acc: 0.094]\n",
      "1542 [D loss: 0.627(R 0.601, F0.654)] [D acc: 0.664(R 0.688, F 0.641)] [G loss: 0.627] [G acc: 0.141]\n",
      "1543 [D loss: 0.656(R 0.726, F0.587)] [D acc: 0.625(R 0.547, F 0.703)] [G loss: 0.656] [G acc: 0.078]\n",
      "1544 [D loss: 0.566(R 0.635, F0.498)] [D acc: 0.719(R 0.656, F 0.781)] [G loss: 0.566] [G acc: 0.078]\n",
      "1545 [D loss: 0.544(R 0.603, F0.485)] [D acc: 0.688(R 0.578, F 0.797)] [G loss: 0.544] [G acc: 0.078]\n",
      "1546 [D loss: 0.552(R 0.539, F0.565)] [D acc: 0.711(R 0.656, F 0.766)] [G loss: 0.552] [G acc: 0.203]\n",
      "1547 [D loss: 0.597(R 0.574, F0.621)] [D acc: 0.703(R 0.594, F 0.812)] [G loss: 0.597] [G acc: 0.109]\n",
      "1548 [D loss: 0.522(R 0.589, F0.455)] [D acc: 0.742(R 0.609, F 0.875)] [G loss: 0.522] [G acc: 0.141]\n",
      "1549 [D loss: 0.501(R 0.516, F0.486)] [D acc: 0.742(R 0.703, F 0.781)] [G loss: 0.501] [G acc: 0.188]\n",
      "1550 [D loss: 0.508(R 0.482, F0.533)] [D acc: 0.766(R 0.703, F 0.828)] [G loss: 0.508] [G acc: 0.125]\n",
      "1551 [D loss: 0.605(R 0.489, F0.722)] [D acc: 0.680(R 0.734, F 0.625)] [G loss: 0.605] [G acc: 0.172]\n",
      "1552 [D loss: 0.515(R 0.612, F0.418)] [D acc: 0.711(R 0.547, F 0.875)] [G loss: 0.515] [G acc: 0.141]\n",
      "1553 [D loss: 0.534(R 0.475, F0.592)] [D acc: 0.727(R 0.750, F 0.703)] [G loss: 0.534] [G acc: 0.109]\n",
      "1554 [D loss: 0.509(R 0.425, F0.592)] [D acc: 0.750(R 0.750, F 0.750)] [G loss: 0.509] [G acc: 0.125]\n",
      "1555 [D loss: 0.573(R 0.629, F0.516)] [D acc: 0.711(R 0.594, F 0.828)] [G loss: 0.573] [G acc: 0.188]\n",
      "1556 [D loss: 0.596(R 0.555, F0.637)] [D acc: 0.703(R 0.656, F 0.750)] [G loss: 0.596] [G acc: 0.125]\n",
      "1557 [D loss: 0.507(R 0.482, F0.533)] [D acc: 0.695(R 0.703, F 0.688)] [G loss: 0.507] [G acc: 0.062]\n",
      "1558 [D loss: 0.595(R 0.604, F0.586)] [D acc: 0.727(R 0.625, F 0.828)] [G loss: 0.595] [G acc: 0.156]\n",
      "1559 [D loss: 0.504(R 0.479, F0.529)] [D acc: 0.758(R 0.734, F 0.781)] [G loss: 0.504] [G acc: 0.125]\n",
      "1560 [D loss: 0.555(R 0.599, F0.510)] [D acc: 0.695(R 0.625, F 0.766)] [G loss: 0.555] [G acc: 0.078]\n",
      "1561 [D loss: 0.632(R 0.653, F0.612)] [D acc: 0.672(R 0.562, F 0.781)] [G loss: 0.632] [G acc: 0.062]\n",
      "1562 [D loss: 0.591(R 0.704, F0.477)] [D acc: 0.695(R 0.547, F 0.844)] [G loss: 0.591] [G acc: 0.094]\n",
      "1563 [D loss: 0.431(R 0.422, F0.439)] [D acc: 0.789(R 0.734, F 0.844)] [G loss: 0.431] [G acc: 0.141]\n",
      "1564 [D loss: 0.596(R 0.551, F0.641)] [D acc: 0.711(R 0.656, F 0.766)] [G loss: 0.596] [G acc: 0.141]\n",
      "1565 [D loss: 0.548(R 0.619, F0.477)] [D acc: 0.711(R 0.609, F 0.812)] [G loss: 0.548] [G acc: 0.141]\n",
      "1566 [D loss: 0.597(R 0.515, F0.678)] [D acc: 0.680(R 0.656, F 0.703)] [G loss: 0.597] [G acc: 0.047]\n",
      "1567 [D loss: 0.555(R 0.616, F0.495)] [D acc: 0.711(R 0.609, F 0.812)] [G loss: 0.555] [G acc: 0.078]\n",
      "1568 [D loss: 0.612(R 0.618, F0.606)] [D acc: 0.617(R 0.531, F 0.703)] [G loss: 0.612] [G acc: 0.047]\n",
      "1569 [D loss: 0.508(R 0.461, F0.555)] [D acc: 0.766(R 0.734, F 0.797)] [G loss: 0.508] [G acc: 0.109]\n",
      "1570 [D loss: 0.666(R 0.631, F0.701)] [D acc: 0.648(R 0.578, F 0.719)] [G loss: 0.666] [G acc: 0.156]\n",
      "1571 [D loss: 0.594(R 0.667, F0.521)] [D acc: 0.680(R 0.609, F 0.750)] [G loss: 0.594] [G acc: 0.094]\n",
      "1572 [D loss: 0.562(R 0.538, F0.586)] [D acc: 0.680(R 0.625, F 0.734)] [G loss: 0.562] [G acc: 0.141]\n",
      "1573 [D loss: 0.493(R 0.519, F0.467)] [D acc: 0.789(R 0.719, F 0.859)] [G loss: 0.493] [G acc: 0.172]\n",
      "1574 [D loss: 0.547(R 0.482, F0.613)] [D acc: 0.766(R 0.781, F 0.750)] [G loss: 0.547] [G acc: 0.062]\n",
      "1575 [D loss: 0.562(R 0.632, F0.492)] [D acc: 0.703(R 0.609, F 0.797)] [G loss: 0.562] [G acc: 0.094]\n",
      "1576 [D loss: 0.557(R 0.550, F0.563)] [D acc: 0.711(R 0.641, F 0.781)] [G loss: 0.557] [G acc: 0.047]\n",
      "1577 [D loss: 0.552(R 0.518, F0.586)] [D acc: 0.711(R 0.719, F 0.703)] [G loss: 0.552] [G acc: 0.094]\n",
      "1578 [D loss: 0.600(R 0.592, F0.608)] [D acc: 0.672(R 0.625, F 0.719)] [G loss: 0.600] [G acc: 0.109]\n",
      "1579 [D loss: 0.552(R 0.646, F0.459)] [D acc: 0.727(R 0.578, F 0.875)] [G loss: 0.552] [G acc: 0.078]\n",
      "1580 [D loss: 0.513(R 0.517, F0.508)] [D acc: 0.773(R 0.672, F 0.875)] [G loss: 0.513] [G acc: 0.141]\n",
      "1581 [D loss: 0.626(R 0.709, F0.544)] [D acc: 0.664(R 0.516, F 0.812)] [G loss: 0.626] [G acc: 0.109]\n",
      "1582 [D loss: 0.655(R 0.542, F0.769)] [D acc: 0.695(R 0.672, F 0.719)] [G loss: 0.655] [G acc: 0.109]\n",
      "1583 [D loss: 0.609(R 0.624, F0.593)] [D acc: 0.602(R 0.547, F 0.656)] [G loss: 0.609] [G acc: 0.156]\n",
      "1584 [D loss: 0.665(R 0.637, F0.693)] [D acc: 0.656(R 0.562, F 0.750)] [G loss: 0.665] [G acc: 0.141]\n",
      "1585 [D loss: 0.570(R 0.625, F0.515)] [D acc: 0.703(R 0.594, F 0.812)] [G loss: 0.570] [G acc: 0.062]\n",
      "1586 [D loss: 0.452(R 0.472, F0.433)] [D acc: 0.844(R 0.766, F 0.922)] [G loss: 0.452] [G acc: 0.188]\n",
      "1587 [D loss: 0.517(R 0.440, F0.594)] [D acc: 0.750(R 0.719, F 0.781)] [G loss: 0.517] [G acc: 0.125]\n",
      "1588 [D loss: 0.529(R 0.618, F0.440)] [D acc: 0.742(R 0.594, F 0.891)] [G loss: 0.529] [G acc: 0.016]\n",
      "1589 [D loss: 0.533(R 0.412, F0.654)] [D acc: 0.742(R 0.766, F 0.719)] [G loss: 0.533] [G acc: 0.094]\n",
      "1590 [D loss: 0.549(R 0.600, F0.498)] [D acc: 0.742(R 0.688, F 0.797)] [G loss: 0.549] [G acc: 0.062]\n",
      "1591 [D loss: 0.498(R 0.507, F0.489)] [D acc: 0.758(R 0.688, F 0.828)] [G loss: 0.498] [G acc: 0.062]\n",
      "1592 [D loss: 0.654(R 0.601, F0.708)] [D acc: 0.656(R 0.641, F 0.672)] [G loss: 0.654] [G acc: 0.094]\n",
      "1593 [D loss: 0.531(R 0.597, F0.466)] [D acc: 0.734(R 0.656, F 0.812)] [G loss: 0.531] [G acc: 0.109]\n",
      "1594 [D loss: 0.513(R 0.503, F0.523)] [D acc: 0.758(R 0.672, F 0.844)] [G loss: 0.513] [G acc: 0.078]\n",
      "1595 [D loss: 0.508(R 0.525, F0.490)] [D acc: 0.734(R 0.672, F 0.797)] [G loss: 0.508] [G acc: 0.078]\n",
      "1596 [D loss: 0.585(R 0.639, F0.531)] [D acc: 0.688(R 0.578, F 0.797)] [G loss: 0.585] [G acc: 0.172]\n",
      "1597 [D loss: 0.531(R 0.537, F0.525)] [D acc: 0.758(R 0.688, F 0.828)] [G loss: 0.531] [G acc: 0.062]\n",
      "1598 [D loss: 0.544(R 0.574, F0.514)] [D acc: 0.719(R 0.641, F 0.797)] [G loss: 0.544] [G acc: 0.188]\n",
      "1599 [D loss: 0.520(R 0.453, F0.586)] [D acc: 0.758(R 0.734, F 0.781)] [G loss: 0.520] [G acc: 0.125]\n",
      "INFO:tensorflow:Assets written to: ram://7716b79d-d7a5-47bd-ba88-a5b8bdd83297/assets\n",
      "INFO:tensorflow:Assets written to: ram://cffdb808-1fe2-4ce2-8bb3-def7098c0f7c/assets\n",
      "INFO:tensorflow:Assets written to: ram://696b3216-f8f4-4d72-a3a9-0461a9b49d49/assets\n",
      "1600 [D loss: 0.549(R 0.566, F0.531)] [D acc: 0.727(R 0.656, F 0.797)] [G loss: 0.549] [G acc: 0.094]\n",
      "1601 [D loss: 0.526(R 0.555, F0.497)] [D acc: 0.711(R 0.641, F 0.781)] [G loss: 0.526] [G acc: 0.125]\n",
      "1602 [D loss: 0.539(R 0.563, F0.514)] [D acc: 0.750(R 0.672, F 0.828)] [G loss: 0.539] [G acc: 0.109]\n",
      "1603 [D loss: 0.564(R 0.580, F0.547)] [D acc: 0.703(R 0.609, F 0.797)] [G loss: 0.564] [G acc: 0.078]\n",
      "1604 [D loss: 0.575(R 0.633, F0.518)] [D acc: 0.719(R 0.625, F 0.812)] [G loss: 0.575] [G acc: 0.125]\n",
      "1605 [D loss: 0.493(R 0.542, F0.444)] [D acc: 0.773(R 0.719, F 0.828)] [G loss: 0.493] [G acc: 0.109]\n",
      "1606 [D loss: 0.502(R 0.471, F0.533)] [D acc: 0.758(R 0.750, F 0.766)] [G loss: 0.502] [G acc: 0.156]\n",
      "1607 [D loss: 0.600(R 0.530, F0.670)] [D acc: 0.727(R 0.688, F 0.766)] [G loss: 0.600] [G acc: 0.109]\n",
      "1608 [D loss: 0.523(R 0.524, F0.522)] [D acc: 0.711(R 0.641, F 0.781)] [G loss: 0.523] [G acc: 0.156]\n",
      "1609 [D loss: 0.528(R 0.595, F0.461)] [D acc: 0.758(R 0.641, F 0.875)] [G loss: 0.528] [G acc: 0.047]\n",
      "1610 [D loss: 0.534(R 0.530, F0.539)] [D acc: 0.711(R 0.641, F 0.781)] [G loss: 0.534] [G acc: 0.109]\n",
      "1611 [D loss: 0.506(R 0.538, F0.474)] [D acc: 0.750(R 0.688, F 0.812)] [G loss: 0.506] [G acc: 0.188]\n",
      "1612 [D loss: 0.532(R 0.497, F0.568)] [D acc: 0.711(R 0.656, F 0.766)] [G loss: 0.532] [G acc: 0.141]\n",
      "1613 [D loss: 0.555(R 0.577, F0.533)] [D acc: 0.711(R 0.641, F 0.781)] [G loss: 0.555] [G acc: 0.047]\n",
      "1614 [D loss: 0.577(R 0.476, F0.679)] [D acc: 0.719(R 0.734, F 0.703)] [G loss: 0.577] [G acc: 0.125]\n",
      "1615 [D loss: 0.662(R 0.679, F0.645)] [D acc: 0.617(R 0.547, F 0.688)] [G loss: 0.662] [G acc: 0.016]\n",
      "1616 [D loss: 0.613(R 0.630, F0.596)] [D acc: 0.703(R 0.625, F 0.781)] [G loss: 0.613] [G acc: 0.047]\n",
      "1617 [D loss: 0.544(R 0.543, F0.545)] [D acc: 0.711(R 0.672, F 0.750)] [G loss: 0.544] [G acc: 0.125]\n",
      "1618 [D loss: 0.573(R 0.678, F0.468)] [D acc: 0.695(R 0.547, F 0.844)] [G loss: 0.573] [G acc: 0.141]\n",
      "1619 [D loss: 0.570(R 0.561, F0.578)] [D acc: 0.727(R 0.625, F 0.828)] [G loss: 0.570] [G acc: 0.125]\n",
      "1620 [D loss: 0.556(R 0.541, F0.571)] [D acc: 0.703(R 0.625, F 0.781)] [G loss: 0.556] [G acc: 0.078]\n",
      "1621 [D loss: 0.629(R 0.623, F0.635)] [D acc: 0.695(R 0.594, F 0.797)] [G loss: 0.629] [G acc: 0.078]\n",
      "1622 [D loss: 0.567(R 0.560, F0.574)] [D acc: 0.664(R 0.594, F 0.734)] [G loss: 0.567] [G acc: 0.094]\n",
      "1623 [D loss: 0.621(R 0.534, F0.709)] [D acc: 0.727(R 0.719, F 0.734)] [G loss: 0.621] [G acc: 0.078]\n",
      "1624 [D loss: 0.475(R 0.457, F0.492)] [D acc: 0.805(R 0.781, F 0.828)] [G loss: 0.475] [G acc: 0.109]\n",
      "1625 [D loss: 0.520(R 0.572, F0.469)] [D acc: 0.711(R 0.594, F 0.828)] [G loss: 0.520] [G acc: 0.094]\n",
      "1626 [D loss: 0.558(R 0.587, F0.528)] [D acc: 0.719(R 0.672, F 0.766)] [G loss: 0.558] [G acc: 0.078]\n",
      "1627 [D loss: 0.550(R 0.600, F0.501)] [D acc: 0.688(R 0.609, F 0.766)] [G loss: 0.550] [G acc: 0.125]\n",
      "1628 [D loss: 0.508(R 0.487, F0.528)] [D acc: 0.750(R 0.750, F 0.750)] [G loss: 0.508] [G acc: 0.031]\n",
      "1629 [D loss: 0.542(R 0.620, F0.464)] [D acc: 0.742(R 0.609, F 0.875)] [G loss: 0.542] [G acc: 0.109]\n",
      "1630 [D loss: 0.577(R 0.553, F0.602)] [D acc: 0.695(R 0.703, F 0.688)] [G loss: 0.577] [G acc: 0.156]\n",
      "1631 [D loss: 0.509(R 0.496, F0.523)] [D acc: 0.734(R 0.703, F 0.766)] [G loss: 0.509] [G acc: 0.141]\n",
      "1632 [D loss: 0.526(R 0.502, F0.550)] [D acc: 0.734(R 0.719, F 0.750)] [G loss: 0.526] [G acc: 0.141]\n",
      "1633 [D loss: 0.583(R 0.551, F0.615)] [D acc: 0.680(R 0.641, F 0.719)] [G loss: 0.583] [G acc: 0.094]\n",
      "1634 [D loss: 0.570(R 0.624, F0.515)] [D acc: 0.680(R 0.578, F 0.781)] [G loss: 0.570] [G acc: 0.156]\n",
      "1635 [D loss: 0.566(R 0.588, F0.544)] [D acc: 0.688(R 0.609, F 0.766)] [G loss: 0.566] [G acc: 0.078]\n",
      "1636 [D loss: 0.481(R 0.536, F0.427)] [D acc: 0.789(R 0.703, F 0.875)] [G loss: 0.481] [G acc: 0.109]\n",
      "1637 [D loss: 0.504(R 0.521, F0.486)] [D acc: 0.750(R 0.688, F 0.812)] [G loss: 0.504] [G acc: 0.141]\n",
      "1638 [D loss: 0.553(R 0.414, F0.691)] [D acc: 0.750(R 0.766, F 0.734)] [G loss: 0.553] [G acc: 0.219]\n",
      "1639 [D loss: 0.575(R 0.706, F0.445)] [D acc: 0.664(R 0.516, F 0.812)] [G loss: 0.575] [G acc: 0.062]\n",
      "1640 [D loss: 0.566(R 0.586, F0.545)] [D acc: 0.641(R 0.641, F 0.641)] [G loss: 0.566] [G acc: 0.125]\n",
      "1641 [D loss: 0.517(R 0.454, F0.579)] [D acc: 0.734(R 0.734, F 0.734)] [G loss: 0.517] [G acc: 0.141]\n",
      "1642 [D loss: 0.563(R 0.517, F0.609)] [D acc: 0.719(R 0.688, F 0.750)] [G loss: 0.563] [G acc: 0.109]\n",
      "1643 [D loss: 0.571(R 0.558, F0.585)] [D acc: 0.727(R 0.703, F 0.750)] [G loss: 0.571] [G acc: 0.094]\n",
      "1644 [D loss: 0.557(R 0.576, F0.539)] [D acc: 0.727(R 0.719, F 0.734)] [G loss: 0.557] [G acc: 0.094]\n",
      "1645 [D loss: 0.586(R 0.579, F0.594)] [D acc: 0.672(R 0.672, F 0.672)] [G loss: 0.586] [G acc: 0.156]\n",
      "1646 [D loss: 0.499(R 0.510, F0.488)] [D acc: 0.758(R 0.703, F 0.812)] [G loss: 0.499] [G acc: 0.172]\n",
      "1647 [D loss: 0.508(R 0.555, F0.460)] [D acc: 0.727(R 0.625, F 0.828)] [G loss: 0.508] [G acc: 0.109]\n",
      "1648 [D loss: 0.553(R 0.600, F0.506)] [D acc: 0.758(R 0.625, F 0.891)] [G loss: 0.553] [G acc: 0.047]\n",
      "1649 [D loss: 0.538(R 0.492, F0.584)] [D acc: 0.719(R 0.672, F 0.766)] [G loss: 0.538] [G acc: 0.109]\n",
      "1650 [D loss: 0.560(R 0.661, F0.458)] [D acc: 0.742(R 0.625, F 0.859)] [G loss: 0.560] [G acc: 0.125]\n",
      "1651 [D loss: 0.468(R 0.424, F0.511)] [D acc: 0.750(R 0.734, F 0.766)] [G loss: 0.468] [G acc: 0.109]\n",
      "1652 [D loss: 0.489(R 0.452, F0.526)] [D acc: 0.766(R 0.766, F 0.766)] [G loss: 0.489] [G acc: 0.062]\n",
      "1653 [D loss: 0.611(R 0.642, F0.579)] [D acc: 0.695(R 0.641, F 0.750)] [G loss: 0.611] [G acc: 0.094]\n",
      "1654 [D loss: 0.652(R 0.768, F0.535)] [D acc: 0.680(R 0.516, F 0.844)] [G loss: 0.652] [G acc: 0.078]\n",
      "1655 [D loss: 0.521(R 0.493, F0.550)] [D acc: 0.758(R 0.719, F 0.797)] [G loss: 0.521] [G acc: 0.125]\n",
      "1656 [D loss: 0.507(R 0.465, F0.549)] [D acc: 0.773(R 0.734, F 0.812)] [G loss: 0.507] [G acc: 0.125]\n",
      "1657 [D loss: 0.526(R 0.552, F0.500)] [D acc: 0.734(R 0.625, F 0.844)] [G loss: 0.526] [G acc: 0.125]\n",
      "1658 [D loss: 0.564(R 0.613, F0.515)] [D acc: 0.688(R 0.562, F 0.812)] [G loss: 0.564] [G acc: 0.094]\n",
      "1659 [D loss: 0.553(R 0.486, F0.620)] [D acc: 0.750(R 0.719, F 0.781)] [G loss: 0.553] [G acc: 0.172]\n",
      "1660 [D loss: 0.466(R 0.471, F0.460)] [D acc: 0.742(R 0.703, F 0.781)] [G loss: 0.466] [G acc: 0.016]\n",
      "1661 [D loss: 0.518(R 0.505, F0.531)] [D acc: 0.773(R 0.703, F 0.844)] [G loss: 0.518] [G acc: 0.125]\n",
      "1662 [D loss: 0.427(R 0.378, F0.475)] [D acc: 0.781(R 0.781, F 0.781)] [G loss: 0.427] [G acc: 0.109]\n",
      "1663 [D loss: 0.491(R 0.537, F0.444)] [D acc: 0.766(R 0.703, F 0.828)] [G loss: 0.491] [G acc: 0.125]\n",
      "1664 [D loss: 0.552(R 0.567, F0.537)] [D acc: 0.703(R 0.656, F 0.750)] [G loss: 0.552] [G acc: 0.141]\n",
      "1665 [D loss: 0.529(R 0.549, F0.508)] [D acc: 0.766(R 0.688, F 0.844)] [G loss: 0.529] [G acc: 0.094]\n",
      "1666 [D loss: 0.556(R 0.595, F0.517)] [D acc: 0.727(R 0.656, F 0.797)] [G loss: 0.556] [G acc: 0.094]\n",
      "1667 [D loss: 0.577(R 0.485, F0.668)] [D acc: 0.664(R 0.656, F 0.672)] [G loss: 0.577] [G acc: 0.062]\n",
      "1668 [D loss: 0.463(R 0.480, F0.446)] [D acc: 0.828(R 0.797, F 0.859)] [G loss: 0.463] [G acc: 0.047]\n",
      "1669 [D loss: 0.589(R 0.544, F0.634)] [D acc: 0.719(R 0.641, F 0.797)] [G loss: 0.589] [G acc: 0.078]\n",
      "1670 [D loss: 0.584(R 0.654, F0.514)] [D acc: 0.734(R 0.625, F 0.844)] [G loss: 0.584] [G acc: 0.172]\n",
      "1671 [D loss: 0.545(R 0.523, F0.568)] [D acc: 0.742(R 0.703, F 0.781)] [G loss: 0.545] [G acc: 0.062]\n",
      "1672 [D loss: 0.516(R 0.506, F0.527)] [D acc: 0.750(R 0.719, F 0.781)] [G loss: 0.516] [G acc: 0.047]\n",
      "1673 [D loss: 0.526(R 0.613, F0.439)] [D acc: 0.727(R 0.594, F 0.859)] [G loss: 0.526] [G acc: 0.078]\n",
      "1674 [D loss: 0.475(R 0.493, F0.458)] [D acc: 0.781(R 0.750, F 0.812)] [G loss: 0.475] [G acc: 0.109]\n",
      "1675 [D loss: 0.494(R 0.385, F0.604)] [D acc: 0.758(R 0.781, F 0.734)] [G loss: 0.494] [G acc: 0.047]\n",
      "1676 [D loss: 0.525(R 0.603, F0.446)] [D acc: 0.727(R 0.562, F 0.891)] [G loss: 0.525] [G acc: 0.062]\n",
      "1677 [D loss: 0.544(R 0.522, F0.565)] [D acc: 0.734(R 0.672, F 0.797)] [G loss: 0.544] [G acc: 0.109]\n",
      "1678 [D loss: 0.503(R 0.424, F0.581)] [D acc: 0.766(R 0.781, F 0.750)] [G loss: 0.503] [G acc: 0.109]\n",
      "1679 [D loss: 0.482(R 0.534, F0.429)] [D acc: 0.773(R 0.656, F 0.891)] [G loss: 0.482] [G acc: 0.125]\n",
      "1680 [D loss: 0.502(R 0.494, F0.510)] [D acc: 0.711(R 0.672, F 0.750)] [G loss: 0.502] [G acc: 0.094]\n",
      "1681 [D loss: 0.651(R 0.604, F0.697)] [D acc: 0.703(R 0.609, F 0.797)] [G loss: 0.651] [G acc: 0.078]\n",
      "1682 [D loss: 0.572(R 0.535, F0.608)] [D acc: 0.703(R 0.656, F 0.750)] [G loss: 0.572] [G acc: 0.062]\n",
      "1683 [D loss: 0.468(R 0.520, F0.415)] [D acc: 0.766(R 0.672, F 0.859)] [G loss: 0.468] [G acc: 0.062]\n",
      "1684 [D loss: 0.542(R 0.614, F0.470)] [D acc: 0.758(R 0.672, F 0.844)] [G loss: 0.542] [G acc: 0.094]\n",
      "1685 [D loss: 0.493(R 0.540, F0.446)] [D acc: 0.773(R 0.688, F 0.859)] [G loss: 0.493] [G acc: 0.031]\n",
      "1686 [D loss: 0.577(R 0.653, F0.501)] [D acc: 0.734(R 0.594, F 0.875)] [G loss: 0.577] [G acc: 0.125]\n",
      "1687 [D loss: 0.548(R 0.612, F0.484)] [D acc: 0.750(R 0.625, F 0.875)] [G loss: 0.548] [G acc: 0.141]\n",
      "1688 [D loss: 0.606(R 0.483, F0.729)] [D acc: 0.688(R 0.688, F 0.688)] [G loss: 0.606] [G acc: 0.047]\n",
      "1689 [D loss: 0.537(R 0.655, F0.418)] [D acc: 0.742(R 0.609, F 0.875)] [G loss: 0.537] [G acc: 0.141]\n",
      "1690 [D loss: 0.535(R 0.578, F0.493)] [D acc: 0.727(R 0.656, F 0.797)] [G loss: 0.535] [G acc: 0.062]\n",
      "1691 [D loss: 0.446(R 0.416, F0.476)] [D acc: 0.742(R 0.703, F 0.781)] [G loss: 0.446] [G acc: 0.062]\n",
      "1692 [D loss: 0.523(R 0.596, F0.451)] [D acc: 0.695(R 0.609, F 0.781)] [G loss: 0.523] [G acc: 0.047]\n",
      "1693 [D loss: 0.511(R 0.526, F0.496)] [D acc: 0.727(R 0.672, F 0.781)] [G loss: 0.511] [G acc: 0.125]\n",
      "1694 [D loss: 0.576(R 0.585, F0.566)] [D acc: 0.625(R 0.562, F 0.688)] [G loss: 0.576] [G acc: 0.109]\n",
      "1695 [D loss: 0.513(R 0.569, F0.456)] [D acc: 0.727(R 0.625, F 0.828)] [G loss: 0.513] [G acc: 0.125]\n",
      "1696 [D loss: 0.610(R 0.571, F0.650)] [D acc: 0.680(R 0.641, F 0.719)] [G loss: 0.610] [G acc: 0.094]\n",
      "1697 [D loss: 0.452(R 0.469, F0.436)] [D acc: 0.797(R 0.750, F 0.844)] [G loss: 0.452] [G acc: 0.078]\n",
      "1698 [D loss: 0.588(R 0.645, F0.531)] [D acc: 0.664(R 0.531, F 0.797)] [G loss: 0.588] [G acc: 0.094]\n",
      "1699 [D loss: 0.510(R 0.505, F0.514)] [D acc: 0.703(R 0.656, F 0.750)] [G loss: 0.510] [G acc: 0.125]\n",
      "INFO:tensorflow:Assets written to: ram://5766fbb4-312d-40cd-a4d1-2ae5ef05b58d/assets\n",
      "INFO:tensorflow:Assets written to: ram://d6128786-9d4a-4f16-a7d1-edc3d49f7293/assets\n",
      "INFO:tensorflow:Assets written to: ram://3045b176-1aa7-4041-b4cd-33feaa5aed18/assets\n",
      "1700 [D loss: 0.502(R 0.496, F0.507)] [D acc: 0.789(R 0.734, F 0.844)] [G loss: 0.502] [G acc: 0.094]\n",
      "1701 [D loss: 0.558(R 0.573, F0.542)] [D acc: 0.727(R 0.641, F 0.812)] [G loss: 0.558] [G acc: 0.047]\n",
      "1702 [D loss: 0.501(R 0.485, F0.516)] [D acc: 0.750(R 0.703, F 0.797)] [G loss: 0.501] [G acc: 0.094]\n",
      "1703 [D loss: 0.535(R 0.518, F0.552)] [D acc: 0.758(R 0.688, F 0.828)] [G loss: 0.535] [G acc: 0.125]\n",
      "1704 [D loss: 0.525(R 0.529, F0.520)] [D acc: 0.742(R 0.641, F 0.844)] [G loss: 0.525] [G acc: 0.141]\n",
      "1705 [D loss: 0.498(R 0.428, F0.567)] [D acc: 0.758(R 0.750, F 0.766)] [G loss: 0.498] [G acc: 0.156]\n",
      "1706 [D loss: 0.561(R 0.442, F0.679)] [D acc: 0.766(R 0.781, F 0.750)] [G loss: 0.561] [G acc: 0.062]\n",
      "1707 [D loss: 0.608(R 0.676, F0.540)] [D acc: 0.641(R 0.547, F 0.734)] [G loss: 0.608] [G acc: 0.125]\n",
      "1708 [D loss: 0.509(R 0.612, F0.407)] [D acc: 0.797(R 0.672, F 0.922)] [G loss: 0.509] [G acc: 0.281]\n",
      "1709 [D loss: 0.415(R 0.413, F0.416)] [D acc: 0.812(R 0.766, F 0.859)] [G loss: 0.415] [G acc: 0.172]\n",
      "1710 [D loss: 0.606(R 0.439, F0.773)] [D acc: 0.750(R 0.797, F 0.703)] [G loss: 0.606] [G acc: 0.078]\n",
      "1711 [D loss: 0.582(R 0.554, F0.610)] [D acc: 0.703(R 0.719, F 0.688)] [G loss: 0.582] [G acc: 0.047]\n",
      "1712 [D loss: 0.518(R 0.514, F0.522)] [D acc: 0.758(R 0.703, F 0.812)] [G loss: 0.518] [G acc: 0.125]\n",
      "1713 [D loss: 0.576(R 0.685, F0.467)] [D acc: 0.750(R 0.594, F 0.906)] [G loss: 0.576] [G acc: 0.047]\n",
      "1714 [D loss: 0.476(R 0.555, F0.398)] [D acc: 0.719(R 0.594, F 0.844)] [G loss: 0.476] [G acc: 0.156]\n",
      "1715 [D loss: 0.578(R 0.438, F0.718)] [D acc: 0.758(R 0.781, F 0.734)] [G loss: 0.578] [G acc: 0.172]\n",
      "1716 [D loss: 0.547(R 0.475, F0.619)] [D acc: 0.727(R 0.672, F 0.781)] [G loss: 0.547] [G acc: 0.109]\n",
      "1717 [D loss: 0.578(R 0.778, F0.378)] [D acc: 0.672(R 0.438, F 0.906)] [G loss: 0.578] [G acc: 0.172]\n",
      "1718 [D loss: 0.479(R 0.471, F0.487)] [D acc: 0.734(R 0.703, F 0.766)] [G loss: 0.479] [G acc: 0.141]\n",
      "1719 [D loss: 0.519(R 0.516, F0.522)] [D acc: 0.734(R 0.703, F 0.766)] [G loss: 0.519] [G acc: 0.078]\n",
      "1720 [D loss: 0.559(R 0.566, F0.551)] [D acc: 0.750(R 0.688, F 0.812)] [G loss: 0.559] [G acc: 0.125]\n",
      "1721 [D loss: 0.612(R 0.442, F0.781)] [D acc: 0.711(R 0.734, F 0.688)] [G loss: 0.612] [G acc: 0.094]\n",
      "1722 [D loss: 0.528(R 0.590, F0.465)] [D acc: 0.719(R 0.609, F 0.828)] [G loss: 0.528] [G acc: 0.047]\n",
      "1723 [D loss: 0.536(R 0.597, F0.475)] [D acc: 0.758(R 0.672, F 0.844)] [G loss: 0.536] [G acc: 0.109]\n",
      "1724 [D loss: 0.568(R 0.597, F0.539)] [D acc: 0.703(R 0.594, F 0.812)] [G loss: 0.568] [G acc: 0.094]\n",
      "1725 [D loss: 0.603(R 0.551, F0.655)] [D acc: 0.695(R 0.625, F 0.766)] [G loss: 0.603] [G acc: 0.078]\n",
      "1726 [D loss: 0.508(R 0.595, F0.421)] [D acc: 0.711(R 0.578, F 0.844)] [G loss: 0.508] [G acc: 0.109]\n",
      "1727 [D loss: 0.478(R 0.532, F0.424)] [D acc: 0.781(R 0.688, F 0.875)] [G loss: 0.478] [G acc: 0.062]\n",
      "1728 [D loss: 0.531(R 0.498, F0.564)] [D acc: 0.781(R 0.734, F 0.828)] [G loss: 0.531] [G acc: 0.109]\n",
      "1729 [D loss: 0.475(R 0.511, F0.438)] [D acc: 0.758(R 0.703, F 0.812)] [G loss: 0.475] [G acc: 0.094]\n",
      "1730 [D loss: 0.585(R 0.519, F0.652)] [D acc: 0.719(R 0.703, F 0.734)] [G loss: 0.585] [G acc: 0.141]\n",
      "1731 [D loss: 0.569(R 0.681, F0.457)] [D acc: 0.672(R 0.578, F 0.766)] [G loss: 0.569] [G acc: 0.172]\n",
      "1732 [D loss: 0.551(R 0.521, F0.582)] [D acc: 0.703(R 0.672, F 0.734)] [G loss: 0.551] [G acc: 0.094]\n",
      "1733 [D loss: 0.563(R 0.649, F0.478)] [D acc: 0.688(R 0.578, F 0.797)] [G loss: 0.563] [G acc: 0.062]\n",
      "1734 [D loss: 0.559(R 0.619, F0.500)] [D acc: 0.711(R 0.641, F 0.781)] [G loss: 0.559] [G acc: 0.078]\n",
      "1735 [D loss: 0.439(R 0.407, F0.471)] [D acc: 0.805(R 0.781, F 0.828)] [G loss: 0.439] [G acc: 0.047]\n",
      "1736 [D loss: 0.577(R 0.452, F0.702)] [D acc: 0.742(R 0.750, F 0.734)] [G loss: 0.577] [G acc: 0.156]\n",
      "1737 [D loss: 0.484(R 0.497, F0.472)] [D acc: 0.805(R 0.734, F 0.875)] [G loss: 0.484] [G acc: 0.125]\n",
      "1738 [D loss: 0.590(R 0.635, F0.545)] [D acc: 0.688(R 0.594, F 0.781)] [G loss: 0.590] [G acc: 0.141]\n",
      "1739 [D loss: 0.572(R 0.523, F0.622)] [D acc: 0.680(R 0.688, F 0.672)] [G loss: 0.572] [G acc: 0.109]\n",
      "1740 [D loss: 0.490(R 0.448, F0.532)] [D acc: 0.766(R 0.703, F 0.828)] [G loss: 0.490] [G acc: 0.047]\n",
      "1741 [D loss: 0.612(R 0.702, F0.522)] [D acc: 0.648(R 0.516, F 0.781)] [G loss: 0.612] [G acc: 0.156]\n",
      "1742 [D loss: 0.571(R 0.494, F0.648)] [D acc: 0.719(R 0.719, F 0.719)] [G loss: 0.571] [G acc: 0.141]\n",
      "1743 [D loss: 0.643(R 0.673, F0.614)] [D acc: 0.664(R 0.609, F 0.719)] [G loss: 0.643] [G acc: 0.078]\n",
      "1744 [D loss: 0.548(R 0.584, F0.511)] [D acc: 0.750(R 0.672, F 0.828)] [G loss: 0.548] [G acc: 0.047]\n",
      "1745 [D loss: 0.545(R 0.647, F0.443)] [D acc: 0.727(R 0.594, F 0.859)] [G loss: 0.545] [G acc: 0.219]\n",
      "1746 [D loss: 0.574(R 0.465, F0.682)] [D acc: 0.695(R 0.750, F 0.641)] [G loss: 0.574] [G acc: 0.125]\n",
      "1747 [D loss: 0.494(R 0.552, F0.436)] [D acc: 0.727(R 0.656, F 0.797)] [G loss: 0.494] [G acc: 0.031]\n",
      "1748 [D loss: 0.565(R 0.597, F0.534)] [D acc: 0.719(R 0.656, F 0.781)] [G loss: 0.565] [G acc: 0.125]\n",
      "1749 [D loss: 0.556(R 0.565, F0.547)] [D acc: 0.688(R 0.625, F 0.750)] [G loss: 0.556] [G acc: 0.156]\n",
      "1750 [D loss: 0.481(R 0.528, F0.434)] [D acc: 0.727(R 0.625, F 0.828)] [G loss: 0.481] [G acc: 0.062]\n",
      "1751 [D loss: 0.744(R 0.794, F0.694)] [D acc: 0.508(R 0.422, F 0.594)] [G loss: 0.744] [G acc: 0.094]\n",
      "1752 [D loss: 0.593(R 0.604, F0.581)] [D acc: 0.641(R 0.578, F 0.703)] [G loss: 0.593] [G acc: 0.094]\n",
      "1753 [D loss: 0.584(R 0.570, F0.598)] [D acc: 0.688(R 0.672, F 0.703)] [G loss: 0.584] [G acc: 0.094]\n",
      "1754 [D loss: 0.568(R 0.648, F0.488)] [D acc: 0.672(R 0.578, F 0.766)] [G loss: 0.568] [G acc: 0.078]\n",
      "1755 [D loss: 0.483(R 0.451, F0.515)] [D acc: 0.781(R 0.750, F 0.812)] [G loss: 0.483] [G acc: 0.094]\n",
      "1756 [D loss: 0.553(R 0.536, F0.570)] [D acc: 0.680(R 0.641, F 0.719)] [G loss: 0.553] [G acc: 0.125]\n",
      "1757 [D loss: 0.575(R 0.625, F0.525)] [D acc: 0.695(R 0.578, F 0.812)] [G loss: 0.575] [G acc: 0.078]\n",
      "1758 [D loss: 0.613(R 0.671, F0.556)] [D acc: 0.672(R 0.594, F 0.750)] [G loss: 0.613] [G acc: 0.109]\n",
      "1759 [D loss: 0.490(R 0.517, F0.463)] [D acc: 0.758(R 0.672, F 0.844)] [G loss: 0.490] [G acc: 0.078]\n",
      "1760 [D loss: 0.521(R 0.576, F0.465)] [D acc: 0.750(R 0.672, F 0.828)] [G loss: 0.521] [G acc: 0.078]\n",
      "1761 [D loss: 0.527(R 0.570, F0.484)] [D acc: 0.734(R 0.625, F 0.844)] [G loss: 0.527] [G acc: 0.047]\n",
      "1762 [D loss: 0.589(R 0.536, F0.643)] [D acc: 0.648(R 0.625, F 0.672)] [G loss: 0.589] [G acc: 0.125]\n",
      "1763 [D loss: 0.491(R 0.534, F0.448)] [D acc: 0.758(R 0.656, F 0.859)] [G loss: 0.491] [G acc: 0.141]\n",
      "1764 [D loss: 0.525(R 0.535, F0.514)] [D acc: 0.766(R 0.656, F 0.875)] [G loss: 0.525] [G acc: 0.109]\n",
      "1765 [D loss: 0.546(R 0.626, F0.465)] [D acc: 0.781(R 0.688, F 0.875)] [G loss: 0.546] [G acc: 0.109]\n",
      "1766 [D loss: 0.535(R 0.519, F0.550)] [D acc: 0.742(R 0.719, F 0.766)] [G loss: 0.535] [G acc: 0.125]\n",
      "1767 [D loss: 0.476(R 0.453, F0.500)] [D acc: 0.766(R 0.703, F 0.828)] [G loss: 0.476] [G acc: 0.125]\n",
      "1768 [D loss: 0.567(R 0.529, F0.604)] [D acc: 0.719(R 0.719, F 0.719)] [G loss: 0.567] [G acc: 0.203]\n",
      "1769 [D loss: 0.619(R 0.493, F0.745)] [D acc: 0.664(R 0.703, F 0.625)] [G loss: 0.619] [G acc: 0.109]\n",
      "1770 [D loss: 0.572(R 0.665, F0.478)] [D acc: 0.695(R 0.562, F 0.828)] [G loss: 0.572] [G acc: 0.109]\n",
      "1771 [D loss: 0.543(R 0.542, F0.543)] [D acc: 0.695(R 0.625, F 0.766)] [G loss: 0.543] [G acc: 0.062]\n",
      "1772 [D loss: 0.727(R 0.563, F0.891)] [D acc: 0.641(R 0.656, F 0.625)] [G loss: 0.727] [G acc: 0.047]\n",
      "1773 [D loss: 0.551(R 0.602, F0.499)] [D acc: 0.711(R 0.625, F 0.797)] [G loss: 0.551] [G acc: 0.125]\n",
      "1774 [D loss: 0.543(R 0.553, F0.532)] [D acc: 0.695(R 0.656, F 0.734)] [G loss: 0.543] [G acc: 0.125]\n",
      "1775 [D loss: 0.578(R 0.612, F0.545)] [D acc: 0.695(R 0.578, F 0.812)] [G loss: 0.578] [G acc: 0.188]\n",
      "1776 [D loss: 0.526(R 0.587, F0.464)] [D acc: 0.750(R 0.688, F 0.812)] [G loss: 0.526] [G acc: 0.094]\n",
      "1777 [D loss: 0.480(R 0.511, F0.449)] [D acc: 0.766(R 0.688, F 0.844)] [G loss: 0.480] [G acc: 0.156]\n",
      "1778 [D loss: 0.476(R 0.497, F0.455)] [D acc: 0.766(R 0.719, F 0.812)] [G loss: 0.476] [G acc: 0.094]\n",
      "1779 [D loss: 0.463(R 0.414, F0.512)] [D acc: 0.727(R 0.672, F 0.781)] [G loss: 0.463] [G acc: 0.062]\n",
      "1780 [D loss: 0.461(R 0.495, F0.427)] [D acc: 0.781(R 0.672, F 0.891)] [G loss: 0.461] [G acc: 0.172]\n",
      "1781 [D loss: 0.540(R 0.471, F0.608)] [D acc: 0.695(R 0.672, F 0.719)] [G loss: 0.540] [G acc: 0.062]\n",
      "1782 [D loss: 0.569(R 0.654, F0.484)] [D acc: 0.703(R 0.594, F 0.812)] [G loss: 0.569] [G acc: 0.031]\n",
      "1783 [D loss: 0.515(R 0.496, F0.534)] [D acc: 0.688(R 0.656, F 0.719)] [G loss: 0.515] [G acc: 0.062]\n",
      "1784 [D loss: 0.442(R 0.462, F0.422)] [D acc: 0.789(R 0.734, F 0.844)] [G loss: 0.442] [G acc: 0.047]\n",
      "1785 [D loss: 0.536(R 0.558, F0.513)] [D acc: 0.727(R 0.641, F 0.812)] [G loss: 0.536] [G acc: 0.047]\n",
      "1786 [D loss: 0.560(R 0.626, F0.494)] [D acc: 0.695(R 0.625, F 0.766)] [G loss: 0.560] [G acc: 0.109]\n",
      "1787 [D loss: 0.480(R 0.416, F0.544)] [D acc: 0.805(R 0.797, F 0.812)] [G loss: 0.480] [G acc: 0.125]\n",
      "1788 [D loss: 0.617(R 0.675, F0.559)] [D acc: 0.664(R 0.578, F 0.750)] [G loss: 0.617] [G acc: 0.203]\n",
      "1789 [D loss: 0.565(R 0.617, F0.514)] [D acc: 0.742(R 0.672, F 0.812)] [G loss: 0.565] [G acc: 0.094]\n",
      "1790 [D loss: 0.504(R 0.547, F0.462)] [D acc: 0.758(R 0.641, F 0.875)] [G loss: 0.504] [G acc: 0.078]\n",
      "1791 [D loss: 0.416(R 0.347, F0.484)] [D acc: 0.805(R 0.812, F 0.797)] [G loss: 0.416] [G acc: 0.047]\n",
      "1792 [D loss: 0.483(R 0.434, F0.532)] [D acc: 0.773(R 0.734, F 0.812)] [G loss: 0.483] [G acc: 0.062]\n",
      "1793 [D loss: 0.553(R 0.501, F0.606)] [D acc: 0.766(R 0.719, F 0.812)] [G loss: 0.553] [G acc: 0.047]\n",
      "1794 [D loss: 0.595(R 0.725, F0.465)] [D acc: 0.719(R 0.562, F 0.875)] [G loss: 0.595] [G acc: 0.094]\n",
      "1795 [D loss: 0.478(R 0.415, F0.540)] [D acc: 0.750(R 0.766, F 0.734)] [G loss: 0.478] [G acc: 0.078]\n",
      "1796 [D loss: 0.560(R 0.620, F0.499)] [D acc: 0.742(R 0.641, F 0.844)] [G loss: 0.560] [G acc: 0.062]\n",
      "1797 [D loss: 0.462(R 0.470, F0.455)] [D acc: 0.781(R 0.734, F 0.828)] [G loss: 0.462] [G acc: 0.094]\n",
      "1798 [D loss: 0.597(R 0.657, F0.538)] [D acc: 0.688(R 0.547, F 0.828)] [G loss: 0.597] [G acc: 0.109]\n",
      "1799 [D loss: 0.630(R 0.673, F0.588)] [D acc: 0.641(R 0.516, F 0.766)] [G loss: 0.630] [G acc: 0.062]\n",
      "INFO:tensorflow:Assets written to: ram://d7529103-130f-42c7-8bc3-5c61971ce809/assets\n",
      "INFO:tensorflow:Assets written to: ram://b06a226b-3ef3-4933-9abb-c6b5b9063b74/assets\n",
      "INFO:tensorflow:Assets written to: ram://8e294194-d48f-4078-8316-7ec5767d6184/assets\n",
      "1800 [D loss: 0.535(R 0.608, F0.463)] [D acc: 0.727(R 0.609, F 0.844)] [G loss: 0.535] [G acc: 0.062]\n",
      "1801 [D loss: 0.532(R 0.544, F0.521)] [D acc: 0.680(R 0.594, F 0.766)] [G loss: 0.532] [G acc: 0.078]\n",
      "1802 [D loss: 0.491(R 0.479, F0.504)] [D acc: 0.773(R 0.734, F 0.812)] [G loss: 0.491] [G acc: 0.078]\n",
      "1803 [D loss: 0.582(R 0.605, F0.559)] [D acc: 0.680(R 0.594, F 0.766)] [G loss: 0.582] [G acc: 0.094]\n",
      "1804 [D loss: 0.586(R 0.586, F0.586)] [D acc: 0.711(R 0.688, F 0.734)] [G loss: 0.586] [G acc: 0.062]\n",
      "1805 [D loss: 0.540(R 0.561, F0.518)] [D acc: 0.742(R 0.688, F 0.797)] [G loss: 0.540] [G acc: 0.109]\n",
      "1806 [D loss: 0.508(R 0.543, F0.473)] [D acc: 0.727(R 0.609, F 0.844)] [G loss: 0.508] [G acc: 0.109]\n",
      "1807 [D loss: 0.503(R 0.501, F0.505)] [D acc: 0.719(R 0.672, F 0.766)] [G loss: 0.503] [G acc: 0.031]\n",
      "1808 [D loss: 0.556(R 0.520, F0.592)] [D acc: 0.711(R 0.656, F 0.766)] [G loss: 0.556] [G acc: 0.109]\n",
      "1809 [D loss: 0.597(R 0.695, F0.500)] [D acc: 0.672(R 0.531, F 0.812)] [G loss: 0.597] [G acc: 0.141]\n",
      "1810 [D loss: 0.551(R 0.548, F0.553)] [D acc: 0.742(R 0.625, F 0.859)] [G loss: 0.551] [G acc: 0.094]\n",
      "1811 [D loss: 0.540(R 0.688, F0.391)] [D acc: 0.766(R 0.578, F 0.953)] [G loss: 0.540] [G acc: 0.031]\n",
      "1812 [D loss: 0.462(R 0.471, F0.453)] [D acc: 0.812(R 0.750, F 0.875)] [G loss: 0.462] [G acc: 0.047]\n",
      "1813 [D loss: 0.529(R 0.448, F0.610)] [D acc: 0.719(R 0.719, F 0.719)] [G loss: 0.529] [G acc: 0.109]\n",
      "1814 [D loss: 0.579(R 0.616, F0.543)] [D acc: 0.680(R 0.609, F 0.750)] [G loss: 0.579] [G acc: 0.094]\n",
      "1815 [D loss: 0.549(R 0.494, F0.604)] [D acc: 0.719(R 0.734, F 0.703)] [G loss: 0.549] [G acc: 0.125]\n",
      "1816 [D loss: 0.545(R 0.598, F0.491)] [D acc: 0.719(R 0.656, F 0.781)] [G loss: 0.545] [G acc: 0.109]\n",
      "1817 [D loss: 0.450(R 0.479, F0.420)] [D acc: 0.797(R 0.688, F 0.906)] [G loss: 0.450] [G acc: 0.094]\n",
      "1818 [D loss: 0.632(R 0.464, F0.801)] [D acc: 0.703(R 0.703, F 0.703)] [G loss: 0.632] [G acc: 0.078]\n",
      "1819 [D loss: 0.516(R 0.609, F0.422)] [D acc: 0.734(R 0.625, F 0.844)] [G loss: 0.516] [G acc: 0.141]\n",
      "1820 [D loss: 0.567(R 0.579, F0.554)] [D acc: 0.703(R 0.672, F 0.734)] [G loss: 0.567] [G acc: 0.203]\n",
      "1821 [D loss: 0.484(R 0.482, F0.487)] [D acc: 0.766(R 0.734, F 0.797)] [G loss: 0.484] [G acc: 0.094]\n",
      "1822 [D loss: 0.549(R 0.621, F0.477)] [D acc: 0.695(R 0.594, F 0.797)] [G loss: 0.549] [G acc: 0.156]\n",
      "1823 [D loss: 0.431(R 0.390, F0.472)] [D acc: 0.844(R 0.875, F 0.812)] [G loss: 0.431] [G acc: 0.125]\n",
      "1824 [D loss: 0.493(R 0.458, F0.528)] [D acc: 0.742(R 0.719, F 0.766)] [G loss: 0.493] [G acc: 0.078]\n",
      "1825 [D loss: 0.533(R 0.528, F0.539)] [D acc: 0.758(R 0.719, F 0.797)] [G loss: 0.533] [G acc: 0.062]\n",
      "1826 [D loss: 0.568(R 0.657, F0.480)] [D acc: 0.680(R 0.547, F 0.812)] [G loss: 0.568] [G acc: 0.141]\n",
      "1827 [D loss: 0.638(R 0.543, F0.733)] [D acc: 0.719(R 0.719, F 0.719)] [G loss: 0.638] [G acc: 0.078]\n",
      "1828 [D loss: 0.601(R 0.579, F0.622)] [D acc: 0.703(R 0.656, F 0.750)] [G loss: 0.601] [G acc: 0.125]\n",
      "1829 [D loss: 0.528(R 0.573, F0.484)] [D acc: 0.734(R 0.656, F 0.812)] [G loss: 0.528] [G acc: 0.125]\n",
      "1830 [D loss: 0.542(R 0.549, F0.534)] [D acc: 0.680(R 0.656, F 0.703)] [G loss: 0.542] [G acc: 0.109]\n",
      "1831 [D loss: 0.531(R 0.535, F0.528)] [D acc: 0.734(R 0.672, F 0.797)] [G loss: 0.531] [G acc: 0.078]\n",
      "1832 [D loss: 0.543(R 0.631, F0.455)] [D acc: 0.719(R 0.594, F 0.844)] [G loss: 0.543] [G acc: 0.125]\n",
      "1833 [D loss: 0.479(R 0.447, F0.511)] [D acc: 0.750(R 0.734, F 0.766)] [G loss: 0.479] [G acc: 0.078]\n",
      "1834 [D loss: 0.587(R 0.615, F0.558)] [D acc: 0.695(R 0.672, F 0.719)] [G loss: 0.587] [G acc: 0.125]\n",
      "1835 [D loss: 0.540(R 0.528, F0.552)] [D acc: 0.680(R 0.625, F 0.734)] [G loss: 0.540] [G acc: 0.172]\n",
      "1836 [D loss: 0.421(R 0.358, F0.483)] [D acc: 0.828(R 0.812, F 0.844)] [G loss: 0.421] [G acc: 0.047]\n",
      "1837 [D loss: 0.524(R 0.470, F0.578)] [D acc: 0.734(R 0.719, F 0.750)] [G loss: 0.524] [G acc: 0.047]\n",
      "1838 [D loss: 0.612(R 0.690, F0.535)] [D acc: 0.648(R 0.531, F 0.766)] [G loss: 0.612] [G acc: 0.078]\n",
      "1839 [D loss: 0.559(R 0.613, F0.505)] [D acc: 0.711(R 0.625, F 0.797)] [G loss: 0.559] [G acc: 0.062]\n",
      "1840 [D loss: 0.492(R 0.545, F0.439)] [D acc: 0.789(R 0.703, F 0.875)] [G loss: 0.492] [G acc: 0.062]\n",
      "1841 [D loss: 0.692(R 0.558, F0.826)] [D acc: 0.625(R 0.641, F 0.609)] [G loss: 0.692] [G acc: 0.062]\n",
      "1842 [D loss: 0.538(R 0.633, F0.442)] [D acc: 0.742(R 0.562, F 0.922)] [G loss: 0.538] [G acc: 0.062]\n",
      "1843 [D loss: 0.528(R 0.494, F0.562)] [D acc: 0.750(R 0.703, F 0.797)] [G loss: 0.528] [G acc: 0.062]\n",
      "1844 [D loss: 0.510(R 0.486, F0.534)] [D acc: 0.680(R 0.656, F 0.703)] [G loss: 0.510] [G acc: 0.156]\n",
      "1845 [D loss: 0.539(R 0.555, F0.523)] [D acc: 0.750(R 0.672, F 0.828)] [G loss: 0.539] [G acc: 0.016]\n",
      "1846 [D loss: 0.467(R 0.409, F0.525)] [D acc: 0.797(R 0.766, F 0.828)] [G loss: 0.467] [G acc: 0.078]\n",
      "1847 [D loss: 0.549(R 0.581, F0.518)] [D acc: 0.734(R 0.703, F 0.766)] [G loss: 0.549] [G acc: 0.125]\n",
      "1848 [D loss: 0.573(R 0.587, F0.559)] [D acc: 0.680(R 0.594, F 0.766)] [G loss: 0.573] [G acc: 0.109]\n",
      "1849 [D loss: 0.552(R 0.469, F0.634)] [D acc: 0.711(R 0.750, F 0.672)] [G loss: 0.552] [G acc: 0.141]\n",
      "1850 [D loss: 0.557(R 0.618, F0.497)] [D acc: 0.734(R 0.641, F 0.828)] [G loss: 0.557] [G acc: 0.125]\n",
      "1851 [D loss: 0.529(R 0.592, F0.465)] [D acc: 0.734(R 0.609, F 0.859)] [G loss: 0.529] [G acc: 0.109]\n",
      "1852 [D loss: 0.505(R 0.551, F0.460)] [D acc: 0.742(R 0.656, F 0.828)] [G loss: 0.505] [G acc: 0.109]\n",
      "1853 [D loss: 0.468(R 0.363, F0.573)] [D acc: 0.789(R 0.828, F 0.750)] [G loss: 0.468] [G acc: 0.109]\n",
      "1854 [D loss: 0.467(R 0.481, F0.452)] [D acc: 0.773(R 0.719, F 0.828)] [G loss: 0.467] [G acc: 0.109]\n",
      "1855 [D loss: 0.460(R 0.450, F0.470)] [D acc: 0.766(R 0.766, F 0.766)] [G loss: 0.460] [G acc: 0.078]\n",
      "1856 [D loss: 0.487(R 0.536, F0.439)] [D acc: 0.766(R 0.656, F 0.875)] [G loss: 0.487] [G acc: 0.062]\n",
      "1857 [D loss: 0.515(R 0.445, F0.585)] [D acc: 0.727(R 0.734, F 0.719)] [G loss: 0.515] [G acc: 0.094]\n",
      "1858 [D loss: 0.626(R 0.612, F0.640)] [D acc: 0.695(R 0.672, F 0.719)] [G loss: 0.626] [G acc: 0.078]\n",
      "1859 [D loss: 0.540(R 0.620, F0.460)] [D acc: 0.758(R 0.641, F 0.875)] [G loss: 0.540] [G acc: 0.031]\n",
      "1860 [D loss: 0.587(R 0.598, F0.577)] [D acc: 0.750(R 0.688, F 0.812)] [G loss: 0.587] [G acc: 0.141]\n",
      "1861 [D loss: 0.527(R 0.581, F0.474)] [D acc: 0.727(R 0.641, F 0.812)] [G loss: 0.527] [G acc: 0.031]\n",
      "1862 [D loss: 0.596(R 0.620, F0.573)] [D acc: 0.695(R 0.609, F 0.781)] [G loss: 0.596] [G acc: 0.141]\n",
      "1863 [D loss: 0.588(R 0.570, F0.607)] [D acc: 0.742(R 0.719, F 0.766)] [G loss: 0.588] [G acc: 0.125]\n",
      "1864 [D loss: 0.556(R 0.552, F0.560)] [D acc: 0.773(R 0.734, F 0.812)] [G loss: 0.556] [G acc: 0.156]\n",
      "1865 [D loss: 0.581(R 0.616, F0.546)] [D acc: 0.719(R 0.625, F 0.812)] [G loss: 0.581] [G acc: 0.141]\n",
      "1866 [D loss: 0.586(R 0.663, F0.509)] [D acc: 0.656(R 0.562, F 0.750)] [G loss: 0.586] [G acc: 0.094]\n",
      "1867 [D loss: 0.708(R 0.720, F0.696)] [D acc: 0.617(R 0.516, F 0.719)] [G loss: 0.708] [G acc: 0.125]\n",
      "1868 [D loss: 0.531(R 0.579, F0.483)] [D acc: 0.703(R 0.625, F 0.781)] [G loss: 0.531] [G acc: 0.109]\n",
      "1869 [D loss: 0.599(R 0.612, F0.585)] [D acc: 0.664(R 0.609, F 0.719)] [G loss: 0.599] [G acc: 0.078]\n",
      "1870 [D loss: 0.508(R 0.521, F0.494)] [D acc: 0.711(R 0.672, F 0.750)] [G loss: 0.508] [G acc: 0.094]\n",
      "1871 [D loss: 0.584(R 0.555, F0.614)] [D acc: 0.703(R 0.672, F 0.734)] [G loss: 0.584] [G acc: 0.078]\n",
      "1872 [D loss: 0.557(R 0.624, F0.491)] [D acc: 0.727(R 0.641, F 0.812)] [G loss: 0.557] [G acc: 0.094]\n",
      "1873 [D loss: 0.552(R 0.579, F0.525)] [D acc: 0.711(R 0.641, F 0.781)] [G loss: 0.552] [G acc: 0.094]\n",
      "1874 [D loss: 0.523(R 0.463, F0.583)] [D acc: 0.742(R 0.719, F 0.766)] [G loss: 0.523] [G acc: 0.094]\n",
      "1875 [D loss: 0.497(R 0.516, F0.477)] [D acc: 0.766(R 0.703, F 0.828)] [G loss: 0.497] [G acc: 0.109]\n",
      "1876 [D loss: 0.543(R 0.448, F0.638)] [D acc: 0.750(R 0.750, F 0.750)] [G loss: 0.543] [G acc: 0.078]\n",
      "1877 [D loss: 0.544(R 0.634, F0.454)] [D acc: 0.711(R 0.547, F 0.875)] [G loss: 0.544] [G acc: 0.094]\n",
      "1878 [D loss: 0.512(R 0.485, F0.540)] [D acc: 0.734(R 0.672, F 0.797)] [G loss: 0.512] [G acc: 0.109]\n",
      "1879 [D loss: 0.550(R 0.611, F0.490)] [D acc: 0.695(R 0.641, F 0.750)] [G loss: 0.550] [G acc: 0.078]\n",
      "1880 [D loss: 0.465(R 0.455, F0.475)] [D acc: 0.766(R 0.703, F 0.828)] [G loss: 0.465] [G acc: 0.078]\n",
      "1881 [D loss: 0.613(R 0.594, F0.631)] [D acc: 0.711(R 0.656, F 0.766)] [G loss: 0.613] [G acc: 0.078]\n",
      "1882 [D loss: 0.572(R 0.564, F0.579)] [D acc: 0.711(R 0.641, F 0.781)] [G loss: 0.572] [G acc: 0.109]\n",
      "1883 [D loss: 0.579(R 0.629, F0.530)] [D acc: 0.656(R 0.547, F 0.766)] [G loss: 0.579] [G acc: 0.156]\n",
      "1884 [D loss: 0.516(R 0.611, F0.422)] [D acc: 0.781(R 0.672, F 0.891)] [G loss: 0.516] [G acc: 0.141]\n",
      "1885 [D loss: 0.592(R 0.522, F0.661)] [D acc: 0.688(R 0.672, F 0.703)] [G loss: 0.592] [G acc: 0.062]\n",
      "1886 [D loss: 0.553(R 0.619, F0.487)] [D acc: 0.688(R 0.578, F 0.797)] [G loss: 0.553] [G acc: 0.078]\n",
      "1887 [D loss: 0.523(R 0.515, F0.530)] [D acc: 0.758(R 0.688, F 0.828)] [G loss: 0.523] [G acc: 0.047]\n",
      "1888 [D loss: 0.463(R 0.476, F0.450)] [D acc: 0.789(R 0.734, F 0.844)] [G loss: 0.463] [G acc: 0.141]\n",
      "1889 [D loss: 0.610(R 0.567, F0.653)] [D acc: 0.680(R 0.641, F 0.719)] [G loss: 0.610] [G acc: 0.094]\n",
      "1890 [D loss: 0.429(R 0.383, F0.475)] [D acc: 0.812(R 0.797, F 0.828)] [G loss: 0.429] [G acc: 0.125]\n",
      "1891 [D loss: 0.527(R 0.586, F0.468)] [D acc: 0.797(R 0.703, F 0.891)] [G loss: 0.527] [G acc: 0.047]\n",
      "1892 [D loss: 0.572(R 0.626, F0.519)] [D acc: 0.703(R 0.578, F 0.828)] [G loss: 0.572] [G acc: 0.188]\n",
      "1893 [D loss: 0.557(R 0.646, F0.468)] [D acc: 0.711(R 0.578, F 0.844)] [G loss: 0.557] [G acc: 0.078]\n",
      "1894 [D loss: 0.640(R 0.606, F0.675)] [D acc: 0.664(R 0.641, F 0.688)] [G loss: 0.640] [G acc: 0.016]\n",
      "1895 [D loss: 0.718(R 0.651, F0.785)] [D acc: 0.633(R 0.625, F 0.641)] [G loss: 0.718] [G acc: 0.078]\n",
      "1896 [D loss: 0.461(R 0.517, F0.405)] [D acc: 0.812(R 0.703, F 0.922)] [G loss: 0.461] [G acc: 0.109]\n",
      "1897 [D loss: 0.583(R 0.624, F0.543)] [D acc: 0.695(R 0.594, F 0.797)] [G loss: 0.583] [G acc: 0.047]\n",
      "1898 [D loss: 0.535(R 0.514, F0.557)] [D acc: 0.719(R 0.672, F 0.766)] [G loss: 0.535] [G acc: 0.125]\n",
      "1899 [D loss: 0.423(R 0.398, F0.448)] [D acc: 0.820(R 0.797, F 0.844)] [G loss: 0.423] [G acc: 0.031]\n",
      "INFO:tensorflow:Assets written to: ram://85f0c10e-6cf7-47e4-8802-40aefd7d2888/assets\n",
      "INFO:tensorflow:Assets written to: ram://4926cc81-ebd9-494a-812c-20930e5d142a/assets\n",
      "INFO:tensorflow:Assets written to: ram://bba30074-0f5a-4cfd-af55-aaf007aaf173/assets\n",
      "1900 [D loss: 0.494(R 0.552, F0.436)] [D acc: 0.758(R 0.703, F 0.812)] [G loss: 0.494] [G acc: 0.156]\n",
      "1901 [D loss: 0.565(R 0.573, F0.557)] [D acc: 0.703(R 0.656, F 0.750)] [G loss: 0.565] [G acc: 0.125]\n",
      "1902 [D loss: 0.511(R 0.536, F0.486)] [D acc: 0.727(R 0.625, F 0.828)] [G loss: 0.511] [G acc: 0.109]\n",
      "1903 [D loss: 0.535(R 0.642, F0.427)] [D acc: 0.719(R 0.578, F 0.859)] [G loss: 0.535] [G acc: 0.047]\n",
      "1904 [D loss: 0.577(R 0.545, F0.608)] [D acc: 0.703(R 0.672, F 0.734)] [G loss: 0.577] [G acc: 0.062]\n",
      "1905 [D loss: 0.550(R 0.433, F0.667)] [D acc: 0.781(R 0.734, F 0.828)] [G loss: 0.550] [G acc: 0.062]\n",
      "1906 [D loss: 0.547(R 0.603, F0.490)] [D acc: 0.727(R 0.656, F 0.797)] [G loss: 0.547] [G acc: 0.078]\n",
      "1907 [D loss: 0.493(R 0.448, F0.539)] [D acc: 0.734(R 0.781, F 0.688)] [G loss: 0.493] [G acc: 0.078]\n",
      "1908 [D loss: 0.421(R 0.451, F0.391)] [D acc: 0.812(R 0.719, F 0.906)] [G loss: 0.421] [G acc: 0.109]\n",
      "1909 [D loss: 0.591(R 0.614, F0.568)] [D acc: 0.711(R 0.625, F 0.797)] [G loss: 0.591] [G acc: 0.188]\n",
      "1910 [D loss: 0.521(R 0.553, F0.489)] [D acc: 0.727(R 0.641, F 0.812)] [G loss: 0.521] [G acc: 0.109]\n",
      "1911 [D loss: 0.570(R 0.541, F0.600)] [D acc: 0.727(R 0.688, F 0.766)] [G loss: 0.570] [G acc: 0.078]\n",
      "1912 [D loss: 0.497(R 0.554, F0.441)] [D acc: 0.742(R 0.625, F 0.859)] [G loss: 0.497] [G acc: 0.047]\n",
      "1913 [D loss: 0.475(R 0.482, F0.467)] [D acc: 0.773(R 0.734, F 0.812)] [G loss: 0.475] [G acc: 0.062]\n",
      "1914 [D loss: 0.523(R 0.582, F0.463)] [D acc: 0.734(R 0.609, F 0.859)] [G loss: 0.523] [G acc: 0.094]\n",
      "1915 [D loss: 0.457(R 0.525, F0.388)] [D acc: 0.758(R 0.672, F 0.844)] [G loss: 0.457] [G acc: 0.156]\n",
      "1916 [D loss: 0.518(R 0.504, F0.533)] [D acc: 0.766(R 0.734, F 0.797)] [G loss: 0.518] [G acc: 0.062]\n",
      "1917 [D loss: 0.513(R 0.401, F0.626)] [D acc: 0.781(R 0.828, F 0.734)] [G loss: 0.513] [G acc: 0.016]\n",
      "1918 [D loss: 0.591(R 0.661, F0.520)] [D acc: 0.695(R 0.609, F 0.781)] [G loss: 0.591] [G acc: 0.094]\n",
      "1919 [D loss: 0.531(R 0.561, F0.501)] [D acc: 0.703(R 0.641, F 0.766)] [G loss: 0.531] [G acc: 0.172]\n",
      "1920 [D loss: 0.432(R 0.469, F0.395)] [D acc: 0.781(R 0.688, F 0.875)] [G loss: 0.432] [G acc: 0.188]\n",
      "1921 [D loss: 0.585(R 0.559, F0.611)] [D acc: 0.711(R 0.672, F 0.750)] [G loss: 0.585] [G acc: 0.109]\n",
      "1922 [D loss: 0.529(R 0.586, F0.472)] [D acc: 0.758(R 0.656, F 0.859)] [G loss: 0.529] [G acc: 0.062]\n",
      "1923 [D loss: 0.480(R 0.495, F0.466)] [D acc: 0.750(R 0.641, F 0.859)] [G loss: 0.480] [G acc: 0.125]\n",
      "1924 [D loss: 0.615(R 0.602, F0.628)] [D acc: 0.680(R 0.594, F 0.766)] [G loss: 0.615] [G acc: 0.078]\n",
      "1925 [D loss: 0.524(R 0.556, F0.492)] [D acc: 0.758(R 0.641, F 0.875)] [G loss: 0.524] [G acc: 0.094]\n",
      "1926 [D loss: 0.545(R 0.455, F0.635)] [D acc: 0.719(R 0.734, F 0.703)] [G loss: 0.545] [G acc: 0.031]\n",
      "1927 [D loss: 0.512(R 0.544, F0.481)] [D acc: 0.727(R 0.641, F 0.812)] [G loss: 0.512] [G acc: 0.062]\n",
      "1928 [D loss: 0.537(R 0.520, F0.554)] [D acc: 0.734(R 0.641, F 0.828)] [G loss: 0.537] [G acc: 0.062]\n",
      "1929 [D loss: 0.481(R 0.515, F0.446)] [D acc: 0.758(R 0.703, F 0.812)] [G loss: 0.481] [G acc: 0.078]\n",
      "1930 [D loss: 0.504(R 0.503, F0.505)] [D acc: 0.766(R 0.672, F 0.859)] [G loss: 0.504] [G acc: 0.109]\n",
      "1931 [D loss: 0.632(R 0.574, F0.690)] [D acc: 0.625(R 0.609, F 0.641)] [G loss: 0.632] [G acc: 0.109]\n",
      "1932 [D loss: 0.590(R 0.657, F0.524)] [D acc: 0.703(R 0.578, F 0.828)] [G loss: 0.590] [G acc: 0.125]\n",
      "1933 [D loss: 0.590(R 0.560, F0.620)] [D acc: 0.680(R 0.656, F 0.703)] [G loss: 0.590] [G acc: 0.031]\n",
      "1934 [D loss: 0.447(R 0.531, F0.362)] [D acc: 0.820(R 0.703, F 0.938)] [G loss: 0.447] [G acc: 0.078]\n",
      "1935 [D loss: 0.496(R 0.464, F0.528)] [D acc: 0.734(R 0.672, F 0.797)] [G loss: 0.496] [G acc: 0.031]\n",
      "1936 [D loss: 0.489(R 0.446, F0.531)] [D acc: 0.727(R 0.703, F 0.750)] [G loss: 0.489] [G acc: 0.062]\n",
      "1937 [D loss: 0.518(R 0.581, F0.454)] [D acc: 0.727(R 0.641, F 0.812)] [G loss: 0.518] [G acc: 0.016]\n",
      "1938 [D loss: 0.671(R 0.867, F0.474)] [D acc: 0.656(R 0.453, F 0.859)] [G loss: 0.671] [G acc: 0.062]\n",
      "1939 [D loss: 0.462(R 0.470, F0.454)] [D acc: 0.781(R 0.719, F 0.844)] [G loss: 0.462] [G acc: 0.125]\n",
      "1940 [D loss: 0.448(R 0.451, F0.444)] [D acc: 0.766(R 0.688, F 0.844)] [G loss: 0.448] [G acc: 0.141]\n",
      "1941 [D loss: 0.563(R 0.432, F0.694)] [D acc: 0.695(R 0.703, F 0.688)] [G loss: 0.563] [G acc: 0.078]\n",
      "1942 [D loss: 0.635(R 0.581, F0.689)] [D acc: 0.672(R 0.578, F 0.766)] [G loss: 0.635] [G acc: 0.094]\n",
      "1943 [D loss: 0.509(R 0.535, F0.483)] [D acc: 0.742(R 0.625, F 0.859)] [G loss: 0.509] [G acc: 0.141]\n",
      "1944 [D loss: 0.527(R 0.584, F0.469)] [D acc: 0.758(R 0.641, F 0.875)] [G loss: 0.527] [G acc: 0.125]\n",
      "1945 [D loss: 0.525(R 0.457, F0.593)] [D acc: 0.758(R 0.781, F 0.734)] [G loss: 0.525] [G acc: 0.094]\n",
      "1946 [D loss: 0.578(R 0.703, F0.452)] [D acc: 0.703(R 0.562, F 0.844)] [G loss: 0.578] [G acc: 0.109]\n",
      "1947 [D loss: 0.473(R 0.525, F0.421)] [D acc: 0.797(R 0.703, F 0.891)] [G loss: 0.473] [G acc: 0.031]\n",
      "1948 [D loss: 0.580(R 0.692, F0.468)] [D acc: 0.688(R 0.562, F 0.812)] [G loss: 0.580] [G acc: 0.047]\n",
      "1949 [D loss: 0.463(R 0.474, F0.453)] [D acc: 0.773(R 0.734, F 0.812)] [G loss: 0.463] [G acc: 0.141]\n",
      "1950 [D loss: 0.550(R 0.578, F0.522)] [D acc: 0.711(R 0.672, F 0.750)] [G loss: 0.550] [G acc: 0.078]\n",
      "1951 [D loss: 0.512(R 0.568, F0.456)] [D acc: 0.734(R 0.656, F 0.812)] [G loss: 0.512] [G acc: 0.078]\n",
      "1952 [D loss: 0.630(R 0.674, F0.586)] [D acc: 0.703(R 0.594, F 0.812)] [G loss: 0.630] [G acc: 0.078]\n",
      "1953 [D loss: 0.553(R 0.628, F0.479)] [D acc: 0.758(R 0.672, F 0.844)] [G loss: 0.553] [G acc: 0.141]\n",
      "1954 [D loss: 0.537(R 0.512, F0.562)] [D acc: 0.758(R 0.734, F 0.781)] [G loss: 0.537] [G acc: 0.094]\n",
      "1955 [D loss: 0.574(R 0.520, F0.629)] [D acc: 0.734(R 0.688, F 0.781)] [G loss: 0.574] [G acc: 0.047]\n",
      "1956 [D loss: 0.481(R 0.566, F0.397)] [D acc: 0.773(R 0.641, F 0.906)] [G loss: 0.481] [G acc: 0.078]\n",
      "1957 [D loss: 0.498(R 0.504, F0.492)] [D acc: 0.734(R 0.672, F 0.797)] [G loss: 0.498] [G acc: 0.219]\n",
      "1958 [D loss: 0.491(R 0.533, F0.449)] [D acc: 0.742(R 0.672, F 0.812)] [G loss: 0.491] [G acc: 0.109]\n",
      "1959 [D loss: 0.498(R 0.413, F0.582)] [D acc: 0.742(R 0.750, F 0.734)] [G loss: 0.498] [G acc: 0.016]\n",
      "1960 [D loss: 0.534(R 0.520, F0.548)] [D acc: 0.766(R 0.719, F 0.812)] [G loss: 0.534] [G acc: 0.062]\n",
      "1961 [D loss: 0.535(R 0.611, F0.459)] [D acc: 0.711(R 0.609, F 0.812)] [G loss: 0.535] [G acc: 0.062]\n",
      "1962 [D loss: 0.566(R 0.598, F0.534)] [D acc: 0.695(R 0.641, F 0.750)] [G loss: 0.566] [G acc: 0.094]\n",
      "1963 [D loss: 0.497(R 0.509, F0.484)] [D acc: 0.750(R 0.688, F 0.812)] [G loss: 0.497] [G acc: 0.109]\n",
      "1964 [D loss: 0.507(R 0.495, F0.519)] [D acc: 0.719(R 0.672, F 0.766)] [G loss: 0.507] [G acc: 0.125]\n",
      "1965 [D loss: 0.485(R 0.590, F0.380)] [D acc: 0.773(R 0.641, F 0.906)] [G loss: 0.485] [G acc: 0.094]\n",
      "1966 [D loss: 0.566(R 0.609, F0.523)] [D acc: 0.703(R 0.594, F 0.812)] [G loss: 0.566] [G acc: 0.078]\n",
      "1967 [D loss: 0.538(R 0.561, F0.516)] [D acc: 0.750(R 0.672, F 0.828)] [G loss: 0.538] [G acc: 0.125]\n",
      "1968 [D loss: 0.610(R 0.618, F0.603)] [D acc: 0.680(R 0.625, F 0.734)] [G loss: 0.610] [G acc: 0.172]\n",
      "1969 [D loss: 0.571(R 0.622, F0.521)] [D acc: 0.680(R 0.578, F 0.781)] [G loss: 0.571] [G acc: 0.094]\n",
      "1970 [D loss: 0.502(R 0.508, F0.496)] [D acc: 0.773(R 0.734, F 0.812)] [G loss: 0.502] [G acc: 0.125]\n",
      "1971 [D loss: 0.600(R 0.671, F0.530)] [D acc: 0.672(R 0.531, F 0.812)] [G loss: 0.600] [G acc: 0.156]\n",
      "1972 [D loss: 0.549(R 0.431, F0.668)] [D acc: 0.719(R 0.703, F 0.734)] [G loss: 0.549] [G acc: 0.125]\n",
      "1973 [D loss: 0.480(R 0.530, F0.429)] [D acc: 0.742(R 0.594, F 0.891)] [G loss: 0.480] [G acc: 0.172]\n",
      "1974 [D loss: 0.565(R 0.530, F0.601)] [D acc: 0.656(R 0.625, F 0.688)] [G loss: 0.565] [G acc: 0.078]\n",
      "1975 [D loss: 0.602(R 0.702, F0.502)] [D acc: 0.664(R 0.531, F 0.797)] [G loss: 0.602] [G acc: 0.078]\n",
      "1976 [D loss: 0.485(R 0.416, F0.554)] [D acc: 0.805(R 0.797, F 0.812)] [G loss: 0.485] [G acc: 0.125]\n",
      "1977 [D loss: 0.514(R 0.562, F0.466)] [D acc: 0.766(R 0.672, F 0.859)] [G loss: 0.514] [G acc: 0.109]\n",
      "1978 [D loss: 0.567(R 0.658, F0.476)] [D acc: 0.734(R 0.625, F 0.844)] [G loss: 0.567] [G acc: 0.062]\n",
      "1979 [D loss: 0.551(R 0.507, F0.595)] [D acc: 0.719(R 0.703, F 0.734)] [G loss: 0.551] [G acc: 0.047]\n",
      "1980 [D loss: 0.574(R 0.621, F0.528)] [D acc: 0.719(R 0.594, F 0.844)] [G loss: 0.574] [G acc: 0.109]\n",
      "1981 [D loss: 0.483(R 0.439, F0.527)] [D acc: 0.781(R 0.781, F 0.781)] [G loss: 0.483] [G acc: 0.094]\n",
      "1982 [D loss: 0.546(R 0.524, F0.568)] [D acc: 0.703(R 0.641, F 0.766)] [G loss: 0.546] [G acc: 0.094]\n",
      "1983 [D loss: 0.520(R 0.470, F0.570)] [D acc: 0.711(R 0.688, F 0.734)] [G loss: 0.520] [G acc: 0.109]\n",
      "1984 [D loss: 0.452(R 0.464, F0.440)] [D acc: 0.797(R 0.750, F 0.844)] [G loss: 0.452] [G acc: 0.141]\n",
      "1985 [D loss: 0.511(R 0.441, F0.580)] [D acc: 0.711(R 0.703, F 0.719)] [G loss: 0.511] [G acc: 0.062]\n",
      "1986 [D loss: 0.548(R 0.663, F0.433)] [D acc: 0.711(R 0.594, F 0.828)] [G loss: 0.548] [G acc: 0.125]\n",
      "1987 [D loss: 0.554(R 0.548, F0.560)] [D acc: 0.672(R 0.625, F 0.719)] [G loss: 0.554] [G acc: 0.094]\n",
      "1988 [D loss: 0.462(R 0.482, F0.441)] [D acc: 0.766(R 0.703, F 0.828)] [G loss: 0.462] [G acc: 0.078]\n",
      "1989 [D loss: 0.506(R 0.486, F0.527)] [D acc: 0.766(R 0.750, F 0.781)] [G loss: 0.506] [G acc: 0.078]\n",
      "1990 [D loss: 0.518(R 0.459, F0.577)] [D acc: 0.750(R 0.719, F 0.781)] [G loss: 0.518] [G acc: 0.047]\n",
      "1991 [D loss: 0.555(R 0.518, F0.592)] [D acc: 0.719(R 0.656, F 0.781)] [G loss: 0.555] [G acc: 0.125]\n",
      "1992 [D loss: 0.488(R 0.528, F0.448)] [D acc: 0.766(R 0.672, F 0.859)] [G loss: 0.488] [G acc: 0.094]\n",
      "1993 [D loss: 0.593(R 0.601, F0.585)] [D acc: 0.656(R 0.609, F 0.703)] [G loss: 0.593] [G acc: 0.078]\n",
      "1994 [D loss: 0.504(R 0.524, F0.483)] [D acc: 0.773(R 0.719, F 0.828)] [G loss: 0.504] [G acc: 0.094]\n",
      "1995 [D loss: 0.547(R 0.455, F0.640)] [D acc: 0.727(R 0.781, F 0.672)] [G loss: 0.547] [G acc: 0.094]\n",
      "1996 [D loss: 0.553(R 0.702, F0.405)] [D acc: 0.719(R 0.578, F 0.859)] [G loss: 0.553] [G acc: 0.109]\n",
      "1997 [D loss: 0.530(R 0.524, F0.537)] [D acc: 0.742(R 0.703, F 0.781)] [G loss: 0.530] [G acc: 0.125]\n",
      "1998 [D loss: 0.626(R 0.608, F0.644)] [D acc: 0.672(R 0.625, F 0.719)] [G loss: 0.626] [G acc: 0.062]\n",
      "1999 [D loss: 0.540(R 0.647, F0.433)] [D acc: 0.695(R 0.531, F 0.859)] [G loss: 0.540] [G acc: 0.078]\n",
      "INFO:tensorflow:Assets written to: ram://c50b665d-051d-4286-8d92-2874ccd85583/assets\n",
      "INFO:tensorflow:Assets written to: ram://8f009cbc-f6a9-40d3-822b-5a26e486ad12/assets\n",
      "INFO:tensorflow:Assets written to: ram://7bda208a-5fe3-464a-993b-b8adcc785789/assets\n",
      "2000 [D loss: 0.530(R 0.558, F0.502)] [D acc: 0.734(R 0.641, F 0.828)] [G loss: 0.530] [G acc: 0.156]\n",
      "2001 [D loss: 0.446(R 0.465, F0.428)] [D acc: 0.773(R 0.672, F 0.875)] [G loss: 0.446] [G acc: 0.141]\n",
      "2002 [D loss: 0.583(R 0.605, F0.560)] [D acc: 0.711(R 0.641, F 0.781)] [G loss: 0.583] [G acc: 0.250]\n",
      "2003 [D loss: 0.507(R 0.497, F0.516)] [D acc: 0.750(R 0.734, F 0.766)] [G loss: 0.507] [G acc: 0.234]\n",
      "2004 [D loss: 0.554(R 0.547, F0.562)] [D acc: 0.695(R 0.688, F 0.703)] [G loss: 0.554] [G acc: 0.062]\n",
      "2005 [D loss: 0.466(R 0.513, F0.419)] [D acc: 0.766(R 0.625, F 0.906)] [G loss: 0.466] [G acc: 0.078]\n",
      "2006 [D loss: 0.511(R 0.444, F0.578)] [D acc: 0.711(R 0.719, F 0.703)] [G loss: 0.511] [G acc: 0.141]\n",
      "2007 [D loss: 0.519(R 0.463, F0.574)] [D acc: 0.758(R 0.688, F 0.828)] [G loss: 0.519] [G acc: 0.109]\n",
      "2008 [D loss: 0.532(R 0.550, F0.515)] [D acc: 0.695(R 0.594, F 0.797)] [G loss: 0.532] [G acc: 0.156]\n",
      "2009 [D loss: 0.654(R 0.670, F0.637)] [D acc: 0.648(R 0.578, F 0.719)] [G loss: 0.654] [G acc: 0.094]\n",
      "2010 [D loss: 0.540(R 0.527, F0.554)] [D acc: 0.742(R 0.703, F 0.781)] [G loss: 0.540] [G acc: 0.141]\n",
      "2011 [D loss: 0.513(R 0.473, F0.553)] [D acc: 0.750(R 0.734, F 0.766)] [G loss: 0.513] [G acc: 0.141]\n",
      "2012 [D loss: 0.513(R 0.545, F0.482)] [D acc: 0.734(R 0.703, F 0.766)] [G loss: 0.513] [G acc: 0.047]\n",
      "2013 [D loss: 0.537(R 0.476, F0.598)] [D acc: 0.758(R 0.781, F 0.734)] [G loss: 0.537] [G acc: 0.125]\n",
      "2014 [D loss: 0.495(R 0.514, F0.476)] [D acc: 0.734(R 0.656, F 0.812)] [G loss: 0.495] [G acc: 0.078]\n",
      "2015 [D loss: 0.547(R 0.556, F0.538)] [D acc: 0.719(R 0.688, F 0.750)] [G loss: 0.547] [G acc: 0.125]\n",
      "2016 [D loss: 0.525(R 0.533, F0.518)] [D acc: 0.742(R 0.688, F 0.797)] [G loss: 0.525] [G acc: 0.078]\n",
      "2017 [D loss: 0.515(R 0.548, F0.483)] [D acc: 0.758(R 0.672, F 0.844)] [G loss: 0.515] [G acc: 0.141]\n",
      "2018 [D loss: 0.561(R 0.514, F0.607)] [D acc: 0.734(R 0.719, F 0.750)] [G loss: 0.561] [G acc: 0.031]\n",
      "2019 [D loss: 0.525(R 0.563, F0.488)] [D acc: 0.750(R 0.688, F 0.812)] [G loss: 0.525] [G acc: 0.094]\n",
      "2020 [D loss: 0.549(R 0.604, F0.493)] [D acc: 0.719(R 0.625, F 0.812)] [G loss: 0.549] [G acc: 0.141]\n",
      "2021 [D loss: 0.518(R 0.516, F0.520)] [D acc: 0.773(R 0.750, F 0.797)] [G loss: 0.518] [G acc: 0.094]\n",
      "2022 [D loss: 0.546(R 0.634, F0.458)] [D acc: 0.742(R 0.562, F 0.922)] [G loss: 0.546] [G acc: 0.141]\n",
      "2023 [D loss: 0.541(R 0.367, F0.715)] [D acc: 0.703(R 0.750, F 0.656)] [G loss: 0.541] [G acc: 0.047]\n",
      "2024 [D loss: 0.556(R 0.590, F0.522)] [D acc: 0.719(R 0.672, F 0.766)] [G loss: 0.556] [G acc: 0.062]\n",
      "2025 [D loss: 0.587(R 0.622, F0.552)] [D acc: 0.664(R 0.578, F 0.750)] [G loss: 0.587] [G acc: 0.094]\n",
      "2026 [D loss: 0.603(R 0.555, F0.651)] [D acc: 0.695(R 0.656, F 0.734)] [G loss: 0.603] [G acc: 0.062]\n",
      "2027 [D loss: 0.519(R 0.542, F0.497)] [D acc: 0.750(R 0.688, F 0.812)] [G loss: 0.519] [G acc: 0.062]\n",
      "2028 [D loss: 0.505(R 0.514, F0.497)] [D acc: 0.789(R 0.766, F 0.812)] [G loss: 0.505] [G acc: 0.078]\n",
      "2029 [D loss: 0.432(R 0.474, F0.390)] [D acc: 0.781(R 0.672, F 0.891)] [G loss: 0.432] [G acc: 0.062]\n",
      "2030 [D loss: 0.529(R 0.524, F0.534)] [D acc: 0.742(R 0.672, F 0.812)] [G loss: 0.529] [G acc: 0.125]\n",
      "2031 [D loss: 0.520(R 0.653, F0.387)] [D acc: 0.766(R 0.609, F 0.922)] [G loss: 0.520] [G acc: 0.188]\n",
      "2032 [D loss: 0.604(R 0.507, F0.700)] [D acc: 0.672(R 0.672, F 0.672)] [G loss: 0.604] [G acc: 0.078]\n",
      "2033 [D loss: 0.454(R 0.471, F0.438)] [D acc: 0.805(R 0.734, F 0.875)] [G loss: 0.454] [G acc: 0.031]\n",
      "2034 [D loss: 0.589(R 0.589, F0.590)] [D acc: 0.672(R 0.609, F 0.734)] [G loss: 0.589] [G acc: 0.047]\n",
      "2035 [D loss: 0.535(R 0.578, F0.492)] [D acc: 0.742(R 0.641, F 0.844)] [G loss: 0.535] [G acc: 0.125]\n",
      "2036 [D loss: 0.558(R 0.605, F0.511)] [D acc: 0.695(R 0.594, F 0.797)] [G loss: 0.558] [G acc: 0.141]\n",
      "2037 [D loss: 0.619(R 0.665, F0.572)] [D acc: 0.672(R 0.547, F 0.797)] [G loss: 0.619] [G acc: 0.109]\n",
      "2038 [D loss: 0.566(R 0.526, F0.607)] [D acc: 0.680(R 0.641, F 0.719)] [G loss: 0.566] [G acc: 0.078]\n",
      "2039 [D loss: 0.463(R 0.502, F0.424)] [D acc: 0.766(R 0.672, F 0.859)] [G loss: 0.463] [G acc: 0.109]\n",
      "2040 [D loss: 0.419(R 0.411, F0.427)] [D acc: 0.805(R 0.750, F 0.859)] [G loss: 0.419] [G acc: 0.172]\n",
      "2041 [D loss: 0.581(R 0.557, F0.605)] [D acc: 0.695(R 0.656, F 0.734)] [G loss: 0.581] [G acc: 0.172]\n",
      "2042 [D loss: 0.462(R 0.403, F0.521)] [D acc: 0.758(R 0.781, F 0.734)] [G loss: 0.462] [G acc: 0.031]\n",
      "2043 [D loss: 0.574(R 0.594, F0.555)] [D acc: 0.719(R 0.672, F 0.766)] [G loss: 0.574] [G acc: 0.078]\n",
      "2044 [D loss: 0.546(R 0.595, F0.497)] [D acc: 0.703(R 0.625, F 0.781)] [G loss: 0.546] [G acc: 0.125]\n",
      "2045 [D loss: 0.535(R 0.469, F0.601)] [D acc: 0.742(R 0.766, F 0.719)] [G loss: 0.535] [G acc: 0.062]\n",
      "2046 [D loss: 0.415(R 0.463, F0.368)] [D acc: 0.820(R 0.734, F 0.906)] [G loss: 0.415] [G acc: 0.047]\n",
      "2047 [D loss: 0.487(R 0.497, F0.478)] [D acc: 0.734(R 0.688, F 0.781)] [G loss: 0.487] [G acc: 0.078]\n",
      "2048 [D loss: 0.505(R 0.519, F0.490)] [D acc: 0.758(R 0.703, F 0.812)] [G loss: 0.505] [G acc: 0.125]\n",
      "2049 [D loss: 0.506(R 0.518, F0.494)] [D acc: 0.773(R 0.656, F 0.891)] [G loss: 0.506] [G acc: 0.188]\n",
      "2050 [D loss: 0.574(R 0.554, F0.595)] [D acc: 0.695(R 0.688, F 0.703)] [G loss: 0.574] [G acc: 0.141]\n",
      "2051 [D loss: 0.462(R 0.468, F0.456)] [D acc: 0.812(R 0.797, F 0.828)] [G loss: 0.462] [G acc: 0.078]\n",
      "2052 [D loss: 0.521(R 0.467, F0.575)] [D acc: 0.742(R 0.688, F 0.797)] [G loss: 0.521] [G acc: 0.141]\n",
      "2053 [D loss: 0.504(R 0.498, F0.510)] [D acc: 0.781(R 0.734, F 0.828)] [G loss: 0.504] [G acc: 0.109]\n",
      "2054 [D loss: 0.583(R 0.649, F0.516)] [D acc: 0.672(R 0.609, F 0.734)] [G loss: 0.583] [G acc: 0.203]\n",
      "2055 [D loss: 0.529(R 0.462, F0.597)] [D acc: 0.766(R 0.750, F 0.781)] [G loss: 0.529] [G acc: 0.172]\n",
      "2056 [D loss: 0.517(R 0.546, F0.488)] [D acc: 0.773(R 0.703, F 0.844)] [G loss: 0.517] [G acc: 0.078]\n",
      "2057 [D loss: 0.488(R 0.422, F0.554)] [D acc: 0.742(R 0.781, F 0.703)] [G loss: 0.488] [G acc: 0.094]\n",
      "2058 [D loss: 0.461(R 0.485, F0.437)] [D acc: 0.805(R 0.766, F 0.844)] [G loss: 0.461] [G acc: 0.109]\n",
      "2059 [D loss: 0.493(R 0.542, F0.444)] [D acc: 0.766(R 0.703, F 0.828)] [G loss: 0.493] [G acc: 0.125]\n",
      "2060 [D loss: 0.665(R 0.552, F0.779)] [D acc: 0.633(R 0.625, F 0.641)] [G loss: 0.665] [G acc: 0.094]\n",
      "2061 [D loss: 0.583(R 0.730, F0.437)] [D acc: 0.656(R 0.516, F 0.797)] [G loss: 0.583] [G acc: 0.188]\n",
      "2062 [D loss: 0.614(R 0.636, F0.593)] [D acc: 0.680(R 0.562, F 0.797)] [G loss: 0.614] [G acc: 0.109]\n",
      "2063 [D loss: 0.592(R 0.633, F0.550)] [D acc: 0.688(R 0.578, F 0.797)] [G loss: 0.592] [G acc: 0.109]\n",
      "2064 [D loss: 0.476(R 0.538, F0.415)] [D acc: 0.750(R 0.688, F 0.812)] [G loss: 0.476] [G acc: 0.078]\n",
      "2065 [D loss: 0.512(R 0.511, F0.513)] [D acc: 0.688(R 0.641, F 0.734)] [G loss: 0.512] [G acc: 0.141]\n",
      "2066 [D loss: 0.509(R 0.586, F0.431)] [D acc: 0.758(R 0.641, F 0.875)] [G loss: 0.509] [G acc: 0.156]\n",
      "2067 [D loss: 0.457(R 0.365, F0.549)] [D acc: 0.797(R 0.812, F 0.781)] [G loss: 0.457] [G acc: 0.062]\n",
      "2068 [D loss: 0.479(R 0.447, F0.512)] [D acc: 0.773(R 0.719, F 0.828)] [G loss: 0.479] [G acc: 0.094]\n",
      "2069 [D loss: 0.437(R 0.446, F0.428)] [D acc: 0.781(R 0.734, F 0.828)] [G loss: 0.437] [G acc: 0.094]\n",
      "2070 [D loss: 0.578(R 0.586, F0.570)] [D acc: 0.695(R 0.641, F 0.750)] [G loss: 0.578] [G acc: 0.047]\n",
      "2071 [D loss: 0.583(R 0.619, F0.547)] [D acc: 0.688(R 0.625, F 0.750)] [G loss: 0.583] [G acc: 0.125]\n",
      "2072 [D loss: 0.470(R 0.483, F0.457)] [D acc: 0.773(R 0.703, F 0.844)] [G loss: 0.470] [G acc: 0.031]\n",
      "2073 [D loss: 0.501(R 0.543, F0.458)] [D acc: 0.766(R 0.734, F 0.797)] [G loss: 0.501] [G acc: 0.156]\n",
      "2074 [D loss: 0.512(R 0.515, F0.510)] [D acc: 0.727(R 0.688, F 0.766)] [G loss: 0.512] [G acc: 0.094]\n",
      "2075 [D loss: 0.581(R 0.543, F0.619)] [D acc: 0.719(R 0.656, F 0.781)] [G loss: 0.581] [G acc: 0.156]\n",
      "2076 [D loss: 0.523(R 0.625, F0.420)] [D acc: 0.727(R 0.609, F 0.844)] [G loss: 0.523] [G acc: 0.125]\n",
      "2077 [D loss: 0.523(R 0.590, F0.456)] [D acc: 0.719(R 0.641, F 0.797)] [G loss: 0.523] [G acc: 0.109]\n",
      "2078 [D loss: 0.597(R 0.553, F0.641)] [D acc: 0.727(R 0.719, F 0.734)] [G loss: 0.597] [G acc: 0.109]\n",
      "2079 [D loss: 0.602(R 0.764, F0.439)] [D acc: 0.688(R 0.547, F 0.828)] [G loss: 0.602] [G acc: 0.125]\n",
      "2080 [D loss: 0.500(R 0.523, F0.476)] [D acc: 0.758(R 0.719, F 0.797)] [G loss: 0.500] [G acc: 0.078]\n",
      "2081 [D loss: 0.431(R 0.421, F0.440)] [D acc: 0.844(R 0.812, F 0.875)] [G loss: 0.431] [G acc: 0.125]\n",
      "2082 [D loss: 0.501(R 0.569, F0.433)] [D acc: 0.734(R 0.656, F 0.812)] [G loss: 0.501] [G acc: 0.109]\n",
      "2083 [D loss: 0.570(R 0.593, F0.546)] [D acc: 0.617(R 0.594, F 0.641)] [G loss: 0.570] [G acc: 0.094]\n",
      "2084 [D loss: 0.540(R 0.577, F0.503)] [D acc: 0.688(R 0.609, F 0.766)] [G loss: 0.540] [G acc: 0.062]\n",
      "2085 [D loss: 0.611(R 0.705, F0.516)] [D acc: 0.656(R 0.484, F 0.828)] [G loss: 0.611] [G acc: 0.109]\n",
      "2086 [D loss: 0.444(R 0.391, F0.498)] [D acc: 0.812(R 0.797, F 0.828)] [G loss: 0.444] [G acc: 0.109]\n",
      "2087 [D loss: 0.529(R 0.615, F0.443)] [D acc: 0.758(R 0.688, F 0.828)] [G loss: 0.529] [G acc: 0.234]\n",
      "2088 [D loss: 0.560(R 0.444, F0.675)] [D acc: 0.719(R 0.734, F 0.703)] [G loss: 0.560] [G acc: 0.078]\n",
      "2089 [D loss: 0.660(R 0.859, F0.461)] [D acc: 0.625(R 0.375, F 0.875)] [G loss: 0.660] [G acc: 0.062]\n",
      "2090 [D loss: 0.465(R 0.459, F0.471)] [D acc: 0.781(R 0.750, F 0.812)] [G loss: 0.465] [G acc: 0.156]\n",
      "2091 [D loss: 0.518(R 0.454, F0.582)] [D acc: 0.773(R 0.734, F 0.812)] [G loss: 0.518] [G acc: 0.062]\n",
      "2092 [D loss: 0.475(R 0.412, F0.538)] [D acc: 0.773(R 0.750, F 0.797)] [G loss: 0.475] [G acc: 0.156]\n",
      "2093 [D loss: 0.566(R 0.549, F0.584)] [D acc: 0.750(R 0.688, F 0.812)] [G loss: 0.566] [G acc: 0.062]\n",
      "2094 [D loss: 0.578(R 0.648, F0.507)] [D acc: 0.719(R 0.656, F 0.781)] [G loss: 0.578] [G acc: 0.125]\n",
      "2095 [D loss: 0.524(R 0.612, F0.435)] [D acc: 0.727(R 0.594, F 0.859)] [G loss: 0.524] [G acc: 0.109]\n",
      "2096 [D loss: 0.486(R 0.452, F0.521)] [D acc: 0.719(R 0.672, F 0.766)] [G loss: 0.486] [G acc: 0.094]\n",
      "2097 [D loss: 0.572(R 0.632, F0.511)] [D acc: 0.672(R 0.609, F 0.734)] [G loss: 0.572] [G acc: 0.141]\n",
      "2098 [D loss: 0.463(R 0.404, F0.523)] [D acc: 0.797(R 0.797, F 0.797)] [G loss: 0.463] [G acc: 0.156]\n",
      "2099 [D loss: 0.566(R 0.558, F0.573)] [D acc: 0.664(R 0.625, F 0.703)] [G loss: 0.566] [G acc: 0.094]\n",
      "INFO:tensorflow:Assets written to: ram://18a3a1f1-e73f-4ee7-80f7-cf975f634c40/assets\n",
      "INFO:tensorflow:Assets written to: ram://dd118637-030f-4e0e-bf9d-830997662f88/assets\n",
      "INFO:tensorflow:Assets written to: ram://f0096236-8c99-4a63-83ad-d5ef2fcbd858/assets\n",
      "2100 [D loss: 0.667(R 0.597, F0.738)] [D acc: 0.617(R 0.594, F 0.641)] [G loss: 0.667] [G acc: 0.109]\n",
      "2101 [D loss: 0.496(R 0.500, F0.491)] [D acc: 0.781(R 0.719, F 0.844)] [G loss: 0.496] [G acc: 0.141]\n",
      "2102 [D loss: 0.520(R 0.458, F0.581)] [D acc: 0.688(R 0.672, F 0.703)] [G loss: 0.520] [G acc: 0.031]\n",
      "2103 [D loss: 0.523(R 0.490, F0.556)] [D acc: 0.758(R 0.734, F 0.781)] [G loss: 0.523] [G acc: 0.156]\n",
      "2104 [D loss: 0.492(R 0.536, F0.448)] [D acc: 0.773(R 0.656, F 0.891)] [G loss: 0.492] [G acc: 0.062]\n",
      "2105 [D loss: 0.468(R 0.490, F0.447)] [D acc: 0.797(R 0.688, F 0.906)] [G loss: 0.468] [G acc: 0.062]\n",
      "2106 [D loss: 0.587(R 0.548, F0.625)] [D acc: 0.695(R 0.641, F 0.750)] [G loss: 0.587] [G acc: 0.125]\n",
      "2107 [D loss: 0.554(R 0.561, F0.548)] [D acc: 0.727(R 0.703, F 0.750)] [G loss: 0.554] [G acc: 0.047]\n",
      "2108 [D loss: 0.493(R 0.550, F0.435)] [D acc: 0.758(R 0.656, F 0.859)] [G loss: 0.493] [G acc: 0.094]\n",
      "2109 [D loss: 0.507(R 0.488, F0.526)] [D acc: 0.773(R 0.719, F 0.828)] [G loss: 0.507] [G acc: 0.062]\n",
      "2110 [D loss: 0.463(R 0.470, F0.456)] [D acc: 0.773(R 0.688, F 0.859)] [G loss: 0.463] [G acc: 0.094]\n",
      "2111 [D loss: 0.533(R 0.568, F0.497)] [D acc: 0.719(R 0.656, F 0.781)] [G loss: 0.533] [G acc: 0.109]\n",
      "2112 [D loss: 0.494(R 0.468, F0.519)] [D acc: 0.805(R 0.750, F 0.859)] [G loss: 0.494] [G acc: 0.156]\n",
      "2113 [D loss: 0.424(R 0.457, F0.391)] [D acc: 0.828(R 0.750, F 0.906)] [G loss: 0.424] [G acc: 0.078]\n",
      "2114 [D loss: 0.463(R 0.431, F0.495)] [D acc: 0.797(R 0.797, F 0.797)] [G loss: 0.463] [G acc: 0.094]\n",
      "2115 [D loss: 0.620(R 0.538, F0.701)] [D acc: 0.688(R 0.688, F 0.688)] [G loss: 0.620] [G acc: 0.062]\n",
      "2116 [D loss: 0.515(R 0.598, F0.432)] [D acc: 0.734(R 0.656, F 0.812)] [G loss: 0.515] [G acc: 0.109]\n",
      "2117 [D loss: 0.579(R 0.513, F0.646)] [D acc: 0.734(R 0.750, F 0.719)] [G loss: 0.579] [G acc: 0.156]\n",
      "2118 [D loss: 0.613(R 0.664, F0.562)] [D acc: 0.711(R 0.562, F 0.859)] [G loss: 0.613] [G acc: 0.094]\n",
      "2119 [D loss: 0.558(R 0.577, F0.539)] [D acc: 0.703(R 0.688, F 0.719)] [G loss: 0.558] [G acc: 0.062]\n",
      "2120 [D loss: 0.548(R 0.425, F0.672)] [D acc: 0.734(R 0.766, F 0.703)] [G loss: 0.548] [G acc: 0.031]\n",
      "2121 [D loss: 0.501(R 0.615, F0.387)] [D acc: 0.766(R 0.609, F 0.922)] [G loss: 0.501] [G acc: 0.094]\n",
      "2122 [D loss: 0.520(R 0.541, F0.500)] [D acc: 0.750(R 0.672, F 0.828)] [G loss: 0.520] [G acc: 0.062]\n",
      "2123 [D loss: 0.468(R 0.462, F0.475)] [D acc: 0.781(R 0.719, F 0.844)] [G loss: 0.468] [G acc: 0.094]\n",
      "2124 [D loss: 0.604(R 0.605, F0.602)] [D acc: 0.688(R 0.625, F 0.750)] [G loss: 0.604] [G acc: 0.094]\n",
      "2125 [D loss: 0.622(R 0.666, F0.578)] [D acc: 0.648(R 0.547, F 0.750)] [G loss: 0.622] [G acc: 0.125]\n",
      "2126 [D loss: 0.598(R 0.674, F0.522)] [D acc: 0.656(R 0.562, F 0.750)] [G loss: 0.598] [G acc: 0.078]\n",
      "2127 [D loss: 0.487(R 0.515, F0.459)] [D acc: 0.742(R 0.641, F 0.844)] [G loss: 0.487] [G acc: 0.125]\n",
      "2128 [D loss: 0.455(R 0.460, F0.450)] [D acc: 0.758(R 0.688, F 0.828)] [G loss: 0.455] [G acc: 0.047]\n",
      "2129 [D loss: 0.452(R 0.452, F0.452)] [D acc: 0.805(R 0.734, F 0.875)] [G loss: 0.452] [G acc: 0.031]\n",
      "2130 [D loss: 0.563(R 0.672, F0.454)] [D acc: 0.734(R 0.578, F 0.891)] [G loss: 0.563] [G acc: 0.125]\n",
      "2131 [D loss: 0.460(R 0.499, F0.421)] [D acc: 0.812(R 0.703, F 0.922)] [G loss: 0.460] [G acc: 0.047]\n",
      "2132 [D loss: 0.488(R 0.380, F0.596)] [D acc: 0.789(R 0.812, F 0.766)] [G loss: 0.488] [G acc: 0.094]\n",
      "2133 [D loss: 0.572(R 0.514, F0.629)] [D acc: 0.688(R 0.703, F 0.672)] [G loss: 0.572] [G acc: 0.031]\n",
      "2134 [D loss: 0.672(R 0.711, F0.633)] [D acc: 0.641(R 0.531, F 0.750)] [G loss: 0.672] [G acc: 0.078]\n",
      "2135 [D loss: 0.563(R 0.686, F0.441)] [D acc: 0.727(R 0.609, F 0.844)] [G loss: 0.563] [G acc: 0.094]\n",
      "2136 [D loss: 0.501(R 0.429, F0.573)] [D acc: 0.734(R 0.750, F 0.719)] [G loss: 0.501] [G acc: 0.094]\n",
      "2137 [D loss: 0.564(R 0.631, F0.498)] [D acc: 0.688(R 0.594, F 0.781)] [G loss: 0.564] [G acc: 0.047]\n",
      "2138 [D loss: 0.471(R 0.515, F0.427)] [D acc: 0.758(R 0.656, F 0.859)] [G loss: 0.471] [G acc: 0.156]\n",
      "2139 [D loss: 0.535(R 0.501, F0.569)] [D acc: 0.719(R 0.656, F 0.781)] [G loss: 0.535] [G acc: 0.094]\n",
      "2140 [D loss: 0.472(R 0.433, F0.511)] [D acc: 0.797(R 0.766, F 0.828)] [G loss: 0.472] [G acc: 0.078]\n",
      "2141 [D loss: 0.489(R 0.518, F0.460)] [D acc: 0.742(R 0.672, F 0.812)] [G loss: 0.489] [G acc: 0.047]\n",
      "2142 [D loss: 0.610(R 0.495, F0.724)] [D acc: 0.711(R 0.672, F 0.750)] [G loss: 0.610] [G acc: 0.141]\n",
      "2143 [D loss: 0.632(R 0.693, F0.570)] [D acc: 0.617(R 0.531, F 0.703)] [G loss: 0.632] [G acc: 0.094]\n",
      "2144 [D loss: 0.472(R 0.548, F0.396)] [D acc: 0.789(R 0.656, F 0.922)] [G loss: 0.472] [G acc: 0.125]\n",
      "2145 [D loss: 0.476(R 0.444, F0.507)] [D acc: 0.781(R 0.750, F 0.812)] [G loss: 0.476] [G acc: 0.094]\n",
      "2146 [D loss: 0.532(R 0.520, F0.545)] [D acc: 0.703(R 0.656, F 0.750)] [G loss: 0.532] [G acc: 0.047]\n",
      "2147 [D loss: 0.430(R 0.412, F0.448)] [D acc: 0.828(R 0.766, F 0.891)] [G loss: 0.430] [G acc: 0.188]\n",
      "2148 [D loss: 0.451(R 0.538, F0.364)] [D acc: 0.805(R 0.719, F 0.891)] [G loss: 0.451] [G acc: 0.047]\n",
      "2149 [D loss: 0.596(R 0.435, F0.757)] [D acc: 0.719(R 0.719, F 0.719)] [G loss: 0.596] [G acc: 0.062]\n",
      "2150 [D loss: 0.540(R 0.597, F0.483)] [D acc: 0.750(R 0.688, F 0.812)] [G loss: 0.540] [G acc: 0.047]\n",
      "2151 [D loss: 0.441(R 0.381, F0.502)] [D acc: 0.781(R 0.812, F 0.750)] [G loss: 0.441] [G acc: 0.125]\n",
      "2152 [D loss: 0.474(R 0.377, F0.571)] [D acc: 0.781(R 0.750, F 0.812)] [G loss: 0.474] [G acc: 0.031]\n",
      "2153 [D loss: 0.503(R 0.563, F0.443)] [D acc: 0.758(R 0.656, F 0.859)] [G loss: 0.503] [G acc: 0.109]\n",
      "2154 [D loss: 0.541(R 0.578, F0.504)] [D acc: 0.711(R 0.641, F 0.781)] [G loss: 0.541] [G acc: 0.094]\n",
      "2155 [D loss: 0.482(R 0.535, F0.430)] [D acc: 0.766(R 0.656, F 0.875)] [G loss: 0.482] [G acc: 0.094]\n",
      "2156 [D loss: 0.478(R 0.477, F0.480)] [D acc: 0.766(R 0.703, F 0.828)] [G loss: 0.478] [G acc: 0.047]\n",
      "2157 [D loss: 0.537(R 0.646, F0.428)] [D acc: 0.734(R 0.641, F 0.828)] [G loss: 0.537] [G acc: 0.078]\n",
      "2158 [D loss: 0.560(R 0.513, F0.607)] [D acc: 0.695(R 0.719, F 0.672)] [G loss: 0.560] [G acc: 0.109]\n",
      "2159 [D loss: 0.481(R 0.491, F0.470)] [D acc: 0.773(R 0.719, F 0.828)] [G loss: 0.481] [G acc: 0.062]\n",
      "2160 [D loss: 0.541(R 0.557, F0.525)] [D acc: 0.703(R 0.625, F 0.781)] [G loss: 0.541] [G acc: 0.094]\n",
      "2161 [D loss: 0.585(R 0.577, F0.594)] [D acc: 0.672(R 0.609, F 0.734)] [G loss: 0.585] [G acc: 0.047]\n",
      "2162 [D loss: 0.419(R 0.462, F0.376)] [D acc: 0.805(R 0.719, F 0.891)] [G loss: 0.419] [G acc: 0.078]\n",
      "2163 [D loss: 0.555(R 0.368, F0.743)] [D acc: 0.773(R 0.797, F 0.750)] [G loss: 0.555] [G acc: 0.047]\n",
      "2164 [D loss: 0.584(R 0.705, F0.462)] [D acc: 0.727(R 0.609, F 0.844)] [G loss: 0.584] [G acc: 0.109]\n",
      "2165 [D loss: 0.570(R 0.567, F0.572)] [D acc: 0.734(R 0.719, F 0.750)] [G loss: 0.570] [G acc: 0.047]\n",
      "2166 [D loss: 0.513(R 0.588, F0.438)] [D acc: 0.750(R 0.625, F 0.875)] [G loss: 0.513] [G acc: 0.172]\n",
      "2167 [D loss: 0.613(R 0.599, F0.626)] [D acc: 0.688(R 0.656, F 0.719)] [G loss: 0.613] [G acc: 0.109]\n",
      "2168 [D loss: 0.493(R 0.514, F0.471)] [D acc: 0.727(R 0.703, F 0.750)] [G loss: 0.493] [G acc: 0.062]\n",
      "2169 [D loss: 0.482(R 0.549, F0.414)] [D acc: 0.773(R 0.719, F 0.828)] [G loss: 0.482] [G acc: 0.141]\n",
      "2170 [D loss: 0.603(R 0.654, F0.552)] [D acc: 0.742(R 0.641, F 0.844)] [G loss: 0.603] [G acc: 0.125]\n",
      "2171 [D loss: 0.520(R 0.556, F0.484)] [D acc: 0.742(R 0.688, F 0.797)] [G loss: 0.520] [G acc: 0.062]\n",
      "2172 [D loss: 0.583(R 0.682, F0.485)] [D acc: 0.688(R 0.578, F 0.797)] [G loss: 0.583] [G acc: 0.109]\n",
      "2173 [D loss: 0.511(R 0.382, F0.640)] [D acc: 0.773(R 0.812, F 0.734)] [G loss: 0.511] [G acc: 0.031]\n",
      "2174 [D loss: 0.405(R 0.503, F0.307)] [D acc: 0.836(R 0.688, F 0.984)] [G loss: 0.405] [G acc: 0.109]\n",
      "2175 [D loss: 0.514(R 0.487, F0.541)] [D acc: 0.734(R 0.719, F 0.750)] [G loss: 0.514] [G acc: 0.062]\n",
      "2176 [D loss: 0.557(R 0.583, F0.531)] [D acc: 0.719(R 0.688, F 0.750)] [G loss: 0.557] [G acc: 0.188]\n",
      "2177 [D loss: 0.555(R 0.546, F0.563)] [D acc: 0.672(R 0.672, F 0.672)] [G loss: 0.555] [G acc: 0.109]\n",
      "2178 [D loss: 0.522(R 0.546, F0.497)] [D acc: 0.703(R 0.672, F 0.734)] [G loss: 0.522] [G acc: 0.156]\n",
      "2179 [D loss: 0.511(R 0.561, F0.460)] [D acc: 0.758(R 0.625, F 0.891)] [G loss: 0.511] [G acc: 0.016]\n",
      "2180 [D loss: 0.517(R 0.491, F0.542)] [D acc: 0.742(R 0.719, F 0.766)] [G loss: 0.517] [G acc: 0.016]\n",
      "2181 [D loss: 0.563(R 0.675, F0.450)] [D acc: 0.750(R 0.641, F 0.859)] [G loss: 0.563] [G acc: 0.141]\n",
      "2182 [D loss: 0.552(R 0.594, F0.509)] [D acc: 0.672(R 0.578, F 0.766)] [G loss: 0.552] [G acc: 0.062]\n",
      "2183 [D loss: 0.533(R 0.489, F0.577)] [D acc: 0.742(R 0.719, F 0.766)] [G loss: 0.533] [G acc: 0.141]\n",
      "2184 [D loss: 0.446(R 0.436, F0.455)] [D acc: 0.820(R 0.750, F 0.891)] [G loss: 0.446] [G acc: 0.078]\n",
      "2185 [D loss: 0.610(R 0.702, F0.518)] [D acc: 0.703(R 0.531, F 0.875)] [G loss: 0.610] [G acc: 0.047]\n",
      "2186 [D loss: 0.573(R 0.583, F0.563)] [D acc: 0.641(R 0.562, F 0.719)] [G loss: 0.573] [G acc: 0.109]\n",
      "2187 [D loss: 0.494(R 0.564, F0.424)] [D acc: 0.742(R 0.641, F 0.844)] [G loss: 0.494] [G acc: 0.047]\n",
      "2188 [D loss: 0.540(R 0.506, F0.574)] [D acc: 0.734(R 0.688, F 0.781)] [G loss: 0.540] [G acc: 0.109]\n",
      "2189 [D loss: 0.523(R 0.556, F0.489)] [D acc: 0.750(R 0.688, F 0.812)] [G loss: 0.523] [G acc: 0.094]\n",
      "2190 [D loss: 0.502(R 0.498, F0.506)] [D acc: 0.727(R 0.719, F 0.734)] [G loss: 0.502] [G acc: 0.125]\n",
      "2191 [D loss: 0.521(R 0.556, F0.486)] [D acc: 0.719(R 0.625, F 0.812)] [G loss: 0.521] [G acc: 0.125]\n",
      "2192 [D loss: 0.546(R 0.580, F0.513)] [D acc: 0.750(R 0.672, F 0.828)] [G loss: 0.546] [G acc: 0.125]\n",
      "2193 [D loss: 0.529(R 0.525, F0.534)] [D acc: 0.734(R 0.672, F 0.797)] [G loss: 0.529] [G acc: 0.078]\n",
      "2194 [D loss: 0.448(R 0.460, F0.437)] [D acc: 0.797(R 0.719, F 0.875)] [G loss: 0.448] [G acc: 0.094]\n",
      "2195 [D loss: 0.658(R 0.610, F0.707)] [D acc: 0.703(R 0.656, F 0.750)] [G loss: 0.658] [G acc: 0.094]\n",
      "2196 [D loss: 0.608(R 0.646, F0.570)] [D acc: 0.656(R 0.562, F 0.750)] [G loss: 0.608] [G acc: 0.062]\n",
      "2197 [D loss: 0.556(R 0.686, F0.426)] [D acc: 0.727(R 0.562, F 0.891)] [G loss: 0.556] [G acc: 0.125]\n",
      "2198 [D loss: 0.463(R 0.452, F0.473)] [D acc: 0.781(R 0.734, F 0.828)] [G loss: 0.463] [G acc: 0.109]\n",
      "2199 [D loss: 0.580(R 0.544, F0.616)] [D acc: 0.703(R 0.656, F 0.750)] [G loss: 0.580] [G acc: 0.031]\n",
      "INFO:tensorflow:Assets written to: ram://4854b4bf-2d5e-4b39-8428-af9114295267/assets\n",
      "INFO:tensorflow:Assets written to: ram://c5de6255-0a4f-4830-9365-8648c830e3d1/assets\n",
      "INFO:tensorflow:Assets written to: ram://c4556654-1220-41d8-b33a-e8814c925f02/assets\n",
      "2200 [D loss: 0.481(R 0.525, F0.437)] [D acc: 0.758(R 0.641, F 0.875)] [G loss: 0.481] [G acc: 0.094]\n",
      "2201 [D loss: 0.535(R 0.512, F0.557)] [D acc: 0.750(R 0.656, F 0.844)] [G loss: 0.535] [G acc: 0.156]\n",
      "2202 [D loss: 0.515(R 0.446, F0.584)] [D acc: 0.719(R 0.688, F 0.750)] [G loss: 0.515] [G acc: 0.047]\n",
      "2203 [D loss: 0.642(R 0.798, F0.485)] [D acc: 0.664(R 0.438, F 0.891)] [G loss: 0.642] [G acc: 0.062]\n",
      "2204 [D loss: 0.456(R 0.531, F0.381)] [D acc: 0.773(R 0.641, F 0.906)] [G loss: 0.456] [G acc: 0.141]\n",
      "2205 [D loss: 0.511(R 0.457, F0.565)] [D acc: 0.758(R 0.750, F 0.766)] [G loss: 0.511] [G acc: 0.188]\n",
      "2206 [D loss: 0.585(R 0.558, F0.611)] [D acc: 0.688(R 0.656, F 0.719)] [G loss: 0.585] [G acc: 0.156]\n",
      "2207 [D loss: 0.511(R 0.548, F0.474)] [D acc: 0.742(R 0.641, F 0.844)] [G loss: 0.511] [G acc: 0.109]\n",
      "2208 [D loss: 0.555(R 0.558, F0.551)] [D acc: 0.688(R 0.609, F 0.766)] [G loss: 0.555] [G acc: 0.031]\n",
      "2209 [D loss: 0.491(R 0.546, F0.436)] [D acc: 0.758(R 0.641, F 0.875)] [G loss: 0.491] [G acc: 0.109]\n",
      "2210 [D loss: 0.551(R 0.575, F0.527)] [D acc: 0.688(R 0.641, F 0.734)] [G loss: 0.551] [G acc: 0.078]\n",
      "2211 [D loss: 0.521(R 0.537, F0.504)] [D acc: 0.727(R 0.641, F 0.812)] [G loss: 0.521] [G acc: 0.141]\n",
      "2212 [D loss: 0.541(R 0.581, F0.502)] [D acc: 0.742(R 0.641, F 0.844)] [G loss: 0.541] [G acc: 0.141]\n",
      "2213 [D loss: 0.536(R 0.484, F0.589)] [D acc: 0.688(R 0.688, F 0.688)] [G loss: 0.536] [G acc: 0.156]\n",
      "2214 [D loss: 0.578(R 0.588, F0.568)] [D acc: 0.688(R 0.625, F 0.750)] [G loss: 0.578] [G acc: 0.125]\n",
      "2215 [D loss: 0.565(R 0.554, F0.577)] [D acc: 0.719(R 0.672, F 0.766)] [G loss: 0.565] [G acc: 0.078]\n",
      "2216 [D loss: 0.630(R 0.653, F0.607)] [D acc: 0.648(R 0.578, F 0.719)] [G loss: 0.630] [G acc: 0.062]\n",
      "2217 [D loss: 0.620(R 0.652, F0.589)] [D acc: 0.688(R 0.578, F 0.797)] [G loss: 0.620] [G acc: 0.109]\n",
      "2218 [D loss: 0.567(R 0.537, F0.597)] [D acc: 0.734(R 0.688, F 0.781)] [G loss: 0.567] [G acc: 0.109]\n",
      "2219 [D loss: 0.480(R 0.546, F0.415)] [D acc: 0.773(R 0.688, F 0.859)] [G loss: 0.480] [G acc: 0.125]\n",
      "2220 [D loss: 0.501(R 0.476, F0.526)] [D acc: 0.742(R 0.703, F 0.781)] [G loss: 0.501] [G acc: 0.094]\n",
      "2221 [D loss: 0.494(R 0.592, F0.395)] [D acc: 0.758(R 0.625, F 0.891)] [G loss: 0.494] [G acc: 0.078]\n",
      "2222 [D loss: 0.546(R 0.592, F0.499)] [D acc: 0.695(R 0.641, F 0.750)] [G loss: 0.546] [G acc: 0.078]\n",
      "2223 [D loss: 0.588(R 0.596, F0.581)] [D acc: 0.711(R 0.625, F 0.797)] [G loss: 0.588] [G acc: 0.078]\n",
      "2224 [D loss: 0.520(R 0.498, F0.542)] [D acc: 0.719(R 0.656, F 0.781)] [G loss: 0.520] [G acc: 0.062]\n",
      "2225 [D loss: 0.495(R 0.527, F0.464)] [D acc: 0.758(R 0.672, F 0.844)] [G loss: 0.495] [G acc: 0.109]\n",
      "2226 [D loss: 0.535(R 0.470, F0.601)] [D acc: 0.781(R 0.781, F 0.781)] [G loss: 0.535] [G acc: 0.078]\n",
      "2227 [D loss: 0.558(R 0.534, F0.581)] [D acc: 0.711(R 0.656, F 0.766)] [G loss: 0.558] [G acc: 0.047]\n",
      "2228 [D loss: 0.531(R 0.614, F0.449)] [D acc: 0.711(R 0.594, F 0.828)] [G loss: 0.531] [G acc: 0.109]\n",
      "2229 [D loss: 0.550(R 0.529, F0.571)] [D acc: 0.695(R 0.641, F 0.750)] [G loss: 0.550] [G acc: 0.078]\n",
      "2230 [D loss: 0.547(R 0.609, F0.486)] [D acc: 0.805(R 0.719, F 0.891)] [G loss: 0.547] [G acc: 0.078]\n",
      "2231 [D loss: 0.514(R 0.474, F0.553)] [D acc: 0.727(R 0.672, F 0.781)] [G loss: 0.514] [G acc: 0.141]\n",
      "2232 [D loss: 0.513(R 0.449, F0.577)] [D acc: 0.750(R 0.734, F 0.766)] [G loss: 0.513] [G acc: 0.156]\n",
      "2233 [D loss: 0.561(R 0.579, F0.543)] [D acc: 0.703(R 0.625, F 0.781)] [G loss: 0.561] [G acc: 0.125]\n",
      "2234 [D loss: 0.521(R 0.538, F0.503)] [D acc: 0.734(R 0.672, F 0.797)] [G loss: 0.521] [G acc: 0.109]\n",
      "2235 [D loss: 0.567(R 0.478, F0.655)] [D acc: 0.703(R 0.688, F 0.719)] [G loss: 0.567] [G acc: 0.078]\n",
      "2236 [D loss: 0.545(R 0.571, F0.519)] [D acc: 0.719(R 0.672, F 0.766)] [G loss: 0.545] [G acc: 0.094]\n",
      "2237 [D loss: 0.615(R 0.633, F0.597)] [D acc: 0.688(R 0.594, F 0.781)] [G loss: 0.615] [G acc: 0.078]\n",
      "2238 [D loss: 0.533(R 0.571, F0.495)] [D acc: 0.680(R 0.547, F 0.812)] [G loss: 0.533] [G acc: 0.156]\n",
      "2239 [D loss: 0.617(R 0.539, F0.696)] [D acc: 0.672(R 0.625, F 0.719)] [G loss: 0.617] [G acc: 0.078]\n",
      "2240 [D loss: 0.514(R 0.543, F0.485)] [D acc: 0.703(R 0.594, F 0.812)] [G loss: 0.514] [G acc: 0.078]\n",
      "2241 [D loss: 0.539(R 0.629, F0.448)] [D acc: 0.703(R 0.562, F 0.844)] [G loss: 0.539] [G acc: 0.047]\n",
      "2242 [D loss: 0.584(R 0.614, F0.553)] [D acc: 0.719(R 0.609, F 0.828)] [G loss: 0.584] [G acc: 0.094]\n",
      "2243 [D loss: 0.597(R 0.554, F0.641)] [D acc: 0.711(R 0.641, F 0.781)] [G loss: 0.597] [G acc: 0.109]\n",
      "2244 [D loss: 0.563(R 0.575, F0.551)] [D acc: 0.711(R 0.672, F 0.750)] [G loss: 0.563] [G acc: 0.016]\n",
      "2245 [D loss: 0.504(R 0.569, F0.440)] [D acc: 0.742(R 0.641, F 0.844)] [G loss: 0.504] [G acc: 0.094]\n",
      "2246 [D loss: 0.507(R 0.538, F0.477)] [D acc: 0.766(R 0.672, F 0.859)] [G loss: 0.507] [G acc: 0.062]\n",
      "2247 [D loss: 0.458(R 0.459, F0.457)] [D acc: 0.781(R 0.734, F 0.828)] [G loss: 0.458] [G acc: 0.016]\n",
      "2248 [D loss: 0.487(R 0.510, F0.465)] [D acc: 0.734(R 0.688, F 0.781)] [G loss: 0.487] [G acc: 0.031]\n",
      "2249 [D loss: 0.441(R 0.358, F0.524)] [D acc: 0.789(R 0.781, F 0.797)] [G loss: 0.441] [G acc: 0.141]\n",
      "2250 [D loss: 0.487(R 0.536, F0.438)] [D acc: 0.781(R 0.703, F 0.859)] [G loss: 0.487] [G acc: 0.141]\n",
      "2251 [D loss: 0.634(R 0.529, F0.739)] [D acc: 0.703(R 0.719, F 0.688)] [G loss: 0.634] [G acc: 0.094]\n",
      "2252 [D loss: 0.482(R 0.550, F0.414)] [D acc: 0.766(R 0.625, F 0.906)] [G loss: 0.482] [G acc: 0.047]\n",
      "2253 [D loss: 0.540(R 0.597, F0.482)] [D acc: 0.695(R 0.594, F 0.797)] [G loss: 0.540] [G acc: 0.078]\n",
      "2254 [D loss: 0.475(R 0.453, F0.496)] [D acc: 0.742(R 0.688, F 0.797)] [G loss: 0.475] [G acc: 0.062]\n",
      "2255 [D loss: 0.601(R 0.581, F0.620)] [D acc: 0.695(R 0.609, F 0.781)] [G loss: 0.601] [G acc: 0.125]\n",
      "2256 [D loss: 0.548(R 0.630, F0.467)] [D acc: 0.703(R 0.547, F 0.859)] [G loss: 0.548] [G acc: 0.016]\n",
      "2257 [D loss: 0.578(R 0.687, F0.469)] [D acc: 0.742(R 0.547, F 0.938)] [G loss: 0.578] [G acc: 0.047]\n",
      "2258 [D loss: 0.538(R 0.561, F0.514)] [D acc: 0.688(R 0.594, F 0.781)] [G loss: 0.538] [G acc: 0.062]\n",
      "2259 [D loss: 0.502(R 0.489, F0.515)] [D acc: 0.789(R 0.734, F 0.844)] [G loss: 0.502] [G acc: 0.141]\n",
      "2260 [D loss: 0.424(R 0.485, F0.363)] [D acc: 0.820(R 0.719, F 0.922)] [G loss: 0.424] [G acc: 0.094]\n",
      "2261 [D loss: 0.596(R 0.510, F0.682)] [D acc: 0.695(R 0.688, F 0.703)] [G loss: 0.596] [G acc: 0.062]\n",
      "2262 [D loss: 0.575(R 0.631, F0.518)] [D acc: 0.703(R 0.578, F 0.828)] [G loss: 0.575] [G acc: 0.062]\n",
      "2263 [D loss: 0.488(R 0.514, F0.461)] [D acc: 0.789(R 0.703, F 0.875)] [G loss: 0.488] [G acc: 0.078]\n",
      "2264 [D loss: 0.561(R 0.594, F0.527)] [D acc: 0.750(R 0.641, F 0.859)] [G loss: 0.561] [G acc: 0.078]\n",
      "2265 [D loss: 0.619(R 0.671, F0.567)] [D acc: 0.656(R 0.547, F 0.766)] [G loss: 0.619] [G acc: 0.062]\n",
      "2266 [D loss: 0.613(R 0.747, F0.479)] [D acc: 0.672(R 0.531, F 0.812)] [G loss: 0.613] [G acc: 0.094]\n",
      "2267 [D loss: 0.467(R 0.513, F0.420)] [D acc: 0.781(R 0.688, F 0.875)] [G loss: 0.467] [G acc: 0.188]\n",
      "2268 [D loss: 0.521(R 0.478, F0.564)] [D acc: 0.750(R 0.734, F 0.766)] [G loss: 0.521] [G acc: 0.141]\n",
      "2269 [D loss: 0.598(R 0.699, F0.497)] [D acc: 0.695(R 0.578, F 0.812)] [G loss: 0.598] [G acc: 0.188]\n",
      "2270 [D loss: 0.527(R 0.521, F0.534)] [D acc: 0.703(R 0.625, F 0.781)] [G loss: 0.527] [G acc: 0.062]\n",
      "2271 [D loss: 0.504(R 0.470, F0.537)] [D acc: 0.758(R 0.750, F 0.766)] [G loss: 0.504] [G acc: 0.078]\n",
      "2272 [D loss: 0.514(R 0.453, F0.576)] [D acc: 0.758(R 0.734, F 0.781)] [G loss: 0.514] [G acc: 0.109]\n",
      "2273 [D loss: 0.572(R 0.623, F0.522)] [D acc: 0.711(R 0.562, F 0.859)] [G loss: 0.572] [G acc: 0.125]\n",
      "2274 [D loss: 0.474(R 0.457, F0.491)] [D acc: 0.750(R 0.688, F 0.812)] [G loss: 0.474] [G acc: 0.062]\n",
      "2275 [D loss: 0.463(R 0.560, F0.365)] [D acc: 0.812(R 0.719, F 0.906)] [G loss: 0.463] [G acc: 0.062]\n",
      "2276 [D loss: 0.498(R 0.458, F0.538)] [D acc: 0.742(R 0.719, F 0.766)] [G loss: 0.498] [G acc: 0.109]\n",
      "2277 [D loss: 0.522(R 0.566, F0.478)] [D acc: 0.797(R 0.734, F 0.859)] [G loss: 0.522] [G acc: 0.078]\n",
      "2278 [D loss: 0.618(R 0.694, F0.542)] [D acc: 0.703(R 0.578, F 0.828)] [G loss: 0.618] [G acc: 0.047]\n",
      "2279 [D loss: 0.560(R 0.521, F0.598)] [D acc: 0.695(R 0.719, F 0.672)] [G loss: 0.560] [G acc: 0.109]\n",
      "2280 [D loss: 0.472(R 0.488, F0.455)] [D acc: 0.766(R 0.656, F 0.875)] [G loss: 0.472] [G acc: 0.094]\n",
      "2281 [D loss: 0.554(R 0.584, F0.525)] [D acc: 0.719(R 0.594, F 0.844)] [G loss: 0.554] [G acc: 0.125]\n",
      "2282 [D loss: 0.558(R 0.631, F0.484)] [D acc: 0.711(R 0.609, F 0.812)] [G loss: 0.558] [G acc: 0.188]\n",
      "2283 [D loss: 0.495(R 0.488, F0.502)] [D acc: 0.773(R 0.703, F 0.844)] [G loss: 0.495] [G acc: 0.062]\n",
      "2284 [D loss: 0.627(R 0.732, F0.522)] [D acc: 0.656(R 0.531, F 0.781)] [G loss: 0.627] [G acc: 0.125]\n",
      "2285 [D loss: 0.682(R 0.530, F0.835)] [D acc: 0.695(R 0.609, F 0.781)] [G loss: 0.682] [G acc: 0.094]\n",
      "2286 [D loss: 0.525(R 0.575, F0.476)] [D acc: 0.727(R 0.656, F 0.797)] [G loss: 0.525] [G acc: 0.156]\n",
      "2287 [D loss: 0.437(R 0.387, F0.487)] [D acc: 0.828(R 0.812, F 0.844)] [G loss: 0.437] [G acc: 0.125]\n",
      "2288 [D loss: 0.417(R 0.394, F0.441)] [D acc: 0.789(R 0.719, F 0.859)] [G loss: 0.417] [G acc: 0.125]\n",
      "2289 [D loss: 0.547(R 0.542, F0.553)] [D acc: 0.703(R 0.672, F 0.734)] [G loss: 0.547] [G acc: 0.125]\n",
      "2290 [D loss: 0.455(R 0.406, F0.505)] [D acc: 0.781(R 0.750, F 0.812)] [G loss: 0.455] [G acc: 0.156]\n",
      "2291 [D loss: 0.501(R 0.570, F0.431)] [D acc: 0.734(R 0.609, F 0.859)] [G loss: 0.501] [G acc: 0.078]\n",
      "2292 [D loss: 0.502(R 0.528, F0.477)] [D acc: 0.758(R 0.688, F 0.828)] [G loss: 0.502] [G acc: 0.188]\n",
      "2293 [D loss: 0.493(R 0.485, F0.502)] [D acc: 0.758(R 0.750, F 0.766)] [G loss: 0.493] [G acc: 0.062]\n",
      "2294 [D loss: 0.483(R 0.542, F0.423)] [D acc: 0.758(R 0.578, F 0.938)] [G loss: 0.483] [G acc: 0.062]\n",
      "2295 [D loss: 0.443(R 0.376, F0.511)] [D acc: 0.828(R 0.875, F 0.781)] [G loss: 0.443] [G acc: 0.125]\n",
      "2296 [D loss: 0.504(R 0.464, F0.544)] [D acc: 0.773(R 0.750, F 0.797)] [G loss: 0.504] [G acc: 0.094]\n",
      "2297 [D loss: 0.571(R 0.636, F0.506)] [D acc: 0.672(R 0.594, F 0.750)] [G loss: 0.571] [G acc: 0.016]\n",
      "2298 [D loss: 0.540(R 0.622, F0.459)] [D acc: 0.734(R 0.641, F 0.828)] [G loss: 0.540] [G acc: 0.156]\n",
      "2299 [D loss: 0.526(R 0.528, F0.524)] [D acc: 0.703(R 0.656, F 0.750)] [G loss: 0.526] [G acc: 0.047]\n",
      "INFO:tensorflow:Assets written to: ram://a231177f-0205-4306-9776-095b88aaaaa7/assets\n",
      "INFO:tensorflow:Assets written to: ram://40e06650-030f-4f5f-aec1-b5ee87966fd6/assets\n",
      "INFO:tensorflow:Assets written to: ram://9d343aff-badd-4c8c-9e68-e9455495a7ef/assets\n",
      "2300 [D loss: 0.470(R 0.445, F0.494)] [D acc: 0.773(R 0.734, F 0.812)] [G loss: 0.470] [G acc: 0.094]\n",
      "2301 [D loss: 0.549(R 0.639, F0.459)] [D acc: 0.703(R 0.609, F 0.797)] [G loss: 0.549] [G acc: 0.078]\n",
      "2302 [D loss: 0.481(R 0.522, F0.440)] [D acc: 0.758(R 0.672, F 0.844)] [G loss: 0.481] [G acc: 0.078]\n",
      "2303 [D loss: 0.517(R 0.492, F0.541)] [D acc: 0.711(R 0.688, F 0.734)] [G loss: 0.517] [G acc: 0.078]\n",
      "2304 [D loss: 0.493(R 0.527, F0.459)] [D acc: 0.734(R 0.703, F 0.766)] [G loss: 0.493] [G acc: 0.094]\n",
      "2305 [D loss: 0.509(R 0.564, F0.455)] [D acc: 0.750(R 0.641, F 0.859)] [G loss: 0.509] [G acc: 0.078]\n",
      "2306 [D loss: 0.536(R 0.610, F0.461)] [D acc: 0.742(R 0.656, F 0.828)] [G loss: 0.536] [G acc: 0.078]\n",
      "2307 [D loss: 0.520(R 0.504, F0.536)] [D acc: 0.781(R 0.734, F 0.828)] [G loss: 0.520] [G acc: 0.078]\n",
      "2308 [D loss: 0.517(R 0.594, F0.440)] [D acc: 0.711(R 0.594, F 0.828)] [G loss: 0.517] [G acc: 0.047]\n",
      "2309 [D loss: 0.456(R 0.463, F0.449)] [D acc: 0.773(R 0.719, F 0.828)] [G loss: 0.456] [G acc: 0.109]\n",
      "2310 [D loss: 0.538(R 0.523, F0.553)] [D acc: 0.688(R 0.625, F 0.750)] [G loss: 0.538] [G acc: 0.062]\n",
      "2311 [D loss: 0.488(R 0.536, F0.441)] [D acc: 0.750(R 0.672, F 0.828)] [G loss: 0.488] [G acc: 0.234]\n",
      "2312 [D loss: 0.504(R 0.484, F0.525)] [D acc: 0.734(R 0.688, F 0.781)] [G loss: 0.504] [G acc: 0.094]\n",
      "2313 [D loss: 0.579(R 0.681, F0.478)] [D acc: 0.734(R 0.594, F 0.875)] [G loss: 0.579] [G acc: 0.078]\n",
      "2314 [D loss: 0.563(R 0.610, F0.516)] [D acc: 0.695(R 0.594, F 0.797)] [G loss: 0.563] [G acc: 0.062]\n",
      "2315 [D loss: 0.636(R 0.551, F0.722)] [D acc: 0.688(R 0.719, F 0.656)] [G loss: 0.636] [G acc: 0.109]\n",
      "2316 [D loss: 0.550(R 0.538, F0.562)] [D acc: 0.727(R 0.656, F 0.797)] [G loss: 0.550] [G acc: 0.094]\n",
      "2317 [D loss: 0.507(R 0.604, F0.411)] [D acc: 0.719(R 0.562, F 0.875)] [G loss: 0.507] [G acc: 0.094]\n",
      "2318 [D loss: 0.537(R 0.582, F0.492)] [D acc: 0.719(R 0.641, F 0.797)] [G loss: 0.537] [G acc: 0.078]\n",
      "2319 [D loss: 0.523(R 0.576, F0.470)] [D acc: 0.742(R 0.672, F 0.812)] [G loss: 0.523] [G acc: 0.141]\n",
      "2320 [D loss: 0.610(R 0.574, F0.645)] [D acc: 0.641(R 0.594, F 0.688)] [G loss: 0.610] [G acc: 0.109]\n",
      "2321 [D loss: 0.503(R 0.555, F0.451)] [D acc: 0.828(R 0.766, F 0.891)] [G loss: 0.503] [G acc: 0.062]\n",
      "2322 [D loss: 0.498(R 0.498, F0.498)] [D acc: 0.742(R 0.672, F 0.812)] [G loss: 0.498] [G acc: 0.125]\n",
      "2323 [D loss: 0.467(R 0.458, F0.476)] [D acc: 0.742(R 0.703, F 0.781)] [G loss: 0.467] [G acc: 0.156]\n",
      "2324 [D loss: 0.525(R 0.448, F0.602)] [D acc: 0.719(R 0.750, F 0.688)] [G loss: 0.525] [G acc: 0.109]\n",
      "2325 [D loss: 0.565(R 0.685, F0.446)] [D acc: 0.695(R 0.562, F 0.828)] [G loss: 0.565] [G acc: 0.172]\n",
      "2326 [D loss: 0.522(R 0.495, F0.549)] [D acc: 0.688(R 0.656, F 0.719)] [G loss: 0.522] [G acc: 0.094]\n",
      "2327 [D loss: 0.543(R 0.584, F0.503)] [D acc: 0.719(R 0.625, F 0.812)] [G loss: 0.543] [G acc: 0.156]\n",
      "2328 [D loss: 0.542(R 0.541, F0.543)] [D acc: 0.758(R 0.703, F 0.812)] [G loss: 0.542] [G acc: 0.125]\n",
      "2329 [D loss: 0.633(R 0.669, F0.597)] [D acc: 0.672(R 0.562, F 0.781)] [G loss: 0.633] [G acc: 0.094]\n",
      "2330 [D loss: 0.610(R 0.575, F0.645)] [D acc: 0.719(R 0.734, F 0.703)] [G loss: 0.610] [G acc: 0.062]\n",
      "2331 [D loss: 0.543(R 0.495, F0.592)] [D acc: 0.750(R 0.719, F 0.781)] [G loss: 0.543] [G acc: 0.062]\n",
      "2332 [D loss: 0.474(R 0.462, F0.485)] [D acc: 0.812(R 0.766, F 0.859)] [G loss: 0.474] [G acc: 0.047]\n",
      "2333 [D loss: 0.588(R 0.636, F0.539)] [D acc: 0.664(R 0.562, F 0.766)] [G loss: 0.588] [G acc: 0.109]\n",
      "2334 [D loss: 0.563(R 0.622, F0.504)] [D acc: 0.695(R 0.656, F 0.734)] [G loss: 0.563] [G acc: 0.094]\n",
      "2335 [D loss: 0.644(R 0.706, F0.583)] [D acc: 0.625(R 0.516, F 0.734)] [G loss: 0.644] [G acc: 0.141]\n",
      "2336 [D loss: 0.606(R 0.696, F0.516)] [D acc: 0.695(R 0.625, F 0.766)] [G loss: 0.606] [G acc: 0.109]\n",
      "2337 [D loss: 0.527(R 0.590, F0.464)] [D acc: 0.703(R 0.609, F 0.797)] [G loss: 0.527] [G acc: 0.109]\n",
      "2338 [D loss: 0.461(R 0.448, F0.473)] [D acc: 0.805(R 0.750, F 0.859)] [G loss: 0.461] [G acc: 0.141]\n",
      "2339 [D loss: 0.624(R 0.521, F0.726)] [D acc: 0.633(R 0.625, F 0.641)] [G loss: 0.624] [G acc: 0.062]\n",
      "2340 [D loss: 0.603(R 0.664, F0.542)] [D acc: 0.641(R 0.578, F 0.703)] [G loss: 0.603] [G acc: 0.203]\n",
      "2341 [D loss: 0.489(R 0.459, F0.518)] [D acc: 0.812(R 0.750, F 0.875)] [G loss: 0.489] [G acc: 0.156]\n",
      "2342 [D loss: 0.591(R 0.667, F0.514)] [D acc: 0.664(R 0.547, F 0.781)] [G loss: 0.591] [G acc: 0.062]\n",
      "2343 [D loss: 0.444(R 0.420, F0.468)] [D acc: 0.805(R 0.734, F 0.875)] [G loss: 0.444] [G acc: 0.062]\n",
      "2344 [D loss: 0.609(R 0.569, F0.649)] [D acc: 0.664(R 0.641, F 0.688)] [G loss: 0.609] [G acc: 0.109]\n",
      "2345 [D loss: 0.521(R 0.599, F0.443)] [D acc: 0.719(R 0.578, F 0.859)] [G loss: 0.521] [G acc: 0.141]\n",
      "2346 [D loss: 0.482(R 0.432, F0.531)] [D acc: 0.781(R 0.797, F 0.766)] [G loss: 0.482] [G acc: 0.141]\n",
      "2347 [D loss: 0.514(R 0.524, F0.505)] [D acc: 0.719(R 0.688, F 0.750)] [G loss: 0.514] [G acc: 0.109]\n",
      "2348 [D loss: 0.518(R 0.522, F0.514)] [D acc: 0.727(R 0.672, F 0.781)] [G loss: 0.518] [G acc: 0.141]\n",
      "2349 [D loss: 0.451(R 0.457, F0.445)] [D acc: 0.836(R 0.766, F 0.906)] [G loss: 0.451] [G acc: 0.188]\n",
      "2350 [D loss: 0.522(R 0.495, F0.548)] [D acc: 0.758(R 0.734, F 0.781)] [G loss: 0.522] [G acc: 0.109]\n",
      "2351 [D loss: 0.517(R 0.498, F0.535)] [D acc: 0.734(R 0.703, F 0.766)] [G loss: 0.517] [G acc: 0.031]\n",
      "2352 [D loss: 0.734(R 0.868, F0.600)] [D acc: 0.586(R 0.469, F 0.703)] [G loss: 0.734] [G acc: 0.188]\n",
      "2353 [D loss: 0.511(R 0.540, F0.481)] [D acc: 0.750(R 0.641, F 0.859)] [G loss: 0.511] [G acc: 0.125]\n",
      "2354 [D loss: 0.608(R 0.540, F0.677)] [D acc: 0.711(R 0.672, F 0.750)] [G loss: 0.608] [G acc: 0.078]\n",
      "2355 [D loss: 0.485(R 0.473, F0.497)] [D acc: 0.750(R 0.688, F 0.812)] [G loss: 0.485] [G acc: 0.109]\n",
      "2356 [D loss: 0.486(R 0.331, F0.642)] [D acc: 0.789(R 0.844, F 0.734)] [G loss: 0.486] [G acc: 0.188]\n",
      "2357 [D loss: 0.468(R 0.442, F0.495)] [D acc: 0.812(R 0.750, F 0.875)] [G loss: 0.468] [G acc: 0.172]\n",
      "2358 [D loss: 0.570(R 0.513, F0.626)] [D acc: 0.734(R 0.688, F 0.781)] [G loss: 0.570] [G acc: 0.062]\n",
      "2359 [D loss: 0.463(R 0.431, F0.496)] [D acc: 0.773(R 0.703, F 0.844)] [G loss: 0.463] [G acc: 0.062]\n",
      "2360 [D loss: 0.633(R 0.629, F0.637)] [D acc: 0.688(R 0.578, F 0.797)] [G loss: 0.633] [G acc: 0.047]\n",
      "2361 [D loss: 0.492(R 0.538, F0.447)] [D acc: 0.781(R 0.656, F 0.906)] [G loss: 0.492] [G acc: 0.141]\n",
      "2362 [D loss: 0.486(R 0.514, F0.459)] [D acc: 0.766(R 0.719, F 0.812)] [G loss: 0.486] [G acc: 0.109]\n",
      "2363 [D loss: 0.627(R 0.568, F0.687)] [D acc: 0.641(R 0.625, F 0.656)] [G loss: 0.627] [G acc: 0.078]\n",
      "2364 [D loss: 0.570(R 0.502, F0.638)] [D acc: 0.719(R 0.703, F 0.734)] [G loss: 0.570] [G acc: 0.078]\n",
      "2365 [D loss: 0.509(R 0.581, F0.438)] [D acc: 0.797(R 0.672, F 0.922)] [G loss: 0.509] [G acc: 0.109]\n",
      "2366 [D loss: 0.558(R 0.568, F0.548)] [D acc: 0.695(R 0.609, F 0.781)] [G loss: 0.558] [G acc: 0.078]\n",
      "2367 [D loss: 0.519(R 0.572, F0.466)] [D acc: 0.750(R 0.656, F 0.844)] [G loss: 0.519] [G acc: 0.031]\n",
      "2368 [D loss: 0.469(R 0.519, F0.418)] [D acc: 0.805(R 0.719, F 0.891)] [G loss: 0.469] [G acc: 0.047]\n",
      "2369 [D loss: 0.439(R 0.453, F0.424)] [D acc: 0.773(R 0.703, F 0.844)] [G loss: 0.439] [G acc: 0.109]\n",
      "2370 [D loss: 0.601(R 0.425, F0.776)] [D acc: 0.750(R 0.766, F 0.734)] [G loss: 0.601] [G acc: 0.062]\n",
      "2371 [D loss: 0.591(R 0.710, F0.472)] [D acc: 0.664(R 0.547, F 0.781)] [G loss: 0.591] [G acc: 0.109]\n",
      "2372 [D loss: 0.539(R 0.545, F0.534)] [D acc: 0.750(R 0.703, F 0.797)] [G loss: 0.539] [G acc: 0.078]\n",
      "2373 [D loss: 0.612(R 0.686, F0.538)] [D acc: 0.664(R 0.531, F 0.797)] [G loss: 0.612] [G acc: 0.078]\n",
      "2374 [D loss: 0.543(R 0.597, F0.488)] [D acc: 0.719(R 0.625, F 0.812)] [G loss: 0.543] [G acc: 0.141]\n",
      "2375 [D loss: 0.538(R 0.603, F0.473)] [D acc: 0.750(R 0.703, F 0.797)] [G loss: 0.538] [G acc: 0.141]\n",
      "2376 [D loss: 0.630(R 0.549, F0.710)] [D acc: 0.711(R 0.703, F 0.719)] [G loss: 0.630] [G acc: 0.078]\n",
      "2377 [D loss: 0.607(R 0.696, F0.519)] [D acc: 0.734(R 0.578, F 0.891)] [G loss: 0.607] [G acc: 0.156]\n",
      "2378 [D loss: 0.573(R 0.654, F0.492)] [D acc: 0.750(R 0.625, F 0.875)] [G loss: 0.573] [G acc: 0.141]\n",
      "2379 [D loss: 0.505(R 0.476, F0.533)] [D acc: 0.750(R 0.703, F 0.797)] [G loss: 0.505] [G acc: 0.125]\n",
      "2380 [D loss: 0.465(R 0.489, F0.442)] [D acc: 0.781(R 0.734, F 0.828)] [G loss: 0.465] [G acc: 0.125]\n",
      "2381 [D loss: 0.574(R 0.585, F0.564)] [D acc: 0.688(R 0.609, F 0.766)] [G loss: 0.574] [G acc: 0.109]\n",
      "2382 [D loss: 0.475(R 0.431, F0.519)] [D acc: 0.773(R 0.766, F 0.781)] [G loss: 0.475] [G acc: 0.094]\n",
      "2383 [D loss: 0.474(R 0.530, F0.418)] [D acc: 0.789(R 0.656, F 0.922)] [G loss: 0.474] [G acc: 0.031]\n",
      "2384 [D loss: 0.490(R 0.358, F0.622)] [D acc: 0.797(R 0.828, F 0.766)] [G loss: 0.490] [G acc: 0.094]\n",
      "2385 [D loss: 0.527(R 0.578, F0.475)] [D acc: 0.727(R 0.656, F 0.797)] [G loss: 0.527] [G acc: 0.141]\n",
      "2386 [D loss: 0.578(R 0.654, F0.503)] [D acc: 0.719(R 0.594, F 0.844)] [G loss: 0.578] [G acc: 0.094]\n",
      "2387 [D loss: 0.512(R 0.552, F0.473)] [D acc: 0.750(R 0.656, F 0.844)] [G loss: 0.512] [G acc: 0.109]\n",
      "2388 [D loss: 0.603(R 0.492, F0.714)] [D acc: 0.695(R 0.703, F 0.688)] [G loss: 0.603] [G acc: 0.078]\n",
      "2389 [D loss: 0.649(R 0.831, F0.468)] [D acc: 0.672(R 0.484, F 0.859)] [G loss: 0.649] [G acc: 0.062]\n",
      "2390 [D loss: 0.588(R 0.684, F0.492)] [D acc: 0.672(R 0.500, F 0.844)] [G loss: 0.588] [G acc: 0.141]\n",
      "2391 [D loss: 0.524(R 0.580, F0.468)] [D acc: 0.727(R 0.641, F 0.812)] [G loss: 0.524] [G acc: 0.188]\n",
      "2392 [D loss: 0.560(R 0.534, F0.586)] [D acc: 0.703(R 0.688, F 0.719)] [G loss: 0.560] [G acc: 0.109]\n",
      "2393 [D loss: 0.557(R 0.612, F0.502)] [D acc: 0.703(R 0.594, F 0.812)] [G loss: 0.557] [G acc: 0.047]\n",
      "2394 [D loss: 0.469(R 0.419, F0.519)] [D acc: 0.758(R 0.766, F 0.750)] [G loss: 0.469] [G acc: 0.109]\n",
      "2395 [D loss: 0.539(R 0.476, F0.602)] [D acc: 0.734(R 0.719, F 0.750)] [G loss: 0.539] [G acc: 0.062]\n",
      "2396 [D loss: 0.545(R 0.558, F0.533)] [D acc: 0.703(R 0.641, F 0.766)] [G loss: 0.545] [G acc: 0.047]\n",
      "2397 [D loss: 0.525(R 0.574, F0.476)] [D acc: 0.742(R 0.625, F 0.859)] [G loss: 0.525] [G acc: 0.078]\n",
      "2398 [D loss: 0.538(R 0.610, F0.467)] [D acc: 0.688(R 0.578, F 0.797)] [G loss: 0.538] [G acc: 0.094]\n",
      "2399 [D loss: 0.519(R 0.454, F0.583)] [D acc: 0.742(R 0.734, F 0.750)] [G loss: 0.519] [G acc: 0.156]\n",
      "INFO:tensorflow:Assets written to: ram://2ae09f76-81ab-4711-bd2a-79e0c660fd2b/assets\n",
      "INFO:tensorflow:Assets written to: ram://971180e8-7f44-42b9-b60d-14da2db06d00/assets\n",
      "INFO:tensorflow:Assets written to: ram://449fccc1-7370-4ecc-8324-21e61b807e26/assets\n",
      "2400 [D loss: 0.545(R 0.545, F0.544)] [D acc: 0.727(R 0.703, F 0.750)] [G loss: 0.545] [G acc: 0.031]\n",
      "2401 [D loss: 0.532(R 0.575, F0.489)] [D acc: 0.766(R 0.719, F 0.812)] [G loss: 0.532] [G acc: 0.094]\n",
      "2402 [D loss: 0.480(R 0.528, F0.431)] [D acc: 0.750(R 0.703, F 0.797)] [G loss: 0.480] [G acc: 0.094]\n",
      "2403 [D loss: 0.475(R 0.435, F0.515)] [D acc: 0.766(R 0.734, F 0.797)] [G loss: 0.475] [G acc: 0.062]\n",
      "2404 [D loss: 0.499(R 0.562, F0.437)] [D acc: 0.742(R 0.672, F 0.812)] [G loss: 0.499] [G acc: 0.078]\n",
      "2405 [D loss: 0.649(R 0.724, F0.574)] [D acc: 0.664(R 0.594, F 0.734)] [G loss: 0.649] [G acc: 0.172]\n",
      "2406 [D loss: 0.485(R 0.471, F0.498)] [D acc: 0.773(R 0.719, F 0.828)] [G loss: 0.485] [G acc: 0.125]\n",
      "2407 [D loss: 0.497(R 0.554, F0.439)] [D acc: 0.750(R 0.703, F 0.797)] [G loss: 0.497] [G acc: 0.172]\n",
      "2408 [D loss: 0.665(R 0.609, F0.720)] [D acc: 0.711(R 0.641, F 0.781)] [G loss: 0.665] [G acc: 0.141]\n",
      "2409 [D loss: 0.572(R 0.729, F0.416)] [D acc: 0.750(R 0.594, F 0.906)] [G loss: 0.572] [G acc: 0.109]\n",
      "2410 [D loss: 0.549(R 0.425, F0.673)] [D acc: 0.703(R 0.766, F 0.641)] [G loss: 0.549] [G acc: 0.172]\n",
      "2411 [D loss: 0.569(R 0.576, F0.563)] [D acc: 0.695(R 0.656, F 0.734)] [G loss: 0.569] [G acc: 0.156]\n",
      "2412 [D loss: 0.480(R 0.493, F0.466)] [D acc: 0.781(R 0.672, F 0.891)] [G loss: 0.480] [G acc: 0.062]\n",
      "2413 [D loss: 0.540(R 0.549, F0.531)] [D acc: 0.719(R 0.656, F 0.781)] [G loss: 0.540] [G acc: 0.109]\n",
      "2414 [D loss: 0.586(R 0.524, F0.649)] [D acc: 0.688(R 0.688, F 0.688)] [G loss: 0.586] [G acc: 0.031]\n",
      "2415 [D loss: 0.570(R 0.781, F0.359)] [D acc: 0.711(R 0.500, F 0.922)] [G loss: 0.570] [G acc: 0.094]\n",
      "2416 [D loss: 0.544(R 0.560, F0.528)] [D acc: 0.695(R 0.625, F 0.766)] [G loss: 0.544] [G acc: 0.156]\n",
      "2417 [D loss: 0.569(R 0.521, F0.617)] [D acc: 0.766(R 0.734, F 0.797)] [G loss: 0.569] [G acc: 0.109]\n",
      "2418 [D loss: 0.562(R 0.549, F0.575)] [D acc: 0.703(R 0.672, F 0.734)] [G loss: 0.562] [G acc: 0.125]\n",
      "2419 [D loss: 0.441(R 0.444, F0.437)] [D acc: 0.781(R 0.750, F 0.812)] [G loss: 0.441] [G acc: 0.125]\n",
      "2420 [D loss: 0.534(R 0.524, F0.544)] [D acc: 0.703(R 0.703, F 0.703)] [G loss: 0.534] [G acc: 0.188]\n",
      "2421 [D loss: 0.614(R 0.657, F0.572)] [D acc: 0.617(R 0.562, F 0.672)] [G loss: 0.614] [G acc: 0.172]\n",
      "2422 [D loss: 0.643(R 0.726, F0.561)] [D acc: 0.672(R 0.531, F 0.812)] [G loss: 0.643] [G acc: 0.109]\n",
      "2423 [D loss: 0.491(R 0.433, F0.549)] [D acc: 0.719(R 0.719, F 0.719)] [G loss: 0.491] [G acc: 0.125]\n",
      "2424 [D loss: 0.576(R 0.457, F0.696)] [D acc: 0.734(R 0.719, F 0.750)] [G loss: 0.576] [G acc: 0.062]\n",
      "2425 [D loss: 0.513(R 0.562, F0.464)] [D acc: 0.773(R 0.719, F 0.828)] [G loss: 0.513] [G acc: 0.094]\n",
      "2426 [D loss: 0.540(R 0.574, F0.506)] [D acc: 0.742(R 0.641, F 0.844)] [G loss: 0.540] [G acc: 0.141]\n",
      "2427 [D loss: 0.627(R 0.556, F0.699)] [D acc: 0.664(R 0.625, F 0.703)] [G loss: 0.627] [G acc: 0.047]\n",
      "2428 [D loss: 0.559(R 0.734, F0.383)] [D acc: 0.750(R 0.594, F 0.906)] [G loss: 0.559] [G acc: 0.031]\n",
      "2429 [D loss: 0.521(R 0.497, F0.546)] [D acc: 0.742(R 0.719, F 0.766)] [G loss: 0.521] [G acc: 0.031]\n",
      "2430 [D loss: 0.630(R 0.701, F0.560)] [D acc: 0.672(R 0.562, F 0.781)] [G loss: 0.630] [G acc: 0.125]\n",
      "2431 [D loss: 0.546(R 0.635, F0.457)] [D acc: 0.688(R 0.594, F 0.781)] [G loss: 0.546] [G acc: 0.078]\n",
      "2432 [D loss: 0.547(R 0.469, F0.625)] [D acc: 0.727(R 0.688, F 0.766)] [G loss: 0.547] [G acc: 0.172]\n",
      "2433 [D loss: 0.522(R 0.535, F0.509)] [D acc: 0.727(R 0.688, F 0.766)] [G loss: 0.522] [G acc: 0.109]\n",
      "2434 [D loss: 0.589(R 0.587, F0.590)] [D acc: 0.672(R 0.609, F 0.734)] [G loss: 0.589] [G acc: 0.047]\n",
      "2435 [D loss: 0.524(R 0.594, F0.454)] [D acc: 0.695(R 0.625, F 0.766)] [G loss: 0.524] [G acc: 0.156]\n",
      "2436 [D loss: 0.518(R 0.507, F0.529)] [D acc: 0.734(R 0.641, F 0.828)] [G loss: 0.518] [G acc: 0.062]\n",
      "2437 [D loss: 0.565(R 0.609, F0.520)] [D acc: 0.680(R 0.609, F 0.750)] [G loss: 0.565] [G acc: 0.125]\n",
      "2438 [D loss: 0.580(R 0.633, F0.526)] [D acc: 0.672(R 0.562, F 0.781)] [G loss: 0.580] [G acc: 0.078]\n",
      "2439 [D loss: 0.537(R 0.568, F0.505)] [D acc: 0.680(R 0.609, F 0.750)] [G loss: 0.537] [G acc: 0.156]\n",
      "2440 [D loss: 0.586(R 0.586, F0.586)] [D acc: 0.703(R 0.641, F 0.766)] [G loss: 0.586] [G acc: 0.078]\n",
      "2441 [D loss: 0.638(R 0.611, F0.666)] [D acc: 0.672(R 0.641, F 0.703)] [G loss: 0.638] [G acc: 0.047]\n",
      "2442 [D loss: 0.535(R 0.591, F0.480)] [D acc: 0.711(R 0.625, F 0.797)] [G loss: 0.535] [G acc: 0.109]\n",
      "2443 [D loss: 0.532(R 0.543, F0.521)] [D acc: 0.688(R 0.594, F 0.781)] [G loss: 0.532] [G acc: 0.078]\n",
      "2444 [D loss: 0.488(R 0.546, F0.430)] [D acc: 0.742(R 0.656, F 0.828)] [G loss: 0.488] [G acc: 0.078]\n",
      "2445 [D loss: 0.518(R 0.416, F0.620)] [D acc: 0.758(R 0.766, F 0.750)] [G loss: 0.518] [G acc: 0.109]\n",
      "2446 [D loss: 0.543(R 0.497, F0.589)] [D acc: 0.727(R 0.719, F 0.734)] [G loss: 0.543] [G acc: 0.125]\n",
      "2447 [D loss: 0.495(R 0.469, F0.520)] [D acc: 0.766(R 0.766, F 0.766)] [G loss: 0.495] [G acc: 0.078]\n",
      "2448 [D loss: 0.537(R 0.581, F0.493)] [D acc: 0.711(R 0.656, F 0.766)] [G loss: 0.537] [G acc: 0.062]\n",
      "2449 [D loss: 0.537(R 0.504, F0.570)] [D acc: 0.703(R 0.688, F 0.719)] [G loss: 0.537] [G acc: 0.062]\n",
      "2450 [D loss: 0.487(R 0.592, F0.382)] [D acc: 0.727(R 0.547, F 0.906)] [G loss: 0.487] [G acc: 0.109]\n",
      "2451 [D loss: 0.484(R 0.446, F0.521)] [D acc: 0.781(R 0.750, F 0.812)] [G loss: 0.484] [G acc: 0.078]\n",
      "2452 [D loss: 0.506(R 0.430, F0.582)] [D acc: 0.750(R 0.750, F 0.750)] [G loss: 0.506] [G acc: 0.078]\n",
      "2453 [D loss: 0.579(R 0.552, F0.607)] [D acc: 0.719(R 0.641, F 0.797)] [G loss: 0.579] [G acc: 0.062]\n",
      "2454 [D loss: 0.430(R 0.391, F0.468)] [D acc: 0.844(R 0.734, F 0.953)] [G loss: 0.430] [G acc: 0.078]\n",
      "2455 [D loss: 0.623(R 0.593, F0.652)] [D acc: 0.672(R 0.578, F 0.766)] [G loss: 0.623] [G acc: 0.062]\n",
      "2456 [D loss: 0.570(R 0.677, F0.464)] [D acc: 0.703(R 0.562, F 0.844)] [G loss: 0.570] [G acc: 0.156]\n",
      "2457 [D loss: 0.498(R 0.450, F0.546)] [D acc: 0.742(R 0.734, F 0.750)] [G loss: 0.498] [G acc: 0.156]\n",
      "2458 [D loss: 0.587(R 0.653, F0.521)] [D acc: 0.711(R 0.625, F 0.797)] [G loss: 0.587] [G acc: 0.125]\n",
      "2459 [D loss: 0.527(R 0.483, F0.571)] [D acc: 0.750(R 0.688, F 0.812)] [G loss: 0.527] [G acc: 0.125]\n",
      "2460 [D loss: 0.585(R 0.741, F0.428)] [D acc: 0.672(R 0.500, F 0.844)] [G loss: 0.585] [G acc: 0.109]\n",
      "2461 [D loss: 0.455(R 0.423, F0.486)] [D acc: 0.820(R 0.750, F 0.891)] [G loss: 0.455] [G acc: 0.062]\n",
      "2462 [D loss: 0.501(R 0.507, F0.494)] [D acc: 0.719(R 0.625, F 0.812)] [G loss: 0.501] [G acc: 0.094]\n",
      "2463 [D loss: 0.603(R 0.519, F0.686)] [D acc: 0.703(R 0.703, F 0.703)] [G loss: 0.603] [G acc: 0.094]\n",
      "2464 [D loss: 0.568(R 0.588, F0.548)] [D acc: 0.695(R 0.562, F 0.828)] [G loss: 0.568] [G acc: 0.094]\n",
      "2465 [D loss: 0.494(R 0.505, F0.483)] [D acc: 0.781(R 0.672, F 0.891)] [G loss: 0.494] [G acc: 0.125]\n",
      "2466 [D loss: 0.557(R 0.442, F0.672)] [D acc: 0.727(R 0.750, F 0.703)] [G loss: 0.557] [G acc: 0.078]\n",
      "2467 [D loss: 0.567(R 0.664, F0.471)] [D acc: 0.688(R 0.578, F 0.797)] [G loss: 0.567] [G acc: 0.094]\n",
      "2468 [D loss: 0.489(R 0.472, F0.506)] [D acc: 0.789(R 0.719, F 0.859)] [G loss: 0.489] [G acc: 0.078]\n",
      "2469 [D loss: 0.553(R 0.561, F0.545)] [D acc: 0.703(R 0.625, F 0.781)] [G loss: 0.553] [G acc: 0.094]\n",
      "2470 [D loss: 0.591(R 0.768, F0.414)] [D acc: 0.703(R 0.516, F 0.891)] [G loss: 0.591] [G acc: 0.062]\n",
      "2471 [D loss: 0.513(R 0.475, F0.551)] [D acc: 0.766(R 0.688, F 0.844)] [G loss: 0.513] [G acc: 0.062]\n",
      "2472 [D loss: 0.484(R 0.471, F0.497)] [D acc: 0.789(R 0.734, F 0.844)] [G loss: 0.484] [G acc: 0.109]\n",
      "2473 [D loss: 0.548(R 0.600, F0.496)] [D acc: 0.734(R 0.641, F 0.828)] [G loss: 0.548] [G acc: 0.109]\n",
      "2474 [D loss: 0.565(R 0.516, F0.615)] [D acc: 0.688(R 0.703, F 0.672)] [G loss: 0.565] [G acc: 0.125]\n",
      "2475 [D loss: 0.481(R 0.464, F0.498)] [D acc: 0.758(R 0.734, F 0.781)] [G loss: 0.481] [G acc: 0.156]\n",
      "2476 [D loss: 0.429(R 0.425, F0.433)] [D acc: 0.820(R 0.734, F 0.906)] [G loss: 0.429] [G acc: 0.047]\n",
      "2477 [D loss: 0.486(R 0.499, F0.472)] [D acc: 0.797(R 0.734, F 0.859)] [G loss: 0.486] [G acc: 0.109]\n",
      "2478 [D loss: 0.526(R 0.536, F0.516)] [D acc: 0.766(R 0.703, F 0.828)] [G loss: 0.526] [G acc: 0.062]\n",
      "2479 [D loss: 0.548(R 0.616, F0.480)] [D acc: 0.711(R 0.594, F 0.828)] [G loss: 0.548] [G acc: 0.141]\n",
      "2480 [D loss: 0.515(R 0.554, F0.476)] [D acc: 0.742(R 0.625, F 0.859)] [G loss: 0.515] [G acc: 0.062]\n",
      "2481 [D loss: 0.493(R 0.424, F0.561)] [D acc: 0.758(R 0.734, F 0.781)] [G loss: 0.493] [G acc: 0.062]\n",
      "2482 [D loss: 0.525(R 0.536, F0.514)] [D acc: 0.781(R 0.734, F 0.828)] [G loss: 0.525] [G acc: 0.109]\n",
      "2483 [D loss: 0.522(R 0.559, F0.486)] [D acc: 0.727(R 0.656, F 0.797)] [G loss: 0.522] [G acc: 0.062]\n",
      "2484 [D loss: 0.563(R 0.695, F0.431)] [D acc: 0.688(R 0.500, F 0.875)] [G loss: 0.563] [G acc: 0.156]\n",
      "2485 [D loss: 0.563(R 0.529, F0.596)] [D acc: 0.734(R 0.719, F 0.750)] [G loss: 0.563] [G acc: 0.094]\n",
      "2486 [D loss: 0.551(R 0.534, F0.567)] [D acc: 0.719(R 0.703, F 0.734)] [G loss: 0.551] [G acc: 0.109]\n",
      "2487 [D loss: 0.507(R 0.584, F0.430)] [D acc: 0.742(R 0.609, F 0.875)] [G loss: 0.507] [G acc: 0.125]\n",
      "2488 [D loss: 0.558(R 0.552, F0.564)] [D acc: 0.711(R 0.656, F 0.766)] [G loss: 0.558] [G acc: 0.078]\n",
      "2489 [D loss: 0.581(R 0.678, F0.484)] [D acc: 0.711(R 0.547, F 0.875)] [G loss: 0.581] [G acc: 0.062]\n",
      "2490 [D loss: 0.600(R 0.610, F0.589)] [D acc: 0.703(R 0.609, F 0.797)] [G loss: 0.600] [G acc: 0.062]\n",
      "2491 [D loss: 0.452(R 0.470, F0.434)] [D acc: 0.781(R 0.734, F 0.828)] [G loss: 0.452] [G acc: 0.109]\n",
      "2492 [D loss: 0.539(R 0.636, F0.441)] [D acc: 0.742(R 0.594, F 0.891)] [G loss: 0.539] [G acc: 0.109]\n",
      "2493 [D loss: 0.535(R 0.562, F0.508)] [D acc: 0.688(R 0.594, F 0.781)] [G loss: 0.535] [G acc: 0.141]\n",
      "2494 [D loss: 0.562(R 0.607, F0.517)] [D acc: 0.703(R 0.641, F 0.766)] [G loss: 0.562] [G acc: 0.156]\n",
      "2495 [D loss: 0.511(R 0.452, F0.569)] [D acc: 0.719(R 0.703, F 0.734)] [G loss: 0.511] [G acc: 0.125]\n",
      "2496 [D loss: 0.525(R 0.589, F0.461)] [D acc: 0.750(R 0.656, F 0.844)] [G loss: 0.525] [G acc: 0.109]\n",
      "2497 [D loss: 0.589(R 0.513, F0.665)] [D acc: 0.719(R 0.688, F 0.750)] [G loss: 0.589] [G acc: 0.047]\n",
      "2498 [D loss: 0.546(R 0.660, F0.432)] [D acc: 0.695(R 0.547, F 0.844)] [G loss: 0.546] [G acc: 0.031]\n",
      "2499 [D loss: 0.551(R 0.580, F0.522)] [D acc: 0.750(R 0.656, F 0.844)] [G loss: 0.551] [G acc: 0.062]\n",
      "INFO:tensorflow:Assets written to: ram://a89416ec-4026-4de1-aed5-a27808fccd49/assets\n",
      "INFO:tensorflow:Assets written to: ram://1fb13ac2-3639-41f8-944e-70f669450abe/assets\n",
      "INFO:tensorflow:Assets written to: ram://075e5cbc-e9a7-4f65-862e-b6b3ccb86f21/assets\n",
      "2500 [D loss: 0.537(R 0.562, F0.513)] [D acc: 0.750(R 0.672, F 0.828)] [G loss: 0.537] [G acc: 0.047]\n",
      "2501 [D loss: 0.601(R 0.580, F0.621)] [D acc: 0.664(R 0.578, F 0.750)] [G loss: 0.601] [G acc: 0.062]\n",
      "2502 [D loss: 0.603(R 0.610, F0.596)] [D acc: 0.672(R 0.594, F 0.750)] [G loss: 0.603] [G acc: 0.062]\n",
      "2503 [D loss: 0.546(R 0.592, F0.501)] [D acc: 0.703(R 0.609, F 0.797)] [G loss: 0.546] [G acc: 0.141]\n",
      "2504 [D loss: 0.534(R 0.589, F0.479)] [D acc: 0.742(R 0.594, F 0.891)] [G loss: 0.534] [G acc: 0.188]\n",
      "2505 [D loss: 0.496(R 0.477, F0.515)] [D acc: 0.789(R 0.750, F 0.828)] [G loss: 0.496] [G acc: 0.031]\n",
      "2506 [D loss: 0.513(R 0.449, F0.578)] [D acc: 0.742(R 0.750, F 0.734)] [G loss: 0.513] [G acc: 0.094]\n",
      "2507 [D loss: 0.549(R 0.530, F0.568)] [D acc: 0.727(R 0.625, F 0.828)] [G loss: 0.549] [G acc: 0.078]\n",
      "2508 [D loss: 0.522(R 0.539, F0.506)] [D acc: 0.711(R 0.656, F 0.766)] [G loss: 0.522] [G acc: 0.062]\n",
      "2509 [D loss: 0.488(R 0.567, F0.408)] [D acc: 0.797(R 0.641, F 0.953)] [G loss: 0.488] [G acc: 0.078]\n",
      "2510 [D loss: 0.522(R 0.504, F0.541)] [D acc: 0.727(R 0.688, F 0.766)] [G loss: 0.522] [G acc: 0.062]\n",
      "2511 [D loss: 0.589(R 0.693, F0.485)] [D acc: 0.641(R 0.500, F 0.781)] [G loss: 0.589] [G acc: 0.062]\n",
      "2512 [D loss: 0.569(R 0.518, F0.619)] [D acc: 0.719(R 0.719, F 0.719)] [G loss: 0.569] [G acc: 0.047]\n",
      "2513 [D loss: 0.547(R 0.547, F0.546)] [D acc: 0.758(R 0.688, F 0.828)] [G loss: 0.547] [G acc: 0.109]\n",
      "2514 [D loss: 0.563(R 0.634, F0.491)] [D acc: 0.719(R 0.609, F 0.828)] [G loss: 0.563] [G acc: 0.141]\n",
      "2515 [D loss: 0.576(R 0.530, F0.623)] [D acc: 0.695(R 0.672, F 0.719)] [G loss: 0.576] [G acc: 0.094]\n",
      "2516 [D loss: 0.524(R 0.488, F0.561)] [D acc: 0.695(R 0.672, F 0.719)] [G loss: 0.524] [G acc: 0.125]\n",
      "2517 [D loss: 0.548(R 0.564, F0.532)] [D acc: 0.719(R 0.656, F 0.781)] [G loss: 0.548] [G acc: 0.125]\n",
      "2518 [D loss: 0.609(R 0.523, F0.695)] [D acc: 0.648(R 0.656, F 0.641)] [G loss: 0.609] [G acc: 0.062]\n",
      "2519 [D loss: 0.571(R 0.562, F0.579)] [D acc: 0.766(R 0.672, F 0.859)] [G loss: 0.571] [G acc: 0.047]\n",
      "2520 [D loss: 0.546(R 0.571, F0.520)] [D acc: 0.680(R 0.547, F 0.812)] [G loss: 0.546] [G acc: 0.078]\n",
      "2521 [D loss: 0.527(R 0.596, F0.458)] [D acc: 0.695(R 0.609, F 0.781)] [G loss: 0.527] [G acc: 0.203]\n",
      "2522 [D loss: 0.590(R 0.538, F0.642)] [D acc: 0.719(R 0.688, F 0.750)] [G loss: 0.590] [G acc: 0.141]\n",
      "2523 [D loss: 0.562(R 0.578, F0.547)] [D acc: 0.688(R 0.594, F 0.781)] [G loss: 0.562] [G acc: 0.125]\n",
      "2524 [D loss: 0.515(R 0.590, F0.440)] [D acc: 0.703(R 0.625, F 0.781)] [G loss: 0.515] [G acc: 0.062]\n",
      "2525 [D loss: 0.509(R 0.540, F0.479)] [D acc: 0.742(R 0.625, F 0.859)] [G loss: 0.509] [G acc: 0.094]\n",
      "2526 [D loss: 0.499(R 0.505, F0.493)] [D acc: 0.789(R 0.750, F 0.828)] [G loss: 0.499] [G acc: 0.156]\n",
      "2527 [D loss: 0.497(R 0.458, F0.537)] [D acc: 0.727(R 0.719, F 0.734)] [G loss: 0.497] [G acc: 0.109]\n",
      "2528 [D loss: 0.472(R 0.439, F0.506)] [D acc: 0.805(R 0.750, F 0.859)] [G loss: 0.472] [G acc: 0.156]\n",
      "2529 [D loss: 0.543(R 0.542, F0.545)] [D acc: 0.711(R 0.625, F 0.797)] [G loss: 0.543] [G acc: 0.047]\n",
      "2530 [D loss: 0.550(R 0.584, F0.516)] [D acc: 0.727(R 0.609, F 0.844)] [G loss: 0.550] [G acc: 0.078]\n",
      "2531 [D loss: 0.624(R 0.654, F0.594)] [D acc: 0.688(R 0.672, F 0.703)] [G loss: 0.624] [G acc: 0.078]\n",
      "2532 [D loss: 0.599(R 0.631, F0.567)] [D acc: 0.648(R 0.547, F 0.750)] [G loss: 0.599] [G acc: 0.109]\n",
      "2533 [D loss: 0.472(R 0.529, F0.414)] [D acc: 0.820(R 0.750, F 0.891)] [G loss: 0.472] [G acc: 0.125]\n",
      "2534 [D loss: 0.480(R 0.455, F0.505)] [D acc: 0.773(R 0.719, F 0.828)] [G loss: 0.480] [G acc: 0.047]\n",
      "2535 [D loss: 0.513(R 0.434, F0.593)] [D acc: 0.734(R 0.750, F 0.719)] [G loss: 0.513] [G acc: 0.172]\n",
      "2536 [D loss: 0.600(R 0.588, F0.613)] [D acc: 0.656(R 0.625, F 0.688)] [G loss: 0.600] [G acc: 0.062]\n",
      "2537 [D loss: 0.544(R 0.504, F0.583)] [D acc: 0.758(R 0.703, F 0.812)] [G loss: 0.544] [G acc: 0.094]\n",
      "2538 [D loss: 0.467(R 0.542, F0.392)] [D acc: 0.766(R 0.672, F 0.859)] [G loss: 0.467] [G acc: 0.141]\n",
      "2539 [D loss: 0.450(R 0.439, F0.460)] [D acc: 0.836(R 0.797, F 0.875)] [G loss: 0.450] [G acc: 0.062]\n",
      "2540 [D loss: 0.521(R 0.572, F0.470)] [D acc: 0.703(R 0.656, F 0.750)] [G loss: 0.521] [G acc: 0.047]\n",
      "2541 [D loss: 0.470(R 0.536, F0.403)] [D acc: 0.734(R 0.656, F 0.812)] [G loss: 0.470] [G acc: 0.141]\n",
      "2542 [D loss: 0.477(R 0.443, F0.510)] [D acc: 0.781(R 0.750, F 0.812)] [G loss: 0.477] [G acc: 0.141]\n",
      "2543 [D loss: 0.584(R 0.706, F0.462)] [D acc: 0.719(R 0.594, F 0.844)] [G loss: 0.584] [G acc: 0.094]\n",
      "2544 [D loss: 0.536(R 0.537, F0.534)] [D acc: 0.695(R 0.656, F 0.734)] [G loss: 0.536] [G acc: 0.047]\n",
      "2545 [D loss: 0.579(R 0.635, F0.522)] [D acc: 0.703(R 0.641, F 0.766)] [G loss: 0.579] [G acc: 0.125]\n",
      "2546 [D loss: 0.556(R 0.557, F0.555)] [D acc: 0.695(R 0.656, F 0.734)] [G loss: 0.556] [G acc: 0.141]\n",
      "2547 [D loss: 0.611(R 0.579, F0.643)] [D acc: 0.641(R 0.609, F 0.672)] [G loss: 0.611] [G acc: 0.078]\n",
      "2548 [D loss: 0.526(R 0.565, F0.487)] [D acc: 0.734(R 0.609, F 0.859)] [G loss: 0.526] [G acc: 0.078]\n",
      "2549 [D loss: 0.520(R 0.557, F0.483)] [D acc: 0.734(R 0.672, F 0.797)] [G loss: 0.520] [G acc: 0.109]\n",
      "2550 [D loss: 0.581(R 0.548, F0.614)] [D acc: 0.727(R 0.719, F 0.734)] [G loss: 0.581] [G acc: 0.031]\n",
      "2551 [D loss: 0.583(R 0.623, F0.544)] [D acc: 0.688(R 0.578, F 0.797)] [G loss: 0.583] [G acc: 0.094]\n",
      "2552 [D loss: 0.487(R 0.578, F0.396)] [D acc: 0.773(R 0.672, F 0.875)] [G loss: 0.487] [G acc: 0.125]\n",
      "2553 [D loss: 0.508(R 0.548, F0.468)] [D acc: 0.703(R 0.578, F 0.828)] [G loss: 0.508] [G acc: 0.094]\n",
      "2554 [D loss: 0.513(R 0.397, F0.629)] [D acc: 0.789(R 0.828, F 0.750)] [G loss: 0.513] [G acc: 0.062]\n",
      "2555 [D loss: 0.503(R 0.459, F0.548)] [D acc: 0.727(R 0.703, F 0.750)] [G loss: 0.503] [G acc: 0.047]\n",
      "2556 [D loss: 0.577(R 0.625, F0.528)] [D acc: 0.664(R 0.578, F 0.750)] [G loss: 0.577] [G acc: 0.109]\n",
      "2557 [D loss: 0.502(R 0.596, F0.408)] [D acc: 0.773(R 0.641, F 0.906)] [G loss: 0.502] [G acc: 0.109]\n",
      "2558 [D loss: 0.530(R 0.589, F0.471)] [D acc: 0.734(R 0.656, F 0.812)] [G loss: 0.530] [G acc: 0.172]\n",
      "2559 [D loss: 0.509(R 0.429, F0.589)] [D acc: 0.742(R 0.703, F 0.781)] [G loss: 0.509] [G acc: 0.094]\n",
      "2560 [D loss: 0.475(R 0.430, F0.520)] [D acc: 0.781(R 0.797, F 0.766)] [G loss: 0.475] [G acc: 0.156]\n",
      "2561 [D loss: 0.521(R 0.520, F0.522)] [D acc: 0.766(R 0.734, F 0.797)] [G loss: 0.521] [G acc: 0.094]\n",
      "2562 [D loss: 0.581(R 0.558, F0.604)] [D acc: 0.664(R 0.641, F 0.688)] [G loss: 0.581] [G acc: 0.078]\n",
      "2563 [D loss: 0.564(R 0.565, F0.563)] [D acc: 0.758(R 0.672, F 0.844)] [G loss: 0.564] [G acc: 0.078]\n",
      "2564 [D loss: 0.477(R 0.451, F0.503)] [D acc: 0.773(R 0.719, F 0.828)] [G loss: 0.477] [G acc: 0.094]\n",
      "2565 [D loss: 0.443(R 0.437, F0.450)] [D acc: 0.805(R 0.781, F 0.828)] [G loss: 0.443] [G acc: 0.078]\n",
      "2566 [D loss: 0.475(R 0.530, F0.420)] [D acc: 0.773(R 0.672, F 0.875)] [G loss: 0.475] [G acc: 0.016]\n",
      "2567 [D loss: 0.489(R 0.548, F0.430)] [D acc: 0.781(R 0.719, F 0.844)] [G loss: 0.489] [G acc: 0.094]\n",
      "2568 [D loss: 0.495(R 0.453, F0.537)] [D acc: 0.742(R 0.750, F 0.734)] [G loss: 0.495] [G acc: 0.062]\n",
      "2569 [D loss: 0.493(R 0.575, F0.412)] [D acc: 0.797(R 0.688, F 0.906)] [G loss: 0.493] [G acc: 0.109]\n",
      "2570 [D loss: 0.573(R 0.585, F0.561)] [D acc: 0.656(R 0.609, F 0.703)] [G loss: 0.573] [G acc: 0.109]\n",
      "2571 [D loss: 0.419(R 0.414, F0.425)] [D acc: 0.820(R 0.750, F 0.891)] [G loss: 0.419] [G acc: 0.141]\n",
      "2572 [D loss: 0.510(R 0.549, F0.471)] [D acc: 0.742(R 0.688, F 0.797)] [G loss: 0.510] [G acc: 0.078]\n",
      "2573 [D loss: 0.550(R 0.602, F0.499)] [D acc: 0.711(R 0.625, F 0.797)] [G loss: 0.550] [G acc: 0.031]\n",
      "2574 [D loss: 0.531(R 0.525, F0.537)] [D acc: 0.695(R 0.641, F 0.750)] [G loss: 0.531] [G acc: 0.094]\n",
      "2575 [D loss: 0.530(R 0.502, F0.558)] [D acc: 0.734(R 0.703, F 0.766)] [G loss: 0.530] [G acc: 0.078]\n",
      "2576 [D loss: 0.554(R 0.462, F0.645)] [D acc: 0.719(R 0.734, F 0.703)] [G loss: 0.554] [G acc: 0.109]\n",
      "2577 [D loss: 0.556(R 0.597, F0.515)] [D acc: 0.727(R 0.672, F 0.781)] [G loss: 0.556] [G acc: 0.109]\n",
      "2578 [D loss: 0.494(R 0.504, F0.483)] [D acc: 0.789(R 0.719, F 0.859)] [G loss: 0.494] [G acc: 0.078]\n",
      "2579 [D loss: 0.511(R 0.593, F0.428)] [D acc: 0.797(R 0.719, F 0.875)] [G loss: 0.511] [G acc: 0.125]\n",
      "2580 [D loss: 0.491(R 0.455, F0.527)] [D acc: 0.828(R 0.766, F 0.891)] [G loss: 0.491] [G acc: 0.078]\n",
      "2581 [D loss: 0.474(R 0.540, F0.408)] [D acc: 0.781(R 0.719, F 0.844)] [G loss: 0.474] [G acc: 0.125]\n",
      "2582 [D loss: 0.450(R 0.411, F0.488)] [D acc: 0.781(R 0.734, F 0.828)] [G loss: 0.450] [G acc: 0.172]\n",
      "2583 [D loss: 0.602(R 0.610, F0.595)] [D acc: 0.734(R 0.672, F 0.797)] [G loss: 0.602] [G acc: 0.188]\n",
      "2584 [D loss: 0.559(R 0.502, F0.616)] [D acc: 0.750(R 0.719, F 0.781)] [G loss: 0.559] [G acc: 0.078]\n",
      "2585 [D loss: 0.607(R 0.687, F0.527)] [D acc: 0.688(R 0.578, F 0.797)] [G loss: 0.607] [G acc: 0.047]\n",
      "2586 [D loss: 0.578(R 0.651, F0.504)] [D acc: 0.703(R 0.625, F 0.781)] [G loss: 0.578] [G acc: 0.062]\n",
      "2587 [D loss: 0.571(R 0.579, F0.564)] [D acc: 0.695(R 0.625, F 0.766)] [G loss: 0.571] [G acc: 0.141]\n",
      "2588 [D loss: 0.543(R 0.448, F0.639)] [D acc: 0.789(R 0.781, F 0.797)] [G loss: 0.543] [G acc: 0.047]\n",
      "2589 [D loss: 0.475(R 0.540, F0.410)] [D acc: 0.703(R 0.594, F 0.812)] [G loss: 0.475] [G acc: 0.062]\n",
      "2590 [D loss: 0.534(R 0.512, F0.556)] [D acc: 0.734(R 0.688, F 0.781)] [G loss: 0.534] [G acc: 0.094]\n",
      "2591 [D loss: 0.533(R 0.627, F0.439)] [D acc: 0.734(R 0.594, F 0.875)] [G loss: 0.533] [G acc: 0.141]\n",
      "2592 [D loss: 0.551(R 0.536, F0.567)] [D acc: 0.711(R 0.688, F 0.734)] [G loss: 0.551] [G acc: 0.125]\n",
      "2593 [D loss: 0.459(R 0.457, F0.461)] [D acc: 0.750(R 0.688, F 0.812)] [G loss: 0.459] [G acc: 0.172]\n",
      "2594 [D loss: 0.574(R 0.520, F0.628)] [D acc: 0.680(R 0.703, F 0.656)] [G loss: 0.574] [G acc: 0.109]\n",
      "2595 [D loss: 0.503(R 0.513, F0.494)] [D acc: 0.758(R 0.688, F 0.828)] [G loss: 0.503] [G acc: 0.125]\n",
      "2596 [D loss: 0.584(R 0.549, F0.619)] [D acc: 0.680(R 0.641, F 0.719)] [G loss: 0.584] [G acc: 0.062]\n",
      "2597 [D loss: 0.525(R 0.572, F0.478)] [D acc: 0.750(R 0.656, F 0.844)] [G loss: 0.525] [G acc: 0.125]\n",
      "2598 [D loss: 0.566(R 0.530, F0.602)] [D acc: 0.719(R 0.703, F 0.734)] [G loss: 0.566] [G acc: 0.109]\n",
      "2599 [D loss: 0.554(R 0.512, F0.596)] [D acc: 0.734(R 0.672, F 0.797)] [G loss: 0.554] [G acc: 0.062]\n",
      "INFO:tensorflow:Assets written to: ram://8129e9ab-6e35-4e55-b868-a945c718a521/assets\n",
      "INFO:tensorflow:Assets written to: ram://4ab3af4e-be23-494c-a39b-da15b3e039ff/assets\n",
      "INFO:tensorflow:Assets written to: ram://381f888a-bad8-45cb-a150-c22ee0197e32/assets\n",
      "2600 [D loss: 0.516(R 0.481, F0.552)] [D acc: 0.742(R 0.734, F 0.750)] [G loss: 0.516] [G acc: 0.062]\n",
      "2601 [D loss: 0.528(R 0.604, F0.453)] [D acc: 0.742(R 0.672, F 0.812)] [G loss: 0.528] [G acc: 0.062]\n",
      "2602 [D loss: 0.501(R 0.467, F0.535)] [D acc: 0.742(R 0.719, F 0.766)] [G loss: 0.501] [G acc: 0.031]\n",
      "2603 [D loss: 0.564(R 0.604, F0.524)] [D acc: 0.719(R 0.625, F 0.812)] [G loss: 0.564] [G acc: 0.109]\n",
      "2604 [D loss: 0.475(R 0.477, F0.474)] [D acc: 0.727(R 0.688, F 0.766)] [G loss: 0.475] [G acc: 0.125]\n",
      "2605 [D loss: 0.549(R 0.566, F0.533)] [D acc: 0.734(R 0.688, F 0.781)] [G loss: 0.549] [G acc: 0.047]\n",
      "2606 [D loss: 0.663(R 0.743, F0.583)] [D acc: 0.695(R 0.516, F 0.875)] [G loss: 0.663] [G acc: 0.062]\n",
      "2607 [D loss: 0.596(R 0.604, F0.589)] [D acc: 0.688(R 0.609, F 0.766)] [G loss: 0.596] [G acc: 0.109]\n",
      "2608 [D loss: 0.519(R 0.564, F0.473)] [D acc: 0.727(R 0.609, F 0.844)] [G loss: 0.519] [G acc: 0.125]\n",
      "2609 [D loss: 0.515(R 0.589, F0.440)] [D acc: 0.758(R 0.625, F 0.891)] [G loss: 0.515] [G acc: 0.109]\n",
      "2610 [D loss: 0.449(R 0.457, F0.440)] [D acc: 0.789(R 0.734, F 0.844)] [G loss: 0.449] [G acc: 0.031]\n",
      "2611 [D loss: 0.597(R 0.547, F0.648)] [D acc: 0.695(R 0.641, F 0.750)] [G loss: 0.597] [G acc: 0.078]\n",
      "2612 [D loss: 0.508(R 0.524, F0.491)] [D acc: 0.727(R 0.672, F 0.781)] [G loss: 0.508] [G acc: 0.047]\n",
      "2613 [D loss: 0.523(R 0.554, F0.493)] [D acc: 0.727(R 0.641, F 0.812)] [G loss: 0.523] [G acc: 0.094]\n",
      "2614 [D loss: 0.549(R 0.548, F0.550)] [D acc: 0.711(R 0.672, F 0.750)] [G loss: 0.549] [G acc: 0.094]\n",
      "2615 [D loss: 0.503(R 0.552, F0.455)] [D acc: 0.766(R 0.688, F 0.844)] [G loss: 0.503] [G acc: 0.062]\n",
      "2616 [D loss: 0.538(R 0.600, F0.477)] [D acc: 0.766(R 0.641, F 0.891)] [G loss: 0.538] [G acc: 0.156]\n",
      "2617 [D loss: 0.596(R 0.591, F0.601)] [D acc: 0.703(R 0.625, F 0.781)] [G loss: 0.596] [G acc: 0.094]\n",
      "2618 [D loss: 0.547(R 0.632, F0.463)] [D acc: 0.742(R 0.594, F 0.891)] [G loss: 0.547] [G acc: 0.078]\n",
      "2619 [D loss: 0.578(R 0.610, F0.547)] [D acc: 0.727(R 0.609, F 0.844)] [G loss: 0.578] [G acc: 0.125]\n",
      "2620 [D loss: 0.498(R 0.497, F0.499)] [D acc: 0.758(R 0.688, F 0.828)] [G loss: 0.498] [G acc: 0.109]\n",
      "2621 [D loss: 0.482(R 0.518, F0.446)] [D acc: 0.781(R 0.719, F 0.844)] [G loss: 0.482] [G acc: 0.125]\n",
      "2622 [D loss: 0.563(R 0.480, F0.647)] [D acc: 0.695(R 0.688, F 0.703)] [G loss: 0.563] [G acc: 0.125]\n",
      "2623 [D loss: 0.517(R 0.471, F0.564)] [D acc: 0.781(R 0.766, F 0.797)] [G loss: 0.517] [G acc: 0.047]\n",
      "2624 [D loss: 0.480(R 0.567, F0.393)] [D acc: 0.750(R 0.625, F 0.875)] [G loss: 0.480] [G acc: 0.109]\n",
      "2625 [D loss: 0.547(R 0.423, F0.671)] [D acc: 0.773(R 0.750, F 0.797)] [G loss: 0.547] [G acc: 0.047]\n",
      "2626 [D loss: 0.466(R 0.544, F0.389)] [D acc: 0.773(R 0.641, F 0.906)] [G loss: 0.466] [G acc: 0.031]\n",
      "2627 [D loss: 0.523(R 0.597, F0.449)] [D acc: 0.734(R 0.641, F 0.828)] [G loss: 0.523] [G acc: 0.141]\n",
      "2628 [D loss: 0.513(R 0.521, F0.506)] [D acc: 0.750(R 0.703, F 0.797)] [G loss: 0.513] [G acc: 0.125]\n",
      "2629 [D loss: 0.418(R 0.446, F0.389)] [D acc: 0.789(R 0.719, F 0.859)] [G loss: 0.418] [G acc: 0.094]\n",
      "2630 [D loss: 0.459(R 0.500, F0.419)] [D acc: 0.789(R 0.734, F 0.844)] [G loss: 0.459] [G acc: 0.109]\n",
      "2631 [D loss: 0.604(R 0.483, F0.724)] [D acc: 0.766(R 0.734, F 0.797)] [G loss: 0.604] [G acc: 0.109]\n",
      "2632 [D loss: 0.462(R 0.519, F0.406)] [D acc: 0.805(R 0.734, F 0.875)] [G loss: 0.462] [G acc: 0.094]\n",
      "2633 [D loss: 0.665(R 0.586, F0.743)] [D acc: 0.633(R 0.594, F 0.672)] [G loss: 0.665] [G acc: 0.062]\n",
      "2634 [D loss: 0.548(R 0.670, F0.426)] [D acc: 0.727(R 0.594, F 0.859)] [G loss: 0.548] [G acc: 0.125]\n",
      "2635 [D loss: 0.550(R 0.582, F0.518)] [D acc: 0.680(R 0.609, F 0.750)] [G loss: 0.550] [G acc: 0.141]\n",
      "2636 [D loss: 0.495(R 0.504, F0.485)] [D acc: 0.758(R 0.719, F 0.797)] [G loss: 0.495] [G acc: 0.203]\n",
      "2637 [D loss: 0.598(R 0.441, F0.754)] [D acc: 0.688(R 0.734, F 0.641)] [G loss: 0.598] [G acc: 0.078]\n",
      "2638 [D loss: 0.546(R 0.617, F0.474)] [D acc: 0.758(R 0.625, F 0.891)] [G loss: 0.546] [G acc: 0.094]\n",
      "2639 [D loss: 0.478(R 0.461, F0.495)] [D acc: 0.750(R 0.719, F 0.781)] [G loss: 0.478] [G acc: 0.172]\n",
      "2640 [D loss: 0.439(R 0.478, F0.401)] [D acc: 0.766(R 0.719, F 0.812)] [G loss: 0.439] [G acc: 0.125]\n",
      "2641 [D loss: 0.514(R 0.518, F0.510)] [D acc: 0.750(R 0.734, F 0.766)] [G loss: 0.514] [G acc: 0.047]\n",
      "2642 [D loss: 0.496(R 0.550, F0.441)] [D acc: 0.766(R 0.641, F 0.891)] [G loss: 0.496] [G acc: 0.156]\n",
      "2643 [D loss: 0.567(R 0.646, F0.488)] [D acc: 0.719(R 0.656, F 0.781)] [G loss: 0.567] [G acc: 0.062]\n",
      "2644 [D loss: 0.510(R 0.475, F0.545)] [D acc: 0.719(R 0.672, F 0.766)] [G loss: 0.510] [G acc: 0.094]\n",
      "2645 [D loss: 0.590(R 0.699, F0.482)] [D acc: 0.648(R 0.500, F 0.797)] [G loss: 0.590] [G acc: 0.109]\n",
      "2646 [D loss: 0.499(R 0.507, F0.492)] [D acc: 0.742(R 0.656, F 0.828)] [G loss: 0.499] [G acc: 0.125]\n",
      "2647 [D loss: 0.477(R 0.526, F0.429)] [D acc: 0.773(R 0.719, F 0.828)] [G loss: 0.477] [G acc: 0.109]\n",
      "2648 [D loss: 0.538(R 0.487, F0.590)] [D acc: 0.719(R 0.734, F 0.703)] [G loss: 0.538] [G acc: 0.156]\n",
      "2649 [D loss: 0.535(R 0.490, F0.580)] [D acc: 0.734(R 0.734, F 0.734)] [G loss: 0.535] [G acc: 0.031]\n",
      "2650 [D loss: 0.568(R 0.632, F0.504)] [D acc: 0.727(R 0.672, F 0.781)] [G loss: 0.568] [G acc: 0.109]\n",
      "2651 [D loss: 0.525(R 0.597, F0.453)] [D acc: 0.766(R 0.672, F 0.859)] [G loss: 0.525] [G acc: 0.031]\n",
      "2652 [D loss: 0.556(R 0.546, F0.565)] [D acc: 0.672(R 0.641, F 0.703)] [G loss: 0.556] [G acc: 0.094]\n",
      "2653 [D loss: 0.572(R 0.671, F0.473)] [D acc: 0.703(R 0.547, F 0.859)] [G loss: 0.572] [G acc: 0.078]\n",
      "2654 [D loss: 0.584(R 0.537, F0.631)] [D acc: 0.703(R 0.734, F 0.672)] [G loss: 0.584] [G acc: 0.109]\n",
      "2655 [D loss: 0.547(R 0.554, F0.541)] [D acc: 0.750(R 0.656, F 0.844)] [G loss: 0.547] [G acc: 0.109]\n",
      "2656 [D loss: 0.485(R 0.492, F0.478)] [D acc: 0.758(R 0.703, F 0.812)] [G loss: 0.485] [G acc: 0.078]\n",
      "2657 [D loss: 0.596(R 0.675, F0.516)] [D acc: 0.695(R 0.594, F 0.797)] [G loss: 0.596] [G acc: 0.078]\n",
      "2658 [D loss: 0.591(R 0.558, F0.624)] [D acc: 0.641(R 0.641, F 0.641)] [G loss: 0.591] [G acc: 0.094]\n",
      "2659 [D loss: 0.506(R 0.533, F0.479)] [D acc: 0.719(R 0.578, F 0.859)] [G loss: 0.506] [G acc: 0.109]\n",
      "2660 [D loss: 0.487(R 0.454, F0.519)] [D acc: 0.734(R 0.734, F 0.734)] [G loss: 0.487] [G acc: 0.078]\n",
      "2661 [D loss: 0.573(R 0.696, F0.451)] [D acc: 0.648(R 0.516, F 0.781)] [G loss: 0.573] [G acc: 0.156]\n",
      "2662 [D loss: 0.500(R 0.518, F0.482)] [D acc: 0.773(R 0.750, F 0.797)] [G loss: 0.500] [G acc: 0.156]\n",
      "2663 [D loss: 0.446(R 0.425, F0.466)] [D acc: 0.789(R 0.734, F 0.844)] [G loss: 0.446] [G acc: 0.062]\n",
      "2664 [D loss: 0.553(R 0.564, F0.541)] [D acc: 0.719(R 0.641, F 0.797)] [G loss: 0.553] [G acc: 0.141]\n",
      "2665 [D loss: 0.527(R 0.501, F0.553)] [D acc: 0.750(R 0.719, F 0.781)] [G loss: 0.527] [G acc: 0.062]\n",
      "2666 [D loss: 0.515(R 0.583, F0.447)] [D acc: 0.773(R 0.672, F 0.875)] [G loss: 0.515] [G acc: 0.078]\n",
      "2667 [D loss: 0.572(R 0.605, F0.539)] [D acc: 0.711(R 0.672, F 0.750)] [G loss: 0.572] [G acc: 0.125]\n",
      "2668 [D loss: 0.551(R 0.454, F0.648)] [D acc: 0.727(R 0.766, F 0.688)] [G loss: 0.551] [G acc: 0.125]\n",
      "2669 [D loss: 0.525(R 0.629, F0.420)] [D acc: 0.742(R 0.641, F 0.844)] [G loss: 0.525] [G acc: 0.109]\n",
      "2670 [D loss: 0.560(R 0.661, F0.459)] [D acc: 0.695(R 0.500, F 0.891)] [G loss: 0.560] [G acc: 0.125]\n",
      "2671 [D loss: 0.513(R 0.506, F0.521)] [D acc: 0.734(R 0.672, F 0.797)] [G loss: 0.513] [G acc: 0.062]\n",
      "2672 [D loss: 0.507(R 0.551, F0.463)] [D acc: 0.742(R 0.656, F 0.828)] [G loss: 0.507] [G acc: 0.094]\n",
      "2673 [D loss: 0.529(R 0.578, F0.481)] [D acc: 0.742(R 0.641, F 0.844)] [G loss: 0.529] [G acc: 0.047]\n",
      "2674 [D loss: 0.511(R 0.505, F0.516)] [D acc: 0.742(R 0.688, F 0.797)] [G loss: 0.511] [G acc: 0.109]\n",
      "2675 [D loss: 0.443(R 0.429, F0.458)] [D acc: 0.789(R 0.750, F 0.828)] [G loss: 0.443] [G acc: 0.156]\n",
      "2676 [D loss: 0.516(R 0.544, F0.489)] [D acc: 0.734(R 0.688, F 0.781)] [G loss: 0.516] [G acc: 0.062]\n",
      "2677 [D loss: 0.487(R 0.478, F0.495)] [D acc: 0.758(R 0.719, F 0.797)] [G loss: 0.487] [G acc: 0.062]\n",
      "2678 [D loss: 0.551(R 0.710, F0.392)] [D acc: 0.695(R 0.562, F 0.828)] [G loss: 0.551] [G acc: 0.094]\n",
      "2679 [D loss: 0.535(R 0.467, F0.603)] [D acc: 0.727(R 0.703, F 0.750)] [G loss: 0.535] [G acc: 0.141]\n",
      "2680 [D loss: 0.504(R 0.412, F0.597)] [D acc: 0.773(R 0.797, F 0.750)] [G loss: 0.504] [G acc: 0.172]\n",
      "2681 [D loss: 0.460(R 0.447, F0.472)] [D acc: 0.766(R 0.734, F 0.797)] [G loss: 0.460] [G acc: 0.094]\n",
      "2682 [D loss: 0.603(R 0.667, F0.538)] [D acc: 0.648(R 0.516, F 0.781)] [G loss: 0.603] [G acc: 0.078]\n",
      "2683 [D loss: 0.461(R 0.468, F0.454)] [D acc: 0.805(R 0.766, F 0.844)] [G loss: 0.461] [G acc: 0.078]\n",
      "2684 [D loss: 0.521(R 0.487, F0.555)] [D acc: 0.703(R 0.688, F 0.719)] [G loss: 0.521] [G acc: 0.062]\n",
      "2685 [D loss: 0.468(R 0.507, F0.429)] [D acc: 0.789(R 0.734, F 0.844)] [G loss: 0.468] [G acc: 0.078]\n",
      "2686 [D loss: 0.497(R 0.441, F0.553)] [D acc: 0.781(R 0.766, F 0.797)] [G loss: 0.497] [G acc: 0.094]\n",
      "2687 [D loss: 0.555(R 0.597, F0.514)] [D acc: 0.672(R 0.609, F 0.734)] [G loss: 0.555] [G acc: 0.094]\n",
      "2688 [D loss: 0.553(R 0.518, F0.588)] [D acc: 0.758(R 0.719, F 0.797)] [G loss: 0.553] [G acc: 0.078]\n",
      "2689 [D loss: 0.521(R 0.550, F0.491)] [D acc: 0.742(R 0.641, F 0.844)] [G loss: 0.521] [G acc: 0.031]\n",
      "2690 [D loss: 0.550(R 0.536, F0.564)] [D acc: 0.773(R 0.812, F 0.734)] [G loss: 0.550] [G acc: 0.062]\n",
      "2691 [D loss: 0.470(R 0.527, F0.412)] [D acc: 0.789(R 0.703, F 0.875)] [G loss: 0.470] [G acc: 0.094]\n",
      "2692 [D loss: 0.596(R 0.571, F0.620)] [D acc: 0.719(R 0.688, F 0.750)] [G loss: 0.596] [G acc: 0.109]\n",
      "2693 [D loss: 0.456(R 0.456, F0.455)] [D acc: 0.789(R 0.750, F 0.828)] [G loss: 0.456] [G acc: 0.062]\n",
      "2694 [D loss: 0.521(R 0.649, F0.392)] [D acc: 0.805(R 0.672, F 0.938)] [G loss: 0.521] [G acc: 0.094]\n",
      "2695 [D loss: 0.462(R 0.462, F0.461)] [D acc: 0.797(R 0.750, F 0.844)] [G loss: 0.462] [G acc: 0.109]\n",
      "2696 [D loss: 0.553(R 0.616, F0.491)] [D acc: 0.703(R 0.609, F 0.797)] [G loss: 0.553] [G acc: 0.125]\n",
      "2697 [D loss: 0.584(R 0.553, F0.615)] [D acc: 0.758(R 0.672, F 0.844)] [G loss: 0.584] [G acc: 0.109]\n",
      "2698 [D loss: 0.477(R 0.545, F0.409)] [D acc: 0.758(R 0.641, F 0.875)] [G loss: 0.477] [G acc: 0.125]\n",
      "2699 [D loss: 0.545(R 0.504, F0.587)] [D acc: 0.727(R 0.719, F 0.734)] [G loss: 0.545] [G acc: 0.125]\n",
      "INFO:tensorflow:Assets written to: ram://0388b6bf-c51d-4c42-818d-7840b97e6790/assets\n",
      "INFO:tensorflow:Assets written to: ram://8f74e3c5-9c31-48f4-bac1-4b788c86319d/assets\n",
      "INFO:tensorflow:Assets written to: ram://6b5a0c1b-e66e-44d2-8a01-9f79fe3ecf03/assets\n",
      "2700 [D loss: 0.618(R 0.642, F0.593)] [D acc: 0.664(R 0.547, F 0.781)] [G loss: 0.618] [G acc: 0.078]\n",
      "2701 [D loss: 0.515(R 0.532, F0.498)] [D acc: 0.719(R 0.672, F 0.766)] [G loss: 0.515] [G acc: 0.078]\n",
      "2702 [D loss: 0.510(R 0.582, F0.438)] [D acc: 0.750(R 0.641, F 0.859)] [G loss: 0.510] [G acc: 0.109]\n",
      "2703 [D loss: 0.522(R 0.517, F0.526)] [D acc: 0.750(R 0.688, F 0.812)] [G loss: 0.522] [G acc: 0.094]\n",
      "2704 [D loss: 0.451(R 0.406, F0.495)] [D acc: 0.805(R 0.797, F 0.812)] [G loss: 0.451] [G acc: 0.094]\n",
      "2705 [D loss: 0.539(R 0.609, F0.469)] [D acc: 0.719(R 0.562, F 0.875)] [G loss: 0.539] [G acc: 0.031]\n",
      "2706 [D loss: 0.551(R 0.475, F0.626)] [D acc: 0.766(R 0.734, F 0.797)] [G loss: 0.551] [G acc: 0.125]\n",
      "2707 [D loss: 0.531(R 0.519, F0.544)] [D acc: 0.680(R 0.688, F 0.672)] [G loss: 0.531] [G acc: 0.047]\n",
      "2708 [D loss: 0.586(R 0.590, F0.582)] [D acc: 0.703(R 0.703, F 0.703)] [G loss: 0.586] [G acc: 0.047]\n",
      "2709 [D loss: 0.554(R 0.533, F0.575)] [D acc: 0.750(R 0.703, F 0.797)] [G loss: 0.554] [G acc: 0.062]\n",
      "2710 [D loss: 0.590(R 0.639, F0.540)] [D acc: 0.695(R 0.625, F 0.766)] [G loss: 0.590] [G acc: 0.094]\n",
      "2711 [D loss: 0.547(R 0.607, F0.487)] [D acc: 0.703(R 0.578, F 0.828)] [G loss: 0.547] [G acc: 0.062]\n",
      "2712 [D loss: 0.584(R 0.625, F0.543)] [D acc: 0.695(R 0.609, F 0.781)] [G loss: 0.584] [G acc: 0.062]\n",
      "2713 [D loss: 0.584(R 0.566, F0.601)] [D acc: 0.680(R 0.594, F 0.766)] [G loss: 0.584] [G acc: 0.094]\n",
      "2714 [D loss: 0.526(R 0.522, F0.530)] [D acc: 0.750(R 0.703, F 0.797)] [G loss: 0.526] [G acc: 0.062]\n",
      "2715 [D loss: 0.469(R 0.530, F0.407)] [D acc: 0.805(R 0.703, F 0.906)] [G loss: 0.469] [G acc: 0.109]\n",
      "2716 [D loss: 0.531(R 0.530, F0.532)] [D acc: 0.734(R 0.656, F 0.812)] [G loss: 0.531] [G acc: 0.078]\n",
      "2717 [D loss: 0.451(R 0.508, F0.394)] [D acc: 0.812(R 0.750, F 0.875)] [G loss: 0.451] [G acc: 0.078]\n",
      "2718 [D loss: 0.454(R 0.436, F0.473)] [D acc: 0.820(R 0.781, F 0.859)] [G loss: 0.454] [G acc: 0.047]\n",
      "2719 [D loss: 0.530(R 0.516, F0.544)] [D acc: 0.695(R 0.625, F 0.766)] [G loss: 0.530] [G acc: 0.047]\n",
      "2720 [D loss: 0.631(R 0.578, F0.684)] [D acc: 0.703(R 0.656, F 0.750)] [G loss: 0.631] [G acc: 0.062]\n",
      "2721 [D loss: 0.481(R 0.574, F0.388)] [D acc: 0.773(R 0.656, F 0.891)] [G loss: 0.481] [G acc: 0.078]\n",
      "2722 [D loss: 0.578(R 0.639, F0.517)] [D acc: 0.695(R 0.516, F 0.875)] [G loss: 0.578] [G acc: 0.062]\n",
      "2723 [D loss: 0.566(R 0.562, F0.569)] [D acc: 0.688(R 0.656, F 0.719)] [G loss: 0.566] [G acc: 0.125]\n",
      "2724 [D loss: 0.547(R 0.566, F0.528)] [D acc: 0.672(R 0.609, F 0.734)] [G loss: 0.547] [G acc: 0.047]\n",
      "2725 [D loss: 0.583(R 0.608, F0.559)] [D acc: 0.703(R 0.609, F 0.797)] [G loss: 0.583] [G acc: 0.062]\n",
      "2726 [D loss: 0.436(R 0.515, F0.357)] [D acc: 0.781(R 0.672, F 0.891)] [G loss: 0.436] [G acc: 0.047]\n",
      "2727 [D loss: 0.463(R 0.456, F0.471)] [D acc: 0.758(R 0.750, F 0.766)] [G loss: 0.463] [G acc: 0.062]\n",
      "2728 [D loss: 0.526(R 0.504, F0.549)] [D acc: 0.734(R 0.703, F 0.766)] [G loss: 0.526] [G acc: 0.047]\n",
      "2729 [D loss: 0.633(R 0.615, F0.651)] [D acc: 0.703(R 0.641, F 0.766)] [G loss: 0.633] [G acc: 0.109]\n",
      "2730 [D loss: 0.579(R 0.584, F0.574)] [D acc: 0.656(R 0.594, F 0.719)] [G loss: 0.579] [G acc: 0.047]\n",
      "2731 [D loss: 0.586(R 0.558, F0.615)] [D acc: 0.719(R 0.625, F 0.812)] [G loss: 0.586] [G acc: 0.172]\n",
      "2732 [D loss: 0.506(R 0.581, F0.431)] [D acc: 0.742(R 0.625, F 0.859)] [G loss: 0.506] [G acc: 0.141]\n",
      "2733 [D loss: 0.430(R 0.419, F0.441)] [D acc: 0.828(R 0.781, F 0.875)] [G loss: 0.430] [G acc: 0.062]\n",
      "2734 [D loss: 0.514(R 0.592, F0.436)] [D acc: 0.711(R 0.562, F 0.859)] [G loss: 0.514] [G acc: 0.016]\n",
      "2735 [D loss: 0.522(R 0.499, F0.546)] [D acc: 0.711(R 0.656, F 0.766)] [G loss: 0.522] [G acc: 0.094]\n",
      "2736 [D loss: 0.476(R 0.498, F0.455)] [D acc: 0.758(R 0.672, F 0.844)] [G loss: 0.476] [G acc: 0.125]\n",
      "2737 [D loss: 0.477(R 0.444, F0.509)] [D acc: 0.789(R 0.781, F 0.797)] [G loss: 0.477] [G acc: 0.172]\n",
      "2738 [D loss: 0.467(R 0.504, F0.431)] [D acc: 0.766(R 0.672, F 0.859)] [G loss: 0.467] [G acc: 0.109]\n",
      "2739 [D loss: 0.503(R 0.480, F0.525)] [D acc: 0.773(R 0.703, F 0.844)] [G loss: 0.503] [G acc: 0.125]\n",
      "2740 [D loss: 0.406(R 0.371, F0.441)] [D acc: 0.805(R 0.828, F 0.781)] [G loss: 0.406] [G acc: 0.094]\n",
      "2741 [D loss: 0.528(R 0.531, F0.525)] [D acc: 0.750(R 0.719, F 0.781)] [G loss: 0.528] [G acc: 0.047]\n",
      "2742 [D loss: 0.623(R 0.760, F0.487)] [D acc: 0.680(R 0.578, F 0.781)] [G loss: 0.623] [G acc: 0.047]\n",
      "2743 [D loss: 0.684(R 0.649, F0.719)] [D acc: 0.711(R 0.672, F 0.750)] [G loss: 0.684] [G acc: 0.062]\n",
      "2744 [D loss: 0.590(R 0.745, F0.435)] [D acc: 0.680(R 0.516, F 0.844)] [G loss: 0.590] [G acc: 0.125]\n",
      "2745 [D loss: 0.482(R 0.563, F0.401)] [D acc: 0.781(R 0.672, F 0.891)] [G loss: 0.482] [G acc: 0.125]\n",
      "2746 [D loss: 0.440(R 0.321, F0.559)] [D acc: 0.805(R 0.812, F 0.797)] [G loss: 0.440] [G acc: 0.109]\n",
      "2747 [D loss: 0.486(R 0.530, F0.442)] [D acc: 0.734(R 0.703, F 0.766)] [G loss: 0.486] [G acc: 0.094]\n",
      "2748 [D loss: 0.502(R 0.455, F0.548)] [D acc: 0.734(R 0.719, F 0.750)] [G loss: 0.502] [G acc: 0.078]\n",
      "2749 [D loss: 0.564(R 0.551, F0.576)] [D acc: 0.734(R 0.688, F 0.781)] [G loss: 0.564] [G acc: 0.109]\n",
      "2750 [D loss: 0.558(R 0.621, F0.494)] [D acc: 0.688(R 0.594, F 0.781)] [G loss: 0.558] [G acc: 0.078]\n",
      "2751 [D loss: 0.517(R 0.547, F0.487)] [D acc: 0.758(R 0.719, F 0.797)] [G loss: 0.517] [G acc: 0.109]\n",
      "2752 [D loss: 0.558(R 0.606, F0.511)] [D acc: 0.695(R 0.625, F 0.766)] [G loss: 0.558] [G acc: 0.109]\n",
      "2753 [D loss: 0.552(R 0.617, F0.487)] [D acc: 0.727(R 0.625, F 0.828)] [G loss: 0.552] [G acc: 0.047]\n",
      "2754 [D loss: 0.577(R 0.567, F0.587)] [D acc: 0.695(R 0.688, F 0.703)] [G loss: 0.577] [G acc: 0.062]\n",
      "2755 [D loss: 0.514(R 0.635, F0.393)] [D acc: 0.750(R 0.609, F 0.891)] [G loss: 0.514] [G acc: 0.125]\n",
      "2756 [D loss: 0.466(R 0.462, F0.470)] [D acc: 0.734(R 0.734, F 0.734)] [G loss: 0.466] [G acc: 0.094]\n",
      "2757 [D loss: 0.470(R 0.402, F0.538)] [D acc: 0.750(R 0.766, F 0.734)] [G loss: 0.470] [G acc: 0.047]\n",
      "2758 [D loss: 0.519(R 0.597, F0.442)] [D acc: 0.734(R 0.594, F 0.875)] [G loss: 0.519] [G acc: 0.094]\n",
      "2759 [D loss: 0.519(R 0.411, F0.627)] [D acc: 0.742(R 0.688, F 0.797)] [G loss: 0.519] [G acc: 0.094]\n",
      "2760 [D loss: 0.571(R 0.707, F0.436)] [D acc: 0.695(R 0.500, F 0.891)] [G loss: 0.571] [G acc: 0.094]\n",
      "2761 [D loss: 0.518(R 0.556, F0.479)] [D acc: 0.742(R 0.672, F 0.812)] [G loss: 0.518] [G acc: 0.078]\n",
      "2762 [D loss: 0.554(R 0.605, F0.502)] [D acc: 0.719(R 0.609, F 0.828)] [G loss: 0.554] [G acc: 0.078]\n",
      "2763 [D loss: 0.623(R 0.728, F0.519)] [D acc: 0.648(R 0.547, F 0.750)] [G loss: 0.623] [G acc: 0.094]\n",
      "2764 [D loss: 0.530(R 0.486, F0.575)] [D acc: 0.703(R 0.656, F 0.750)] [G loss: 0.530] [G acc: 0.094]\n",
      "2765 [D loss: 0.581(R 0.586, F0.575)] [D acc: 0.688(R 0.594, F 0.781)] [G loss: 0.581] [G acc: 0.109]\n",
      "2766 [D loss: 0.563(R 0.606, F0.520)] [D acc: 0.711(R 0.656, F 0.766)] [G loss: 0.563] [G acc: 0.078]\n",
      "2767 [D loss: 0.501(R 0.527, F0.476)] [D acc: 0.797(R 0.766, F 0.828)] [G loss: 0.501] [G acc: 0.094]\n",
      "2768 [D loss: 0.577(R 0.638, F0.515)] [D acc: 0.711(R 0.609, F 0.812)] [G loss: 0.577] [G acc: 0.078]\n",
      "2769 [D loss: 0.573(R 0.660, F0.487)] [D acc: 0.680(R 0.594, F 0.766)] [G loss: 0.573] [G acc: 0.141]\n",
      "2770 [D loss: 0.479(R 0.449, F0.509)] [D acc: 0.797(R 0.797, F 0.797)] [G loss: 0.479] [G acc: 0.078]\n",
      "2771 [D loss: 0.543(R 0.590, F0.495)] [D acc: 0.695(R 0.609, F 0.781)] [G loss: 0.543] [G acc: 0.172]\n",
      "2772 [D loss: 0.610(R 0.635, F0.586)] [D acc: 0.680(R 0.594, F 0.766)] [G loss: 0.610] [G acc: 0.141]\n",
      "2773 [D loss: 0.493(R 0.497, F0.489)] [D acc: 0.734(R 0.688, F 0.781)] [G loss: 0.493] [G acc: 0.094]\n",
      "2774 [D loss: 0.588(R 0.595, F0.582)] [D acc: 0.688(R 0.625, F 0.750)] [G loss: 0.588] [G acc: 0.125]\n",
      "2775 [D loss: 0.565(R 0.598, F0.533)] [D acc: 0.742(R 0.656, F 0.828)] [G loss: 0.565] [G acc: 0.125]\n",
      "2776 [D loss: 0.450(R 0.470, F0.431)] [D acc: 0.820(R 0.781, F 0.859)] [G loss: 0.450] [G acc: 0.125]\n",
      "2777 [D loss: 0.554(R 0.563, F0.546)] [D acc: 0.734(R 0.719, F 0.750)] [G loss: 0.554] [G acc: 0.156]\n",
      "2778 [D loss: 0.632(R 0.641, F0.624)] [D acc: 0.727(R 0.672, F 0.781)] [G loss: 0.632] [G acc: 0.031]\n",
      "2779 [D loss: 0.491(R 0.474, F0.508)] [D acc: 0.781(R 0.734, F 0.828)] [G loss: 0.491] [G acc: 0.141]\n",
      "2780 [D loss: 0.565(R 0.624, F0.506)] [D acc: 0.742(R 0.641, F 0.844)] [G loss: 0.565] [G acc: 0.094]\n",
      "2781 [D loss: 0.608(R 0.645, F0.571)] [D acc: 0.672(R 0.594, F 0.750)] [G loss: 0.608] [G acc: 0.125]\n",
      "2782 [D loss: 0.554(R 0.676, F0.432)] [D acc: 0.711(R 0.547, F 0.875)] [G loss: 0.554] [G acc: 0.109]\n",
      "2783 [D loss: 0.551(R 0.564, F0.537)] [D acc: 0.688(R 0.594, F 0.781)] [G loss: 0.551] [G acc: 0.109]\n",
      "2784 [D loss: 0.485(R 0.449, F0.522)] [D acc: 0.781(R 0.750, F 0.812)] [G loss: 0.485] [G acc: 0.109]\n",
      "2785 [D loss: 0.494(R 0.522, F0.466)] [D acc: 0.758(R 0.656, F 0.859)] [G loss: 0.494] [G acc: 0.141]\n",
      "2786 [D loss: 0.570(R 0.586, F0.554)] [D acc: 0.711(R 0.641, F 0.781)] [G loss: 0.570] [G acc: 0.047]\n",
      "2787 [D loss: 0.505(R 0.487, F0.523)] [D acc: 0.727(R 0.703, F 0.750)] [G loss: 0.505] [G acc: 0.125]\n",
      "2788 [D loss: 0.604(R 0.704, F0.504)] [D acc: 0.711(R 0.625, F 0.797)] [G loss: 0.604] [G acc: 0.109]\n",
      "2789 [D loss: 0.594(R 0.605, F0.582)] [D acc: 0.648(R 0.625, F 0.672)] [G loss: 0.594] [G acc: 0.109]\n",
      "2790 [D loss: 0.544(R 0.550, F0.537)] [D acc: 0.695(R 0.641, F 0.750)] [G loss: 0.544] [G acc: 0.047]\n",
      "2791 [D loss: 0.550(R 0.594, F0.507)] [D acc: 0.727(R 0.641, F 0.812)] [G loss: 0.550] [G acc: 0.172]\n",
      "2792 [D loss: 0.563(R 0.448, F0.678)] [D acc: 0.703(R 0.734, F 0.672)] [G loss: 0.563] [G acc: 0.109]\n",
      "2793 [D loss: 0.585(R 0.600, F0.570)] [D acc: 0.719(R 0.672, F 0.766)] [G loss: 0.585] [G acc: 0.094]\n",
      "2794 [D loss: 0.545(R 0.569, F0.521)] [D acc: 0.750(R 0.719, F 0.781)] [G loss: 0.545] [G acc: 0.141]\n",
      "2795 [D loss: 0.515(R 0.551, F0.479)] [D acc: 0.766(R 0.672, F 0.859)] [G loss: 0.515] [G acc: 0.109]\n",
      "2796 [D loss: 0.487(R 0.544, F0.429)] [D acc: 0.719(R 0.625, F 0.812)] [G loss: 0.487] [G acc: 0.109]\n",
      "2797 [D loss: 0.578(R 0.540, F0.616)] [D acc: 0.750(R 0.750, F 0.750)] [G loss: 0.578] [G acc: 0.156]\n",
      "2798 [D loss: 0.511(R 0.466, F0.556)] [D acc: 0.758(R 0.703, F 0.812)] [G loss: 0.511] [G acc: 0.062]\n",
      "2799 [D loss: 0.500(R 0.515, F0.484)] [D acc: 0.766(R 0.688, F 0.844)] [G loss: 0.500] [G acc: 0.125]\n",
      "INFO:tensorflow:Assets written to: ram://78778dc4-c497-45a9-b750-48424ba3421e/assets\n",
      "INFO:tensorflow:Assets written to: ram://7fae94c8-ae0d-496d-a5ed-fc226cc2f2d6/assets\n",
      "INFO:tensorflow:Assets written to: ram://5e6942de-3630-4a5b-a798-a172cb04136e/assets\n",
      "2800 [D loss: 0.561(R 0.563, F0.559)] [D acc: 0.711(R 0.688, F 0.734)] [G loss: 0.561] [G acc: 0.109]\n",
      "2801 [D loss: 0.461(R 0.482, F0.441)] [D acc: 0.805(R 0.734, F 0.875)] [G loss: 0.461] [G acc: 0.156]\n",
      "2802 [D loss: 0.497(R 0.478, F0.517)] [D acc: 0.773(R 0.734, F 0.812)] [G loss: 0.497] [G acc: 0.109]\n",
      "2803 [D loss: 0.480(R 0.457, F0.503)] [D acc: 0.758(R 0.750, F 0.766)] [G loss: 0.480] [G acc: 0.141]\n",
      "2804 [D loss: 0.510(R 0.472, F0.548)] [D acc: 0.742(R 0.719, F 0.766)] [G loss: 0.510] [G acc: 0.062]\n",
      "2805 [D loss: 0.559(R 0.503, F0.615)] [D acc: 0.742(R 0.672, F 0.812)] [G loss: 0.559] [G acc: 0.156]\n",
      "2806 [D loss: 0.442(R 0.468, F0.416)] [D acc: 0.758(R 0.641, F 0.875)] [G loss: 0.442] [G acc: 0.031]\n",
      "2807 [D loss: 0.656(R 0.706, F0.605)] [D acc: 0.680(R 0.609, F 0.750)] [G loss: 0.656] [G acc: 0.047]\n",
      "2808 [D loss: 0.622(R 0.720, F0.523)] [D acc: 0.703(R 0.562, F 0.844)] [G loss: 0.622] [G acc: 0.078]\n",
      "2809 [D loss: 0.533(R 0.521, F0.545)] [D acc: 0.727(R 0.703, F 0.750)] [G loss: 0.533] [G acc: 0.047]\n",
      "2810 [D loss: 0.538(R 0.635, F0.442)] [D acc: 0.750(R 0.625, F 0.875)] [G loss: 0.538] [G acc: 0.062]\n",
      "2811 [D loss: 0.533(R 0.588, F0.477)] [D acc: 0.727(R 0.594, F 0.859)] [G loss: 0.533] [G acc: 0.141]\n",
      "2812 [D loss: 0.583(R 0.470, F0.695)] [D acc: 0.664(R 0.656, F 0.672)] [G loss: 0.583] [G acc: 0.141]\n",
      "2813 [D loss: 0.658(R 0.666, F0.649)] [D acc: 0.695(R 0.562, F 0.828)] [G loss: 0.658] [G acc: 0.156]\n",
      "2814 [D loss: 0.501(R 0.552, F0.451)] [D acc: 0.750(R 0.641, F 0.859)] [G loss: 0.501] [G acc: 0.094]\n",
      "2815 [D loss: 0.527(R 0.555, F0.499)] [D acc: 0.758(R 0.703, F 0.812)] [G loss: 0.527] [G acc: 0.062]\n",
      "2816 [D loss: 0.426(R 0.362, F0.490)] [D acc: 0.789(R 0.781, F 0.797)] [G loss: 0.426] [G acc: 0.047]\n",
      "2817 [D loss: 0.483(R 0.548, F0.417)] [D acc: 0.758(R 0.625, F 0.891)] [G loss: 0.483] [G acc: 0.062]\n",
      "2818 [D loss: 0.568(R 0.496, F0.639)] [D acc: 0.766(R 0.750, F 0.781)] [G loss: 0.568] [G acc: 0.062]\n",
      "2819 [D loss: 0.565(R 0.657, F0.473)] [D acc: 0.711(R 0.578, F 0.844)] [G loss: 0.565] [G acc: 0.094]\n",
      "2820 [D loss: 0.508(R 0.520, F0.496)] [D acc: 0.695(R 0.625, F 0.766)] [G loss: 0.508] [G acc: 0.094]\n",
      "2821 [D loss: 0.657(R 0.616, F0.699)] [D acc: 0.672(R 0.609, F 0.734)] [G loss: 0.657] [G acc: 0.094]\n",
      "2822 [D loss: 0.477(R 0.444, F0.509)] [D acc: 0.789(R 0.766, F 0.812)] [G loss: 0.477] [G acc: 0.047]\n",
      "2823 [D loss: 0.572(R 0.599, F0.546)] [D acc: 0.688(R 0.609, F 0.766)] [G loss: 0.572] [G acc: 0.078]\n",
      "2824 [D loss: 0.503(R 0.472, F0.534)] [D acc: 0.742(R 0.703, F 0.781)] [G loss: 0.503] [G acc: 0.062]\n",
      "2825 [D loss: 0.511(R 0.570, F0.453)] [D acc: 0.742(R 0.594, F 0.891)] [G loss: 0.511] [G acc: 0.125]\n",
      "2826 [D loss: 0.572(R 0.552, F0.591)] [D acc: 0.680(R 0.609, F 0.750)] [G loss: 0.572] [G acc: 0.109]\n",
      "2827 [D loss: 0.556(R 0.611, F0.502)] [D acc: 0.688(R 0.578, F 0.797)] [G loss: 0.556] [G acc: 0.109]\n",
      "2828 [D loss: 0.519(R 0.597, F0.442)] [D acc: 0.773(R 0.672, F 0.875)] [G loss: 0.519] [G acc: 0.109]\n",
      "2829 [D loss: 0.450(R 0.446, F0.454)] [D acc: 0.766(R 0.719, F 0.812)] [G loss: 0.450] [G acc: 0.047]\n",
      "2830 [D loss: 0.509(R 0.466, F0.551)] [D acc: 0.766(R 0.750, F 0.781)] [G loss: 0.509] [G acc: 0.094]\n",
      "2831 [D loss: 0.502(R 0.440, F0.563)] [D acc: 0.797(R 0.750, F 0.844)] [G loss: 0.502] [G acc: 0.062]\n",
      "2832 [D loss: 0.529(R 0.601, F0.457)] [D acc: 0.758(R 0.688, F 0.828)] [G loss: 0.529] [G acc: 0.078]\n",
      "2833 [D loss: 0.513(R 0.626, F0.401)] [D acc: 0.781(R 0.656, F 0.906)] [G loss: 0.513] [G acc: 0.031]\n",
      "2834 [D loss: 0.597(R 0.480, F0.714)] [D acc: 0.688(R 0.641, F 0.734)] [G loss: 0.597] [G acc: 0.016]\n",
      "2835 [D loss: 0.573(R 0.702, F0.445)] [D acc: 0.703(R 0.547, F 0.859)] [G loss: 0.573] [G acc: 0.188]\n",
      "2836 [D loss: 0.519(R 0.569, F0.469)] [D acc: 0.719(R 0.594, F 0.844)] [G loss: 0.519] [G acc: 0.109]\n",
      "2837 [D loss: 0.478(R 0.488, F0.467)] [D acc: 0.758(R 0.656, F 0.859)] [G loss: 0.478] [G acc: 0.141]\n",
      "2838 [D loss: 0.594(R 0.618, F0.570)] [D acc: 0.703(R 0.641, F 0.766)] [G loss: 0.594] [G acc: 0.094]\n",
      "2839 [D loss: 0.557(R 0.595, F0.519)] [D acc: 0.695(R 0.625, F 0.766)] [G loss: 0.557] [G acc: 0.062]\n",
      "2840 [D loss: 0.545(R 0.516, F0.575)] [D acc: 0.727(R 0.641, F 0.812)] [G loss: 0.545] [G acc: 0.125]\n",
      "2841 [D loss: 0.561(R 0.671, F0.450)] [D acc: 0.703(R 0.578, F 0.828)] [G loss: 0.561] [G acc: 0.047]\n",
      "2842 [D loss: 0.515(R 0.593, F0.437)] [D acc: 0.727(R 0.594, F 0.859)] [G loss: 0.515] [G acc: 0.062]\n",
      "2843 [D loss: 0.458(R 0.447, F0.469)] [D acc: 0.789(R 0.750, F 0.828)] [G loss: 0.458] [G acc: 0.078]\n",
      "2844 [D loss: 0.498(R 0.513, F0.483)] [D acc: 0.773(R 0.719, F 0.828)] [G loss: 0.498] [G acc: 0.141]\n",
      "2845 [D loss: 0.477(R 0.506, F0.448)] [D acc: 0.781(R 0.719, F 0.844)] [G loss: 0.477] [G acc: 0.156]\n",
      "2846 [D loss: 0.501(R 0.430, F0.572)] [D acc: 0.734(R 0.719, F 0.750)] [G loss: 0.501] [G acc: 0.219]\n",
      "2847 [D loss: 0.630(R 0.688, F0.572)] [D acc: 0.680(R 0.578, F 0.781)] [G loss: 0.630] [G acc: 0.125]\n",
      "2848 [D loss: 0.555(R 0.508, F0.602)] [D acc: 0.695(R 0.656, F 0.734)] [G loss: 0.555] [G acc: 0.094]\n",
      "2849 [D loss: 0.653(R 0.561, F0.744)] [D acc: 0.648(R 0.625, F 0.672)] [G loss: 0.653] [G acc: 0.094]\n",
      "2850 [D loss: 0.504(R 0.542, F0.466)] [D acc: 0.766(R 0.703, F 0.828)] [G loss: 0.504] [G acc: 0.109]\n",
      "2851 [D loss: 0.468(R 0.510, F0.426)] [D acc: 0.766(R 0.719, F 0.812)] [G loss: 0.468] [G acc: 0.141]\n",
      "2852 [D loss: 0.532(R 0.574, F0.490)] [D acc: 0.750(R 0.672, F 0.828)] [G loss: 0.532] [G acc: 0.078]\n",
      "2853 [D loss: 0.549(R 0.532, F0.566)] [D acc: 0.680(R 0.641, F 0.719)] [G loss: 0.549] [G acc: 0.062]\n",
      "2854 [D loss: 0.479(R 0.547, F0.412)] [D acc: 0.773(R 0.672, F 0.875)] [G loss: 0.479] [G acc: 0.188]\n",
      "2855 [D loss: 0.620(R 0.584, F0.656)] [D acc: 0.664(R 0.656, F 0.672)] [G loss: 0.620] [G acc: 0.125]\n",
      "2856 [D loss: 0.541(R 0.494, F0.589)] [D acc: 0.750(R 0.688, F 0.812)] [G loss: 0.541] [G acc: 0.016]\n",
      "2857 [D loss: 0.559(R 0.689, F0.429)] [D acc: 0.688(R 0.547, F 0.828)] [G loss: 0.559] [G acc: 0.078]\n",
      "2858 [D loss: 0.494(R 0.547, F0.441)] [D acc: 0.742(R 0.656, F 0.828)] [G loss: 0.494] [G acc: 0.078]\n",
      "2859 [D loss: 0.535(R 0.452, F0.617)] [D acc: 0.727(R 0.672, F 0.781)] [G loss: 0.535] [G acc: 0.094]\n",
      "2860 [D loss: 0.454(R 0.489, F0.418)] [D acc: 0.781(R 0.703, F 0.859)] [G loss: 0.454] [G acc: 0.125]\n",
      "2861 [D loss: 0.424(R 0.491, F0.356)] [D acc: 0.781(R 0.688, F 0.875)] [G loss: 0.424] [G acc: 0.141]\n",
      "2862 [D loss: 0.663(R 0.566, F0.761)] [D acc: 0.688(R 0.703, F 0.672)] [G loss: 0.663] [G acc: 0.078]\n",
      "2863 [D loss: 0.560(R 0.662, F0.459)] [D acc: 0.719(R 0.578, F 0.859)] [G loss: 0.560] [G acc: 0.109]\n",
      "2864 [D loss: 0.553(R 0.518, F0.587)] [D acc: 0.758(R 0.703, F 0.812)] [G loss: 0.553] [G acc: 0.062]\n",
      "2865 [D loss: 0.505(R 0.550, F0.460)] [D acc: 0.766(R 0.703, F 0.828)] [G loss: 0.505] [G acc: 0.094]\n",
      "2866 [D loss: 0.493(R 0.478, F0.507)] [D acc: 0.773(R 0.719, F 0.828)] [G loss: 0.493] [G acc: 0.141]\n",
      "2867 [D loss: 0.558(R 0.641, F0.474)] [D acc: 0.680(R 0.594, F 0.766)] [G loss: 0.558] [G acc: 0.094]\n",
      "2868 [D loss: 0.506(R 0.407, F0.605)] [D acc: 0.727(R 0.766, F 0.688)] [G loss: 0.506] [G acc: 0.078]\n",
      "2869 [D loss: 0.518(R 0.544, F0.492)] [D acc: 0.750(R 0.656, F 0.844)] [G loss: 0.518] [G acc: 0.094]\n",
      "2870 [D loss: 0.463(R 0.508, F0.417)] [D acc: 0.797(R 0.719, F 0.875)] [G loss: 0.463] [G acc: 0.125]\n",
      "2871 [D loss: 0.445(R 0.329, F0.561)] [D acc: 0.766(R 0.812, F 0.719)] [G loss: 0.445] [G acc: 0.094]\n",
      "2872 [D loss: 0.598(R 0.635, F0.561)] [D acc: 0.680(R 0.625, F 0.734)] [G loss: 0.598] [G acc: 0.078]\n",
      "2873 [D loss: 0.571(R 0.703, F0.440)] [D acc: 0.664(R 0.531, F 0.797)] [G loss: 0.571] [G acc: 0.141]\n",
      "2874 [D loss: 0.468(R 0.438, F0.498)] [D acc: 0.781(R 0.734, F 0.828)] [G loss: 0.468] [G acc: 0.094]\n",
      "2875 [D loss: 0.475(R 0.464, F0.485)] [D acc: 0.773(R 0.734, F 0.812)] [G loss: 0.475] [G acc: 0.062]\n",
      "2876 [D loss: 0.484(R 0.531, F0.436)] [D acc: 0.766(R 0.656, F 0.875)] [G loss: 0.484] [G acc: 0.031]\n",
      "2877 [D loss: 0.597(R 0.506, F0.687)] [D acc: 0.734(R 0.719, F 0.750)] [G loss: 0.597] [G acc: 0.109]\n",
      "2878 [D loss: 0.633(R 0.736, F0.529)] [D acc: 0.664(R 0.531, F 0.797)] [G loss: 0.633] [G acc: 0.156]\n",
      "2879 [D loss: 0.529(R 0.563, F0.495)] [D acc: 0.742(R 0.688, F 0.797)] [G loss: 0.529] [G acc: 0.125]\n",
      "2880 [D loss: 0.526(R 0.555, F0.497)] [D acc: 0.781(R 0.688, F 0.875)] [G loss: 0.526] [G acc: 0.109]\n",
      "2881 [D loss: 0.524(R 0.464, F0.584)] [D acc: 0.766(R 0.766, F 0.766)] [G loss: 0.524] [G acc: 0.109]\n",
      "2882 [D loss: 0.555(R 0.658, F0.453)] [D acc: 0.672(R 0.531, F 0.812)] [G loss: 0.555] [G acc: 0.125]\n",
      "2883 [D loss: 0.541(R 0.571, F0.512)] [D acc: 0.719(R 0.625, F 0.812)] [G loss: 0.541] [G acc: 0.047]\n",
      "2884 [D loss: 0.479(R 0.496, F0.461)] [D acc: 0.789(R 0.703, F 0.875)] [G loss: 0.479] [G acc: 0.109]\n",
      "2885 [D loss: 0.551(R 0.542, F0.559)] [D acc: 0.758(R 0.688, F 0.828)] [G loss: 0.551] [G acc: 0.141]\n",
      "2886 [D loss: 0.504(R 0.519, F0.490)] [D acc: 0.789(R 0.703, F 0.875)] [G loss: 0.504] [G acc: 0.094]\n",
      "2887 [D loss: 0.610(R 0.631, F0.588)] [D acc: 0.648(R 0.578, F 0.719)] [G loss: 0.610] [G acc: 0.125]\n",
      "2888 [D loss: 0.595(R 0.662, F0.529)] [D acc: 0.719(R 0.625, F 0.812)] [G loss: 0.595] [G acc: 0.031]\n",
      "2889 [D loss: 0.581(R 0.621, F0.542)] [D acc: 0.711(R 0.656, F 0.766)] [G loss: 0.581] [G acc: 0.078]\n",
      "2890 [D loss: 0.485(R 0.471, F0.500)] [D acc: 0.781(R 0.750, F 0.812)] [G loss: 0.485] [G acc: 0.109]\n",
      "2891 [D loss: 0.592(R 0.663, F0.521)] [D acc: 0.672(R 0.547, F 0.797)] [G loss: 0.592] [G acc: 0.094]\n",
      "2892 [D loss: 0.548(R 0.650, F0.447)] [D acc: 0.727(R 0.578, F 0.875)] [G loss: 0.548] [G acc: 0.125]\n",
      "2893 [D loss: 0.552(R 0.618, F0.486)] [D acc: 0.734(R 0.641, F 0.828)] [G loss: 0.552] [G acc: 0.109]\n",
      "2894 [D loss: 0.529(R 0.562, F0.497)] [D acc: 0.727(R 0.641, F 0.812)] [G loss: 0.529] [G acc: 0.078]\n",
      "2895 [D loss: 0.502(R 0.498, F0.506)] [D acc: 0.758(R 0.734, F 0.781)] [G loss: 0.502] [G acc: 0.156]\n",
      "2896 [D loss: 0.525(R 0.539, F0.510)] [D acc: 0.719(R 0.672, F 0.766)] [G loss: 0.525] [G acc: 0.094]\n",
      "2897 [D loss: 0.465(R 0.436, F0.493)] [D acc: 0.805(R 0.766, F 0.844)] [G loss: 0.465] [G acc: 0.078]\n",
      "2898 [D loss: 0.560(R 0.701, F0.420)] [D acc: 0.727(R 0.562, F 0.891)] [G loss: 0.560] [G acc: 0.141]\n",
      "2899 [D loss: 0.597(R 0.695, F0.498)] [D acc: 0.727(R 0.594, F 0.859)] [G loss: 0.597] [G acc: 0.062]\n",
      "INFO:tensorflow:Assets written to: ram://7b528fc6-96b7-4fd3-acc7-559125d795db/assets\n",
      "INFO:tensorflow:Assets written to: ram://c8f37279-e807-41a7-9938-034233f883e1/assets\n",
      "INFO:tensorflow:Assets written to: ram://333ebcb8-7e2a-4f86-8705-c67147cd016a/assets\n",
      "2900 [D loss: 0.590(R 0.532, F0.648)] [D acc: 0.680(R 0.672, F 0.688)] [G loss: 0.590] [G acc: 0.156]\n",
      "2901 [D loss: 0.574(R 0.661, F0.487)] [D acc: 0.688(R 0.531, F 0.844)] [G loss: 0.574] [G acc: 0.062]\n",
      "2902 [D loss: 0.560(R 0.664, F0.456)] [D acc: 0.695(R 0.531, F 0.859)] [G loss: 0.560] [G acc: 0.078]\n",
      "2903 [D loss: 0.560(R 0.468, F0.651)] [D acc: 0.727(R 0.734, F 0.719)] [G loss: 0.560] [G acc: 0.094]\n",
      "2904 [D loss: 0.599(R 0.595, F0.603)] [D acc: 0.672(R 0.656, F 0.688)] [G loss: 0.599] [G acc: 0.062]\n",
      "2905 [D loss: 0.525(R 0.587, F0.462)] [D acc: 0.711(R 0.594, F 0.828)] [G loss: 0.525] [G acc: 0.078]\n",
      "2906 [D loss: 0.585(R 0.438, F0.731)] [D acc: 0.711(R 0.734, F 0.688)] [G loss: 0.585] [G acc: 0.094]\n",
      "2907 [D loss: 0.514(R 0.467, F0.561)] [D acc: 0.734(R 0.734, F 0.734)] [G loss: 0.514] [G acc: 0.047]\n",
      "2908 [D loss: 0.538(R 0.549, F0.528)] [D acc: 0.688(R 0.656, F 0.719)] [G loss: 0.538] [G acc: 0.172]\n",
      "2909 [D loss: 0.519(R 0.549, F0.489)] [D acc: 0.766(R 0.703, F 0.828)] [G loss: 0.519] [G acc: 0.094]\n",
      "2910 [D loss: 0.470(R 0.504, F0.437)] [D acc: 0.781(R 0.688, F 0.875)] [G loss: 0.470] [G acc: 0.125]\n",
      "2911 [D loss: 0.639(R 0.613, F0.666)] [D acc: 0.672(R 0.641, F 0.703)] [G loss: 0.639] [G acc: 0.047]\n",
      "2912 [D loss: 0.548(R 0.600, F0.496)] [D acc: 0.703(R 0.594, F 0.812)] [G loss: 0.548] [G acc: 0.094]\n",
      "2913 [D loss: 0.553(R 0.569, F0.536)] [D acc: 0.773(R 0.734, F 0.812)] [G loss: 0.553] [G acc: 0.141]\n",
      "2914 [D loss: 0.591(R 0.609, F0.573)] [D acc: 0.672(R 0.625, F 0.719)] [G loss: 0.591] [G acc: 0.094]\n",
      "2915 [D loss: 0.535(R 0.536, F0.535)] [D acc: 0.734(R 0.688, F 0.781)] [G loss: 0.535] [G acc: 0.156]\n",
      "2916 [D loss: 0.533(R 0.585, F0.481)] [D acc: 0.711(R 0.641, F 0.781)] [G loss: 0.533] [G acc: 0.094]\n",
      "2917 [D loss: 0.568(R 0.546, F0.590)] [D acc: 0.750(R 0.672, F 0.828)] [G loss: 0.568] [G acc: 0.109]\n",
      "2918 [D loss: 0.556(R 0.425, F0.686)] [D acc: 0.742(R 0.781, F 0.703)] [G loss: 0.556] [G acc: 0.078]\n",
      "2919 [D loss: 0.534(R 0.555, F0.512)] [D acc: 0.727(R 0.656, F 0.797)] [G loss: 0.534] [G acc: 0.094]\n",
      "2920 [D loss: 0.535(R 0.546, F0.524)] [D acc: 0.742(R 0.656, F 0.828)] [G loss: 0.535] [G acc: 0.062]\n",
      "2921 [D loss: 0.519(R 0.639, F0.400)] [D acc: 0.766(R 0.656, F 0.875)] [G loss: 0.519] [G acc: 0.109]\n",
      "2922 [D loss: 0.502(R 0.508, F0.497)] [D acc: 0.781(R 0.719, F 0.844)] [G loss: 0.502] [G acc: 0.125]\n",
      "2923 [D loss: 0.481(R 0.455, F0.508)] [D acc: 0.734(R 0.688, F 0.781)] [G loss: 0.481] [G acc: 0.125]\n",
      "2924 [D loss: 0.498(R 0.466, F0.530)] [D acc: 0.719(R 0.719, F 0.719)] [G loss: 0.498] [G acc: 0.125]\n",
      "2925 [D loss: 0.532(R 0.488, F0.577)] [D acc: 0.734(R 0.703, F 0.766)] [G loss: 0.532] [G acc: 0.125]\n",
      "2926 [D loss: 0.520(R 0.494, F0.545)] [D acc: 0.742(R 0.719, F 0.766)] [G loss: 0.520] [G acc: 0.078]\n",
      "2927 [D loss: 0.546(R 0.516, F0.577)] [D acc: 0.727(R 0.703, F 0.750)] [G loss: 0.546] [G acc: 0.062]\n",
      "2928 [D loss: 0.551(R 0.450, F0.651)] [D acc: 0.727(R 0.734, F 0.719)] [G loss: 0.551] [G acc: 0.031]\n",
      "2929 [D loss: 0.510(R 0.650, F0.370)] [D acc: 0.766(R 0.594, F 0.938)] [G loss: 0.510] [G acc: 0.062]\n",
      "2930 [D loss: 0.462(R 0.471, F0.452)] [D acc: 0.766(R 0.719, F 0.812)] [G loss: 0.462] [G acc: 0.047]\n",
      "2931 [D loss: 0.469(R 0.501, F0.437)] [D acc: 0.758(R 0.688, F 0.828)] [G loss: 0.469] [G acc: 0.078]\n",
      "2932 [D loss: 0.466(R 0.541, F0.391)] [D acc: 0.781(R 0.719, F 0.844)] [G loss: 0.466] [G acc: 0.062]\n",
      "2933 [D loss: 0.517(R 0.481, F0.553)] [D acc: 0.742(R 0.734, F 0.750)] [G loss: 0.517] [G acc: 0.062]\n",
      "2934 [D loss: 0.547(R 0.472, F0.622)] [D acc: 0.711(R 0.688, F 0.734)] [G loss: 0.547] [G acc: 0.062]\n",
      "2935 [D loss: 0.510(R 0.619, F0.401)] [D acc: 0.773(R 0.641, F 0.906)] [G loss: 0.510] [G acc: 0.047]\n",
      "2936 [D loss: 0.417(R 0.390, F0.444)] [D acc: 0.805(R 0.797, F 0.812)] [G loss: 0.417] [G acc: 0.125]\n",
      "2937 [D loss: 0.498(R 0.536, F0.460)] [D acc: 0.758(R 0.703, F 0.812)] [G loss: 0.498] [G acc: 0.062]\n",
      "2938 [D loss: 0.543(R 0.512, F0.573)] [D acc: 0.734(R 0.688, F 0.781)] [G loss: 0.543] [G acc: 0.062]\n",
      "2939 [D loss: 0.501(R 0.504, F0.498)] [D acc: 0.727(R 0.672, F 0.781)] [G loss: 0.501] [G acc: 0.078]\n",
      "2940 [D loss: 0.454(R 0.548, F0.360)] [D acc: 0.758(R 0.641, F 0.875)] [G loss: 0.454] [G acc: 0.062]\n",
      "2941 [D loss: 0.524(R 0.460, F0.589)] [D acc: 0.750(R 0.750, F 0.750)] [G loss: 0.524] [G acc: 0.125]\n",
      "2942 [D loss: 0.494(R 0.557, F0.430)] [D acc: 0.742(R 0.672, F 0.812)] [G loss: 0.494] [G acc: 0.062]\n",
      "2943 [D loss: 0.517(R 0.552, F0.482)] [D acc: 0.727(R 0.625, F 0.828)] [G loss: 0.517] [G acc: 0.094]\n",
      "2944 [D loss: 0.551(R 0.533, F0.569)] [D acc: 0.703(R 0.641, F 0.766)] [G loss: 0.551] [G acc: 0.047]\n",
      "2945 [D loss: 0.505(R 0.615, F0.395)] [D acc: 0.773(R 0.641, F 0.906)] [G loss: 0.505] [G acc: 0.078]\n",
      "2946 [D loss: 0.557(R 0.510, F0.604)] [D acc: 0.703(R 0.672, F 0.734)] [G loss: 0.557] [G acc: 0.062]\n",
      "2947 [D loss: 0.419(R 0.440, F0.397)] [D acc: 0.828(R 0.766, F 0.891)] [G loss: 0.419] [G acc: 0.109]\n",
      "2948 [D loss: 0.529(R 0.566, F0.492)] [D acc: 0.727(R 0.656, F 0.797)] [G loss: 0.529] [G acc: 0.125]\n",
      "2949 [D loss: 0.503(R 0.484, F0.522)] [D acc: 0.734(R 0.672, F 0.797)] [G loss: 0.503] [G acc: 0.188]\n",
      "2950 [D loss: 0.534(R 0.548, F0.521)] [D acc: 0.758(R 0.703, F 0.812)] [G loss: 0.534] [G acc: 0.156]\n",
      "2951 [D loss: 0.486(R 0.435, F0.536)] [D acc: 0.773(R 0.766, F 0.781)] [G loss: 0.486] [G acc: 0.047]\n",
      "2952 [D loss: 0.505(R 0.526, F0.483)] [D acc: 0.742(R 0.672, F 0.812)] [G loss: 0.505] [G acc: 0.094]\n",
      "2953 [D loss: 0.556(R 0.559, F0.554)] [D acc: 0.703(R 0.688, F 0.719)] [G loss: 0.556] [G acc: 0.062]\n",
      "2954 [D loss: 0.521(R 0.529, F0.514)] [D acc: 0.773(R 0.734, F 0.812)] [G loss: 0.521] [G acc: 0.109]\n",
      "2955 [D loss: 0.432(R 0.407, F0.456)] [D acc: 0.797(R 0.734, F 0.859)] [G loss: 0.432] [G acc: 0.094]\n",
      "2956 [D loss: 0.493(R 0.521, F0.465)] [D acc: 0.789(R 0.719, F 0.859)] [G loss: 0.493] [G acc: 0.062]\n",
      "2957 [D loss: 0.721(R 0.811, F0.631)] [D acc: 0.625(R 0.562, F 0.688)] [G loss: 0.721] [G acc: 0.094]\n",
      "2958 [D loss: 0.527(R 0.534, F0.520)] [D acc: 0.719(R 0.641, F 0.797)] [G loss: 0.527] [G acc: 0.141]\n",
      "2959 [D loss: 0.466(R 0.419, F0.513)] [D acc: 0.766(R 0.750, F 0.781)] [G loss: 0.466] [G acc: 0.062]\n",
      "2960 [D loss: 0.453(R 0.550, F0.355)] [D acc: 0.781(R 0.656, F 0.906)] [G loss: 0.453] [G acc: 0.156]\n",
      "2961 [D loss: 0.440(R 0.401, F0.480)] [D acc: 0.789(R 0.766, F 0.812)] [G loss: 0.440] [G acc: 0.141]\n",
      "2962 [D loss: 0.545(R 0.563, F0.526)] [D acc: 0.672(R 0.625, F 0.719)] [G loss: 0.545] [G acc: 0.062]\n",
      "2963 [D loss: 0.526(R 0.476, F0.577)] [D acc: 0.773(R 0.734, F 0.812)] [G loss: 0.526] [G acc: 0.047]\n",
      "2964 [D loss: 0.602(R 0.629, F0.575)] [D acc: 0.695(R 0.609, F 0.781)] [G loss: 0.602] [G acc: 0.062]\n",
      "2965 [D loss: 0.494(R 0.563, F0.425)] [D acc: 0.773(R 0.656, F 0.891)] [G loss: 0.494] [G acc: 0.109]\n",
      "2966 [D loss: 0.569(R 0.627, F0.512)] [D acc: 0.750(R 0.672, F 0.828)] [G loss: 0.569] [G acc: 0.141]\n",
      "2967 [D loss: 0.484(R 0.454, F0.513)] [D acc: 0.797(R 0.750, F 0.844)] [G loss: 0.484] [G acc: 0.094]\n",
      "2968 [D loss: 0.504(R 0.467, F0.540)] [D acc: 0.797(R 0.750, F 0.844)] [G loss: 0.504] [G acc: 0.109]\n",
      "2969 [D loss: 0.587(R 0.637, F0.537)] [D acc: 0.688(R 0.578, F 0.797)] [G loss: 0.587] [G acc: 0.062]\n",
      "2970 [D loss: 0.586(R 0.586, F0.587)] [D acc: 0.703(R 0.641, F 0.766)] [G loss: 0.586] [G acc: 0.078]\n",
      "2971 [D loss: 0.525(R 0.595, F0.455)] [D acc: 0.750(R 0.641, F 0.859)] [G loss: 0.525] [G acc: 0.062]\n",
      "2972 [D loss: 0.478(R 0.404, F0.552)] [D acc: 0.766(R 0.719, F 0.812)] [G loss: 0.478] [G acc: 0.109]\n",
      "2973 [D loss: 0.456(R 0.466, F0.446)] [D acc: 0.797(R 0.766, F 0.828)] [G loss: 0.456] [G acc: 0.125]\n",
      "2974 [D loss: 0.493(R 0.573, F0.412)] [D acc: 0.773(R 0.688, F 0.859)] [G loss: 0.493] [G acc: 0.094]\n",
      "2975 [D loss: 0.571(R 0.551, F0.592)] [D acc: 0.695(R 0.672, F 0.719)] [G loss: 0.571] [G acc: 0.141]\n",
      "2976 [D loss: 0.499(R 0.423, F0.574)] [D acc: 0.742(R 0.734, F 0.750)] [G loss: 0.499] [G acc: 0.078]\n",
      "2977 [D loss: 0.509(R 0.577, F0.440)] [D acc: 0.758(R 0.625, F 0.891)] [G loss: 0.509] [G acc: 0.062]\n",
      "2978 [D loss: 0.572(R 0.548, F0.596)] [D acc: 0.695(R 0.625, F 0.766)] [G loss: 0.572] [G acc: 0.125]\n",
      "2979 [D loss: 0.553(R 0.546, F0.559)] [D acc: 0.750(R 0.703, F 0.797)] [G loss: 0.553] [G acc: 0.078]\n",
      "2980 [D loss: 0.546(R 0.468, F0.624)] [D acc: 0.734(R 0.719, F 0.750)] [G loss: 0.546] [G acc: 0.047]\n",
      "2981 [D loss: 0.419(R 0.517, F0.321)] [D acc: 0.828(R 0.703, F 0.953)] [G loss: 0.419] [G acc: 0.078]\n",
      "2982 [D loss: 0.625(R 0.594, F0.656)] [D acc: 0.680(R 0.641, F 0.719)] [G loss: 0.625] [G acc: 0.047]\n",
      "2983 [D loss: 0.586(R 0.688, F0.484)] [D acc: 0.703(R 0.547, F 0.859)] [G loss: 0.586] [G acc: 0.125]\n",
      "2984 [D loss: 0.587(R 0.632, F0.543)] [D acc: 0.672(R 0.547, F 0.797)] [G loss: 0.587] [G acc: 0.094]\n",
      "2985 [D loss: 0.591(R 0.620, F0.562)] [D acc: 0.688(R 0.578, F 0.797)] [G loss: 0.591] [G acc: 0.094]\n",
      "2986 [D loss: 0.515(R 0.606, F0.424)] [D acc: 0.742(R 0.641, F 0.844)] [G loss: 0.515] [G acc: 0.094]\n",
      "2987 [D loss: 0.578(R 0.632, F0.524)] [D acc: 0.742(R 0.656, F 0.828)] [G loss: 0.578] [G acc: 0.141]\n",
      "2988 [D loss: 0.456(R 0.507, F0.405)] [D acc: 0.797(R 0.734, F 0.859)] [G loss: 0.456] [G acc: 0.156]\n",
      "2989 [D loss: 0.526(R 0.434, F0.619)] [D acc: 0.719(R 0.750, F 0.688)] [G loss: 0.526] [G acc: 0.078]\n",
      "2990 [D loss: 0.511(R 0.627, F0.394)] [D acc: 0.750(R 0.594, F 0.906)] [G loss: 0.511] [G acc: 0.078]\n",
      "2991 [D loss: 0.540(R 0.516, F0.564)] [D acc: 0.734(R 0.672, F 0.797)] [G loss: 0.540] [G acc: 0.094]\n",
      "2992 [D loss: 0.536(R 0.624, F0.447)] [D acc: 0.711(R 0.578, F 0.844)] [G loss: 0.536] [G acc: 0.109]\n",
      "2993 [D loss: 0.598(R 0.580, F0.615)] [D acc: 0.703(R 0.641, F 0.766)] [G loss: 0.598] [G acc: 0.188]\n",
      "2994 [D loss: 0.557(R 0.552, F0.562)] [D acc: 0.719(R 0.703, F 0.734)] [G loss: 0.557] [G acc: 0.156]\n",
      "2995 [D loss: 0.522(R 0.568, F0.477)] [D acc: 0.734(R 0.688, F 0.781)] [G loss: 0.522] [G acc: 0.094]\n",
      "2996 [D loss: 0.524(R 0.424, F0.625)] [D acc: 0.742(R 0.781, F 0.703)] [G loss: 0.524] [G acc: 0.109]\n",
      "2997 [D loss: 0.589(R 0.632, F0.546)] [D acc: 0.688(R 0.609, F 0.766)] [G loss: 0.589] [G acc: 0.062]\n",
      "2998 [D loss: 0.530(R 0.620, F0.439)] [D acc: 0.758(R 0.625, F 0.891)] [G loss: 0.530] [G acc: 0.094]\n",
      "2999 [D loss: 0.540(R 0.497, F0.583)] [D acc: 0.695(R 0.641, F 0.750)] [G loss: 0.540] [G acc: 0.094]\n",
      "INFO:tensorflow:Assets written to: ram://341cc7f0-39f9-4a88-8471-6addae4e203f/assets\n",
      "INFO:tensorflow:Assets written to: ram://9ff4963b-590d-47c2-9243-16f89fceb3c8/assets\n",
      "INFO:tensorflow:Assets written to: ram://da6ca1f0-da1c-43a3-adbc-ab3bc19b9b05/assets\n",
      "3000 [D loss: 0.511(R 0.532, F0.490)] [D acc: 0.766(R 0.672, F 0.859)] [G loss: 0.511] [G acc: 0.156]\n",
      "3001 [D loss: 0.586(R 0.567, F0.606)] [D acc: 0.688(R 0.688, F 0.688)] [G loss: 0.586] [G acc: 0.203]\n",
      "3002 [D loss: 0.521(R 0.533, F0.508)] [D acc: 0.727(R 0.656, F 0.797)] [G loss: 0.521] [G acc: 0.000]\n",
      "3003 [D loss: 0.598(R 0.660, F0.536)] [D acc: 0.703(R 0.625, F 0.781)] [G loss: 0.598] [G acc: 0.078]\n",
      "3004 [D loss: 0.424(R 0.349, F0.499)] [D acc: 0.812(R 0.781, F 0.844)] [G loss: 0.424] [G acc: 0.094]\n",
      "3005 [D loss: 0.525(R 0.566, F0.485)] [D acc: 0.711(R 0.625, F 0.797)] [G loss: 0.525] [G acc: 0.062]\n",
      "3006 [D loss: 0.575(R 0.508, F0.641)] [D acc: 0.703(R 0.656, F 0.750)] [G loss: 0.575] [G acc: 0.109]\n",
      "3007 [D loss: 0.520(R 0.629, F0.411)] [D acc: 0.742(R 0.609, F 0.875)] [G loss: 0.520] [G acc: 0.141]\n",
      "3008 [D loss: 0.565(R 0.562, F0.567)] [D acc: 0.719(R 0.625, F 0.812)] [G loss: 0.565] [G acc: 0.250]\n",
      "3009 [D loss: 0.480(R 0.485, F0.475)] [D acc: 0.758(R 0.719, F 0.797)] [G loss: 0.480] [G acc: 0.125]\n",
      "3010 [D loss: 0.546(R 0.609, F0.482)] [D acc: 0.703(R 0.562, F 0.844)] [G loss: 0.546] [G acc: 0.000]\n",
      "3011 [D loss: 0.726(R 0.646, F0.806)] [D acc: 0.641(R 0.594, F 0.688)] [G loss: 0.726] [G acc: 0.031]\n",
      "3012 [D loss: 0.544(R 0.567, F0.521)] [D acc: 0.734(R 0.672, F 0.797)] [G loss: 0.544] [G acc: 0.094]\n",
      "3013 [D loss: 0.569(R 0.642, F0.497)] [D acc: 0.672(R 0.547, F 0.797)] [G loss: 0.569] [G acc: 0.109]\n",
      "3014 [D loss: 0.535(R 0.535, F0.535)] [D acc: 0.773(R 0.734, F 0.812)] [G loss: 0.535] [G acc: 0.062]\n",
      "3015 [D loss: 0.495(R 0.553, F0.437)] [D acc: 0.789(R 0.688, F 0.891)] [G loss: 0.495] [G acc: 0.109]\n",
      "3016 [D loss: 0.522(R 0.482, F0.562)] [D acc: 0.758(R 0.719, F 0.797)] [G loss: 0.522] [G acc: 0.156]\n",
      "3017 [D loss: 0.612(R 0.654, F0.570)] [D acc: 0.648(R 0.531, F 0.766)] [G loss: 0.612] [G acc: 0.109]\n",
      "3018 [D loss: 0.480(R 0.496, F0.463)] [D acc: 0.805(R 0.750, F 0.859)] [G loss: 0.480] [G acc: 0.109]\n",
      "3019 [D loss: 0.649(R 0.633, F0.664)] [D acc: 0.664(R 0.562, F 0.766)] [G loss: 0.649] [G acc: 0.109]\n",
      "3020 [D loss: 0.470(R 0.485, F0.456)] [D acc: 0.789(R 0.719, F 0.859)] [G loss: 0.470] [G acc: 0.109]\n",
      "3021 [D loss: 0.578(R 0.528, F0.627)] [D acc: 0.703(R 0.656, F 0.750)] [G loss: 0.578] [G acc: 0.125]\n",
      "3022 [D loss: 0.462(R 0.475, F0.449)] [D acc: 0.773(R 0.672, F 0.875)] [G loss: 0.462] [G acc: 0.062]\n",
      "3023 [D loss: 0.582(R 0.643, F0.521)] [D acc: 0.656(R 0.578, F 0.734)] [G loss: 0.582] [G acc: 0.062]\n",
      "3024 [D loss: 0.525(R 0.553, F0.496)] [D acc: 0.758(R 0.656, F 0.859)] [G loss: 0.525] [G acc: 0.078]\n",
      "3025 [D loss: 0.444(R 0.482, F0.406)] [D acc: 0.773(R 0.672, F 0.875)] [G loss: 0.444] [G acc: 0.141]\n",
      "3026 [D loss: 0.585(R 0.531, F0.639)] [D acc: 0.688(R 0.703, F 0.672)] [G loss: 0.585] [G acc: 0.156]\n",
      "3027 [D loss: 0.539(R 0.601, F0.478)] [D acc: 0.727(R 0.656, F 0.797)] [G loss: 0.539] [G acc: 0.109]\n",
      "3028 [D loss: 0.505(R 0.511, F0.500)] [D acc: 0.750(R 0.703, F 0.797)] [G loss: 0.505] [G acc: 0.062]\n",
      "3029 [D loss: 0.477(R 0.552, F0.401)] [D acc: 0.789(R 0.703, F 0.875)] [G loss: 0.477] [G acc: 0.094]\n",
      "3030 [D loss: 0.600(R 0.575, F0.625)] [D acc: 0.672(R 0.641, F 0.703)] [G loss: 0.600] [G acc: 0.141]\n",
      "3031 [D loss: 0.557(R 0.611, F0.504)] [D acc: 0.727(R 0.672, F 0.781)] [G loss: 0.557] [G acc: 0.141]\n",
      "3032 [D loss: 0.482(R 0.509, F0.455)] [D acc: 0.781(R 0.719, F 0.844)] [G loss: 0.482] [G acc: 0.031]\n",
      "3033 [D loss: 0.441(R 0.492, F0.390)] [D acc: 0.789(R 0.672, F 0.906)] [G loss: 0.441] [G acc: 0.078]\n",
      "3034 [D loss: 0.591(R 0.567, F0.614)] [D acc: 0.742(R 0.672, F 0.812)] [G loss: 0.591] [G acc: 0.078]\n",
      "3035 [D loss: 0.473(R 0.533, F0.412)] [D acc: 0.812(R 0.688, F 0.938)] [G loss: 0.473] [G acc: 0.078]\n",
      "3036 [D loss: 0.588(R 0.584, F0.592)] [D acc: 0.664(R 0.594, F 0.734)] [G loss: 0.588] [G acc: 0.094]\n",
      "3037 [D loss: 0.562(R 0.505, F0.619)] [D acc: 0.727(R 0.719, F 0.734)] [G loss: 0.562] [G acc: 0.047]\n",
      "3038 [D loss: 0.592(R 0.695, F0.488)] [D acc: 0.672(R 0.562, F 0.781)] [G loss: 0.592] [G acc: 0.109]\n",
      "3039 [D loss: 0.496(R 0.547, F0.446)] [D acc: 0.766(R 0.641, F 0.891)] [G loss: 0.496] [G acc: 0.141]\n",
      "3040 [D loss: 0.412(R 0.424, F0.399)] [D acc: 0.836(R 0.766, F 0.906)] [G loss: 0.412] [G acc: 0.062]\n",
      "3041 [D loss: 0.628(R 0.578, F0.679)] [D acc: 0.750(R 0.656, F 0.844)] [G loss: 0.628] [G acc: 0.125]\n",
      "3042 [D loss: 0.579(R 0.563, F0.596)] [D acc: 0.711(R 0.688, F 0.734)] [G loss: 0.579] [G acc: 0.062]\n",
      "3043 [D loss: 0.568(R 0.468, F0.667)] [D acc: 0.766(R 0.719, F 0.812)] [G loss: 0.568] [G acc: 0.062]\n",
      "3044 [D loss: 0.523(R 0.553, F0.493)] [D acc: 0.734(R 0.688, F 0.781)] [G loss: 0.523] [G acc: 0.109]\n",
      "3045 [D loss: 0.503(R 0.603, F0.402)] [D acc: 0.695(R 0.547, F 0.844)] [G loss: 0.503] [G acc: 0.125]\n",
      "3046 [D loss: 0.533(R 0.520, F0.545)] [D acc: 0.695(R 0.656, F 0.734)] [G loss: 0.533] [G acc: 0.156]\n",
      "3047 [D loss: 0.624(R 0.672, F0.576)] [D acc: 0.672(R 0.562, F 0.781)] [G loss: 0.624] [G acc: 0.078]\n",
      "3048 [D loss: 0.607(R 0.591, F0.623)] [D acc: 0.656(R 0.609, F 0.703)] [G loss: 0.607] [G acc: 0.094]\n",
      "3049 [D loss: 0.584(R 0.510, F0.657)] [D acc: 0.719(R 0.703, F 0.734)] [G loss: 0.584] [G acc: 0.078]\n",
      "3050 [D loss: 0.530(R 0.605, F0.454)] [D acc: 0.789(R 0.656, F 0.922)] [G loss: 0.530] [G acc: 0.125]\n",
      "3051 [D loss: 0.528(R 0.544, F0.512)] [D acc: 0.766(R 0.688, F 0.844)] [G loss: 0.528] [G acc: 0.078]\n",
      "3052 [D loss: 0.521(R 0.499, F0.543)] [D acc: 0.734(R 0.672, F 0.797)] [G loss: 0.521] [G acc: 0.109]\n",
      "3053 [D loss: 0.592(R 0.538, F0.647)] [D acc: 0.711(R 0.688, F 0.734)] [G loss: 0.592] [G acc: 0.094]\n",
      "3054 [D loss: 0.547(R 0.561, F0.533)] [D acc: 0.703(R 0.656, F 0.750)] [G loss: 0.547] [G acc: 0.078]\n",
      "3055 [D loss: 0.447(R 0.532, F0.361)] [D acc: 0.781(R 0.672, F 0.891)] [G loss: 0.447] [G acc: 0.172]\n",
      "3056 [D loss: 0.538(R 0.523, F0.552)] [D acc: 0.766(R 0.734, F 0.797)] [G loss: 0.538] [G acc: 0.109]\n",
      "3057 [D loss: 0.595(R 0.600, F0.590)] [D acc: 0.664(R 0.609, F 0.719)] [G loss: 0.595] [G acc: 0.094]\n",
      "3058 [D loss: 0.500(R 0.503, F0.497)] [D acc: 0.734(R 0.719, F 0.750)] [G loss: 0.500] [G acc: 0.062]\n",
      "3059 [D loss: 0.555(R 0.653, F0.457)] [D acc: 0.719(R 0.625, F 0.812)] [G loss: 0.555] [G acc: 0.094]\n",
      "3060 [D loss: 0.575(R 0.516, F0.634)] [D acc: 0.711(R 0.672, F 0.750)] [G loss: 0.575] [G acc: 0.094]\n",
      "3061 [D loss: 0.615(R 0.718, F0.512)] [D acc: 0.688(R 0.531, F 0.844)] [G loss: 0.615] [G acc: 0.156]\n",
      "3062 [D loss: 0.547(R 0.645, F0.450)] [D acc: 0.773(R 0.656, F 0.891)] [G loss: 0.547] [G acc: 0.078]\n",
      "3063 [D loss: 0.556(R 0.614, F0.497)] [D acc: 0.727(R 0.594, F 0.859)] [G loss: 0.556] [G acc: 0.109]\n",
      "3064 [D loss: 0.455(R 0.474, F0.436)] [D acc: 0.797(R 0.750, F 0.844)] [G loss: 0.455] [G acc: 0.047]\n",
      "3065 [D loss: 0.509(R 0.561, F0.456)] [D acc: 0.750(R 0.625, F 0.875)] [G loss: 0.509] [G acc: 0.062]\n",
      "3066 [D loss: 0.565(R 0.521, F0.609)] [D acc: 0.719(R 0.703, F 0.734)] [G loss: 0.565] [G acc: 0.078]\n",
      "3067 [D loss: 0.532(R 0.553, F0.511)] [D acc: 0.727(R 0.625, F 0.828)] [G loss: 0.532] [G acc: 0.109]\n",
      "3068 [D loss: 0.497(R 0.526, F0.468)] [D acc: 0.773(R 0.719, F 0.828)] [G loss: 0.497] [G acc: 0.125]\n",
      "3069 [D loss: 0.529(R 0.567, F0.492)] [D acc: 0.734(R 0.625, F 0.844)] [G loss: 0.529] [G acc: 0.062]\n",
      "3070 [D loss: 0.464(R 0.533, F0.395)] [D acc: 0.781(R 0.672, F 0.891)] [G loss: 0.464] [G acc: 0.078]\n",
      "3071 [D loss: 0.513(R 0.478, F0.549)] [D acc: 0.766(R 0.719, F 0.812)] [G loss: 0.513] [G acc: 0.062]\n",
      "3072 [D loss: 0.527(R 0.493, F0.562)] [D acc: 0.758(R 0.766, F 0.750)] [G loss: 0.527] [G acc: 0.188]\n",
      "3073 [D loss: 0.554(R 0.552, F0.556)] [D acc: 0.727(R 0.688, F 0.766)] [G loss: 0.554] [G acc: 0.109]\n",
      "3074 [D loss: 0.502(R 0.532, F0.472)] [D acc: 0.719(R 0.672, F 0.766)] [G loss: 0.502] [G acc: 0.078]\n",
      "3075 [D loss: 0.433(R 0.475, F0.392)] [D acc: 0.758(R 0.688, F 0.828)] [G loss: 0.433] [G acc: 0.078]\n",
      "3076 [D loss: 0.558(R 0.487, F0.630)] [D acc: 0.773(R 0.766, F 0.781)] [G loss: 0.558] [G acc: 0.062]\n",
      "3077 [D loss: 0.549(R 0.610, F0.488)] [D acc: 0.703(R 0.625, F 0.781)] [G loss: 0.549] [G acc: 0.062]\n",
      "3078 [D loss: 0.492(R 0.511, F0.473)] [D acc: 0.789(R 0.734, F 0.844)] [G loss: 0.492] [G acc: 0.094]\n",
      "3079 [D loss: 0.549(R 0.463, F0.635)] [D acc: 0.719(R 0.703, F 0.734)] [G loss: 0.549] [G acc: 0.078]\n",
      "3080 [D loss: 0.548(R 0.596, F0.499)] [D acc: 0.688(R 0.625, F 0.750)] [G loss: 0.548] [G acc: 0.062]\n",
      "3081 [D loss: 0.514(R 0.617, F0.412)] [D acc: 0.734(R 0.594, F 0.875)] [G loss: 0.514] [G acc: 0.188]\n",
      "3082 [D loss: 0.561(R 0.562, F0.560)] [D acc: 0.734(R 0.688, F 0.781)] [G loss: 0.561] [G acc: 0.078]\n",
      "3083 [D loss: 0.560(R 0.588, F0.531)] [D acc: 0.719(R 0.609, F 0.828)] [G loss: 0.560] [G acc: 0.094]\n",
      "3084 [D loss: 0.536(R 0.482, F0.591)] [D acc: 0.727(R 0.688, F 0.766)] [G loss: 0.536] [G acc: 0.094]\n",
      "3085 [D loss: 0.523(R 0.593, F0.452)] [D acc: 0.766(R 0.641, F 0.891)] [G loss: 0.523] [G acc: 0.031]\n",
      "3086 [D loss: 0.534(R 0.548, F0.520)] [D acc: 0.734(R 0.672, F 0.797)] [G loss: 0.534] [G acc: 0.094]\n",
      "3087 [D loss: 0.528(R 0.573, F0.483)] [D acc: 0.727(R 0.641, F 0.812)] [G loss: 0.528] [G acc: 0.156]\n",
      "3088 [D loss: 0.536(R 0.505, F0.566)] [D acc: 0.750(R 0.688, F 0.812)] [G loss: 0.536] [G acc: 0.031]\n",
      "3089 [D loss: 0.522(R 0.653, F0.390)] [D acc: 0.727(R 0.594, F 0.859)] [G loss: 0.522] [G acc: 0.062]\n",
      "3090 [D loss: 0.515(R 0.517, F0.513)] [D acc: 0.727(R 0.688, F 0.766)] [G loss: 0.515] [G acc: 0.141]\n",
      "3091 [D loss: 0.467(R 0.438, F0.495)] [D acc: 0.781(R 0.750, F 0.812)] [G loss: 0.467] [G acc: 0.078]\n",
      "3092 [D loss: 0.516(R 0.547, F0.485)] [D acc: 0.734(R 0.625, F 0.844)] [G loss: 0.516] [G acc: 0.062]\n",
      "3093 [D loss: 0.529(R 0.574, F0.485)] [D acc: 0.695(R 0.641, F 0.750)] [G loss: 0.529] [G acc: 0.078]\n",
      "3094 [D loss: 0.571(R 0.576, F0.565)] [D acc: 0.711(R 0.656, F 0.766)] [G loss: 0.571] [G acc: 0.078]\n",
      "3095 [D loss: 0.527(R 0.527, F0.527)] [D acc: 0.789(R 0.766, F 0.812)] [G loss: 0.527] [G acc: 0.109]\n",
      "3096 [D loss: 0.496(R 0.537, F0.455)] [D acc: 0.750(R 0.672, F 0.828)] [G loss: 0.496] [G acc: 0.047]\n",
      "3097 [D loss: 0.544(R 0.538, F0.551)] [D acc: 0.680(R 0.672, F 0.688)] [G loss: 0.544] [G acc: 0.156]\n",
      "3098 [D loss: 0.601(R 0.502, F0.699)] [D acc: 0.688(R 0.641, F 0.734)] [G loss: 0.601] [G acc: 0.094]\n",
      "3099 [D loss: 0.562(R 0.541, F0.583)] [D acc: 0.688(R 0.688, F 0.688)] [G loss: 0.562] [G acc: 0.047]\n",
      "INFO:tensorflow:Assets written to: ram://7bf99a92-d8c0-4844-be6e-e5aa8f4d6fcb/assets\n",
      "INFO:tensorflow:Assets written to: ram://e6113a4d-8dd5-4d45-a0d3-08d9bdbbfc53/assets\n",
      "INFO:tensorflow:Assets written to: ram://4d693493-2217-4a10-a511-a3fd2a1debb0/assets\n",
      "3100 [D loss: 0.564(R 0.665, F0.463)] [D acc: 0.734(R 0.578, F 0.891)] [G loss: 0.564] [G acc: 0.062]\n",
      "3101 [D loss: 0.517(R 0.460, F0.574)] [D acc: 0.742(R 0.719, F 0.766)] [G loss: 0.517] [G acc: 0.109]\n",
      "3102 [D loss: 0.595(R 0.499, F0.692)] [D acc: 0.727(R 0.750, F 0.703)] [G loss: 0.595] [G acc: 0.062]\n",
      "3103 [D loss: 0.497(R 0.559, F0.435)] [D acc: 0.758(R 0.656, F 0.859)] [G loss: 0.497] [G acc: 0.031]\n",
      "3104 [D loss: 0.523(R 0.571, F0.475)] [D acc: 0.695(R 0.609, F 0.781)] [G loss: 0.523] [G acc: 0.047]\n",
      "3105 [D loss: 0.532(R 0.569, F0.494)] [D acc: 0.703(R 0.625, F 0.781)] [G loss: 0.532] [G acc: 0.188]\n",
      "3106 [D loss: 0.519(R 0.441, F0.596)] [D acc: 0.742(R 0.703, F 0.781)] [G loss: 0.519] [G acc: 0.109]\n",
      "3107 [D loss: 0.473(R 0.516, F0.430)] [D acc: 0.789(R 0.734, F 0.844)] [G loss: 0.473] [G acc: 0.109]\n",
      "3108 [D loss: 0.523(R 0.553, F0.492)] [D acc: 0.695(R 0.641, F 0.750)] [G loss: 0.523] [G acc: 0.078]\n",
      "3109 [D loss: 0.525(R 0.608, F0.442)] [D acc: 0.797(R 0.703, F 0.891)] [G loss: 0.525] [G acc: 0.125]\n",
      "3110 [D loss: 0.502(R 0.442, F0.562)] [D acc: 0.734(R 0.719, F 0.750)] [G loss: 0.502] [G acc: 0.125]\n",
      "3111 [D loss: 0.641(R 0.548, F0.734)] [D acc: 0.680(R 0.672, F 0.688)] [G loss: 0.641] [G acc: 0.047]\n",
      "3112 [D loss: 0.532(R 0.608, F0.456)] [D acc: 0.750(R 0.641, F 0.859)] [G loss: 0.532] [G acc: 0.094]\n",
      "3113 [D loss: 0.468(R 0.433, F0.504)] [D acc: 0.750(R 0.719, F 0.781)] [G loss: 0.468] [G acc: 0.094]\n",
      "3114 [D loss: 0.585(R 0.689, F0.482)] [D acc: 0.711(R 0.578, F 0.844)] [G loss: 0.585] [G acc: 0.094]\n",
      "3115 [D loss: 0.515(R 0.484, F0.545)] [D acc: 0.742(R 0.734, F 0.750)] [G loss: 0.515] [G acc: 0.125]\n",
      "3116 [D loss: 0.553(R 0.572, F0.534)] [D acc: 0.719(R 0.656, F 0.781)] [G loss: 0.553] [G acc: 0.094]\n",
      "3117 [D loss: 0.444(R 0.524, F0.365)] [D acc: 0.820(R 0.719, F 0.922)] [G loss: 0.444] [G acc: 0.109]\n",
      "3118 [D loss: 0.468(R 0.466, F0.469)] [D acc: 0.758(R 0.688, F 0.828)] [G loss: 0.468] [G acc: 0.078]\n",
      "3119 [D loss: 0.528(R 0.468, F0.588)] [D acc: 0.727(R 0.750, F 0.703)] [G loss: 0.528] [G acc: 0.078]\n",
      "3120 [D loss: 0.547(R 0.589, F0.506)] [D acc: 0.703(R 0.609, F 0.797)] [G loss: 0.547] [G acc: 0.094]\n",
      "3121 [D loss: 0.617(R 0.747, F0.487)] [D acc: 0.672(R 0.547, F 0.797)] [G loss: 0.617] [G acc: 0.062]\n",
      "3122 [D loss: 0.594(R 0.745, F0.444)] [D acc: 0.734(R 0.609, F 0.859)] [G loss: 0.594] [G acc: 0.156]\n",
      "3123 [D loss: 0.589(R 0.560, F0.617)] [D acc: 0.695(R 0.625, F 0.766)] [G loss: 0.589] [G acc: 0.078]\n",
      "3124 [D loss: 0.577(R 0.662, F0.493)] [D acc: 0.656(R 0.516, F 0.797)] [G loss: 0.577] [G acc: 0.062]\n",
      "3125 [D loss: 0.509(R 0.465, F0.553)] [D acc: 0.758(R 0.719, F 0.797)] [G loss: 0.509] [G acc: 0.047]\n",
      "3126 [D loss: 0.520(R 0.586, F0.454)] [D acc: 0.758(R 0.656, F 0.859)] [G loss: 0.520] [G acc: 0.031]\n",
      "3127 [D loss: 0.474(R 0.479, F0.469)] [D acc: 0.797(R 0.703, F 0.891)] [G loss: 0.474] [G acc: 0.156]\n",
      "3128 [D loss: 0.551(R 0.629, F0.473)] [D acc: 0.750(R 0.641, F 0.859)] [G loss: 0.551] [G acc: 0.094]\n",
      "3129 [D loss: 0.543(R 0.412, F0.674)] [D acc: 0.727(R 0.750, F 0.703)] [G loss: 0.543] [G acc: 0.062]\n",
      "3130 [D loss: 0.517(R 0.593, F0.442)] [D acc: 0.750(R 0.625, F 0.875)] [G loss: 0.517] [G acc: 0.141]\n",
      "3131 [D loss: 0.586(R 0.596, F0.575)] [D acc: 0.711(R 0.672, F 0.750)] [G loss: 0.586] [G acc: 0.047]\n",
      "3132 [D loss: 0.548(R 0.606, F0.490)] [D acc: 0.672(R 0.562, F 0.781)] [G loss: 0.548] [G acc: 0.078]\n",
      "3133 [D loss: 0.505(R 0.559, F0.451)] [D acc: 0.750(R 0.688, F 0.812)] [G loss: 0.505] [G acc: 0.094]\n",
      "3134 [D loss: 0.547(R 0.464, F0.630)] [D acc: 0.711(R 0.688, F 0.734)] [G loss: 0.547] [G acc: 0.094]\n",
      "3135 [D loss: 0.496(R 0.527, F0.465)] [D acc: 0.781(R 0.734, F 0.828)] [G loss: 0.496] [G acc: 0.109]\n",
      "3136 [D loss: 0.556(R 0.570, F0.543)] [D acc: 0.742(R 0.688, F 0.797)] [G loss: 0.556] [G acc: 0.156]\n",
      "3137 [D loss: 0.540(R 0.567, F0.512)] [D acc: 0.727(R 0.625, F 0.828)] [G loss: 0.540] [G acc: 0.141]\n",
      "3138 [D loss: 0.533(R 0.516, F0.549)] [D acc: 0.750(R 0.672, F 0.828)] [G loss: 0.533] [G acc: 0.062]\n",
      "3139 [D loss: 0.459(R 0.540, F0.379)] [D acc: 0.781(R 0.672, F 0.891)] [G loss: 0.459] [G acc: 0.141]\n",
      "3140 [D loss: 0.465(R 0.394, F0.536)] [D acc: 0.750(R 0.734, F 0.766)] [G loss: 0.465] [G acc: 0.094]\n",
      "3141 [D loss: 0.537(R 0.524, F0.551)] [D acc: 0.719(R 0.656, F 0.781)] [G loss: 0.537] [G acc: 0.172]\n",
      "3142 [D loss: 0.513(R 0.410, F0.617)] [D acc: 0.773(R 0.797, F 0.750)] [G loss: 0.513] [G acc: 0.078]\n",
      "3143 [D loss: 0.546(R 0.653, F0.439)] [D acc: 0.680(R 0.562, F 0.797)] [G loss: 0.546] [G acc: 0.062]\n",
      "3144 [D loss: 0.487(R 0.488, F0.486)] [D acc: 0.758(R 0.719, F 0.797)] [G loss: 0.487] [G acc: 0.141]\n",
      "3145 [D loss: 0.528(R 0.473, F0.584)] [D acc: 0.750(R 0.734, F 0.766)] [G loss: 0.528] [G acc: 0.109]\n",
      "3146 [D loss: 0.494(R 0.596, F0.393)] [D acc: 0.734(R 0.578, F 0.891)] [G loss: 0.494] [G acc: 0.031]\n",
      "3147 [D loss: 0.422(R 0.379, F0.466)] [D acc: 0.766(R 0.750, F 0.781)] [G loss: 0.422] [G acc: 0.078]\n",
      "3148 [D loss: 0.520(R 0.540, F0.500)] [D acc: 0.805(R 0.734, F 0.875)] [G loss: 0.520] [G acc: 0.094]\n",
      "3149 [D loss: 0.534(R 0.559, F0.509)] [D acc: 0.742(R 0.609, F 0.875)] [G loss: 0.534] [G acc: 0.031]\n",
      "3150 [D loss: 0.551(R 0.618, F0.484)] [D acc: 0.734(R 0.609, F 0.859)] [G loss: 0.551] [G acc: 0.125]\n",
      "3151 [D loss: 0.455(R 0.482, F0.428)] [D acc: 0.789(R 0.734, F 0.844)] [G loss: 0.455] [G acc: 0.188]\n",
      "3152 [D loss: 0.528(R 0.418, F0.639)] [D acc: 0.758(R 0.797, F 0.719)] [G loss: 0.528] [G acc: 0.062]\n",
      "3153 [D loss: 0.560(R 0.724, F0.396)] [D acc: 0.711(R 0.531, F 0.891)] [G loss: 0.560] [G acc: 0.094]\n",
      "3154 [D loss: 0.489(R 0.496, F0.482)] [D acc: 0.773(R 0.734, F 0.812)] [G loss: 0.489] [G acc: 0.141]\n",
      "3155 [D loss: 0.558(R 0.581, F0.535)] [D acc: 0.742(R 0.688, F 0.797)] [G loss: 0.558] [G acc: 0.156]\n",
      "3156 [D loss: 0.464(R 0.585, F0.343)] [D acc: 0.789(R 0.656, F 0.922)] [G loss: 0.464] [G acc: 0.047]\n",
      "3157 [D loss: 0.502(R 0.414, F0.591)] [D acc: 0.773(R 0.781, F 0.766)] [G loss: 0.502] [G acc: 0.172]\n",
      "3158 [D loss: 0.483(R 0.518, F0.448)] [D acc: 0.773(R 0.703, F 0.844)] [G loss: 0.483] [G acc: 0.156]\n",
      "3159 [D loss: 0.518(R 0.544, F0.493)] [D acc: 0.727(R 0.672, F 0.781)] [G loss: 0.518] [G acc: 0.094]\n",
      "3160 [D loss: 0.500(R 0.479, F0.520)] [D acc: 0.734(R 0.719, F 0.750)] [G loss: 0.500] [G acc: 0.062]\n",
      "3161 [D loss: 0.461(R 0.448, F0.473)] [D acc: 0.781(R 0.734, F 0.828)] [G loss: 0.461] [G acc: 0.062]\n",
      "3162 [D loss: 0.586(R 0.690, F0.482)] [D acc: 0.727(R 0.641, F 0.812)] [G loss: 0.586] [G acc: 0.156]\n",
      "3163 [D loss: 0.439(R 0.368, F0.510)] [D acc: 0.781(R 0.781, F 0.781)] [G loss: 0.439] [G acc: 0.094]\n",
      "3164 [D loss: 0.534(R 0.583, F0.485)] [D acc: 0.734(R 0.625, F 0.844)] [G loss: 0.534] [G acc: 0.031]\n",
      "3165 [D loss: 0.471(R 0.325, F0.617)] [D acc: 0.797(R 0.828, F 0.766)] [G loss: 0.471] [G acc: 0.047]\n",
      "3166 [D loss: 0.468(R 0.476, F0.460)] [D acc: 0.758(R 0.688, F 0.828)] [G loss: 0.468] [G acc: 0.031]\n",
      "3167 [D loss: 0.503(R 0.471, F0.535)] [D acc: 0.766(R 0.750, F 0.781)] [G loss: 0.503] [G acc: 0.031]\n",
      "3168 [D loss: 0.514(R 0.523, F0.505)] [D acc: 0.734(R 0.688, F 0.781)] [G loss: 0.514] [G acc: 0.125]\n",
      "3169 [D loss: 0.409(R 0.364, F0.454)] [D acc: 0.781(R 0.797, F 0.766)] [G loss: 0.409] [G acc: 0.062]\n",
      "3170 [D loss: 0.508(R 0.564, F0.453)] [D acc: 0.742(R 0.641, F 0.844)] [G loss: 0.508] [G acc: 0.094]\n",
      "3171 [D loss: 0.429(R 0.476, F0.383)] [D acc: 0.789(R 0.719, F 0.859)] [G loss: 0.429] [G acc: 0.078]\n",
      "3172 [D loss: 0.631(R 0.692, F0.569)] [D acc: 0.602(R 0.484, F 0.719)] [G loss: 0.631] [G acc: 0.062]\n",
      "3173 [D loss: 0.647(R 0.608, F0.686)] [D acc: 0.680(R 0.609, F 0.750)] [G loss: 0.647] [G acc: 0.047]\n",
      "3174 [D loss: 0.613(R 0.760, F0.466)] [D acc: 0.672(R 0.484, F 0.859)] [G loss: 0.613] [G acc: 0.078]\n",
      "3175 [D loss: 0.588(R 0.595, F0.581)] [D acc: 0.750(R 0.688, F 0.812)] [G loss: 0.588] [G acc: 0.062]\n",
      "3176 [D loss: 0.607(R 0.680, F0.534)] [D acc: 0.664(R 0.516, F 0.812)] [G loss: 0.607] [G acc: 0.109]\n",
      "3177 [D loss: 0.538(R 0.589, F0.487)] [D acc: 0.781(R 0.672, F 0.891)] [G loss: 0.538] [G acc: 0.078]\n",
      "3178 [D loss: 0.452(R 0.518, F0.386)] [D acc: 0.797(R 0.703, F 0.891)] [G loss: 0.452] [G acc: 0.219]\n",
      "3179 [D loss: 0.527(R 0.476, F0.579)] [D acc: 0.734(R 0.688, F 0.781)] [G loss: 0.527] [G acc: 0.078]\n",
      "3180 [D loss: 0.563(R 0.559, F0.567)] [D acc: 0.695(R 0.672, F 0.719)] [G loss: 0.563] [G acc: 0.094]\n",
      "3181 [D loss: 0.560(R 0.570, F0.551)] [D acc: 0.711(R 0.656, F 0.766)] [G loss: 0.560] [G acc: 0.078]\n",
      "3182 [D loss: 0.535(R 0.614, F0.456)] [D acc: 0.719(R 0.578, F 0.859)] [G loss: 0.535] [G acc: 0.047]\n",
      "3183 [D loss: 0.544(R 0.575, F0.514)] [D acc: 0.781(R 0.688, F 0.875)] [G loss: 0.544] [G acc: 0.078]\n",
      "3184 [D loss: 0.510(R 0.594, F0.425)] [D acc: 0.750(R 0.625, F 0.875)] [G loss: 0.510] [G acc: 0.109]\n",
      "3185 [D loss: 0.561(R 0.514, F0.608)] [D acc: 0.719(R 0.656, F 0.781)] [G loss: 0.561] [G acc: 0.109]\n",
      "3186 [D loss: 0.493(R 0.496, F0.489)] [D acc: 0.773(R 0.703, F 0.844)] [G loss: 0.493] [G acc: 0.109]\n",
      "3187 [D loss: 0.527(R 0.625, F0.430)] [D acc: 0.703(R 0.562, F 0.844)] [G loss: 0.527] [G acc: 0.094]\n",
      "3188 [D loss: 0.479(R 0.505, F0.453)] [D acc: 0.773(R 0.703, F 0.844)] [G loss: 0.479] [G acc: 0.156]\n",
      "3189 [D loss: 0.486(R 0.446, F0.526)] [D acc: 0.766(R 0.734, F 0.797)] [G loss: 0.486] [G acc: 0.094]\n",
      "3190 [D loss: 0.582(R 0.616, F0.547)] [D acc: 0.719(R 0.625, F 0.812)] [G loss: 0.582] [G acc: 0.047]\n",
      "3191 [D loss: 0.407(R 0.445, F0.370)] [D acc: 0.758(R 0.672, F 0.844)] [G loss: 0.407] [G acc: 0.109]\n",
      "3192 [D loss: 0.528(R 0.539, F0.516)] [D acc: 0.719(R 0.703, F 0.734)] [G loss: 0.528] [G acc: 0.156]\n",
      "3193 [D loss: 0.517(R 0.531, F0.503)] [D acc: 0.758(R 0.672, F 0.844)] [G loss: 0.517] [G acc: 0.078]\n",
      "3194 [D loss: 0.622(R 0.723, F0.520)] [D acc: 0.664(R 0.484, F 0.844)] [G loss: 0.622] [G acc: 0.094]\n",
      "3195 [D loss: 0.513(R 0.525, F0.501)] [D acc: 0.773(R 0.734, F 0.812)] [G loss: 0.513] [G acc: 0.141]\n",
      "3196 [D loss: 0.520(R 0.532, F0.508)] [D acc: 0.711(R 0.641, F 0.781)] [G loss: 0.520] [G acc: 0.047]\n",
      "3197 [D loss: 0.487(R 0.451, F0.523)] [D acc: 0.766(R 0.734, F 0.797)] [G loss: 0.487] [G acc: 0.047]\n",
      "3198 [D loss: 0.409(R 0.444, F0.375)] [D acc: 0.781(R 0.750, F 0.812)] [G loss: 0.409] [G acc: 0.062]\n",
      "3199 [D loss: 0.572(R 0.583, F0.562)] [D acc: 0.727(R 0.688, F 0.766)] [G loss: 0.572] [G acc: 0.156]\n",
      "INFO:tensorflow:Assets written to: ram://13d47a6c-02b0-441d-981c-62db6e53ecdb/assets\n",
      "INFO:tensorflow:Assets written to: ram://4054ab47-af6d-4695-8fcb-f07454fe3b59/assets\n",
      "INFO:tensorflow:Assets written to: ram://cf077738-698a-4341-9097-3bd8b7530eb5/assets\n",
      "3200 [D loss: 0.502(R 0.449, F0.555)] [D acc: 0.773(R 0.781, F 0.766)] [G loss: 0.502] [G acc: 0.125]\n",
      "3201 [D loss: 0.474(R 0.423, F0.525)] [D acc: 0.766(R 0.766, F 0.766)] [G loss: 0.474] [G acc: 0.047]\n",
      "3202 [D loss: 0.578(R 0.552, F0.605)] [D acc: 0.711(R 0.641, F 0.781)] [G loss: 0.578] [G acc: 0.094]\n",
      "3203 [D loss: 0.550(R 0.668, F0.432)] [D acc: 0.781(R 0.656, F 0.906)] [G loss: 0.550] [G acc: 0.125]\n",
      "3204 [D loss: 0.528(R 0.596, F0.460)] [D acc: 0.719(R 0.562, F 0.875)] [G loss: 0.528] [G acc: 0.172]\n",
      "3205 [D loss: 0.419(R 0.380, F0.457)] [D acc: 0.805(R 0.781, F 0.828)] [G loss: 0.419] [G acc: 0.125]\n",
      "3206 [D loss: 0.484(R 0.549, F0.419)] [D acc: 0.734(R 0.656, F 0.812)] [G loss: 0.484] [G acc: 0.203]\n",
      "3207 [D loss: 0.498(R 0.378, F0.617)] [D acc: 0.773(R 0.812, F 0.734)] [G loss: 0.498] [G acc: 0.125]\n",
      "3208 [D loss: 0.421(R 0.466, F0.376)] [D acc: 0.797(R 0.703, F 0.891)] [G loss: 0.421] [G acc: 0.141]\n",
      "3209 [D loss: 0.382(R 0.393, F0.370)] [D acc: 0.789(R 0.734, F 0.844)] [G loss: 0.382] [G acc: 0.078]\n",
      "3210 [D loss: 0.579(R 0.428, F0.730)] [D acc: 0.719(R 0.781, F 0.656)] [G loss: 0.579] [G acc: 0.047]\n",
      "3211 [D loss: 0.579(R 0.755, F0.403)] [D acc: 0.719(R 0.531, F 0.906)] [G loss: 0.579] [G acc: 0.094]\n",
      "3212 [D loss: 0.489(R 0.522, F0.456)] [D acc: 0.773(R 0.703, F 0.844)] [G loss: 0.489] [G acc: 0.156]\n",
      "3213 [D loss: 0.707(R 0.522, F0.893)] [D acc: 0.711(R 0.688, F 0.734)] [G loss: 0.707] [G acc: 0.062]\n",
      "3214 [D loss: 0.557(R 0.722, F0.393)] [D acc: 0.719(R 0.516, F 0.922)] [G loss: 0.557] [G acc: 0.078]\n",
      "3215 [D loss: 0.583(R 0.668, F0.498)] [D acc: 0.711(R 0.594, F 0.828)] [G loss: 0.583] [G acc: 0.062]\n",
      "3216 [D loss: 0.553(R 0.644, F0.462)] [D acc: 0.719(R 0.641, F 0.797)] [G loss: 0.553] [G acc: 0.094]\n",
      "3217 [D loss: 0.541(R 0.580, F0.502)] [D acc: 0.781(R 0.688, F 0.875)] [G loss: 0.541] [G acc: 0.094]\n",
      "3218 [D loss: 0.589(R 0.561, F0.617)] [D acc: 0.734(R 0.641, F 0.828)] [G loss: 0.589] [G acc: 0.141]\n",
      "3219 [D loss: 0.596(R 0.694, F0.497)] [D acc: 0.742(R 0.656, F 0.828)] [G loss: 0.596] [G acc: 0.047]\n",
      "3220 [D loss: 0.469(R 0.578, F0.360)] [D acc: 0.766(R 0.609, F 0.922)] [G loss: 0.469] [G acc: 0.062]\n",
      "3221 [D loss: 0.470(R 0.413, F0.527)] [D acc: 0.805(R 0.766, F 0.844)] [G loss: 0.470] [G acc: 0.062]\n",
      "3222 [D loss: 0.488(R 0.373, F0.604)] [D acc: 0.773(R 0.812, F 0.734)] [G loss: 0.488] [G acc: 0.031]\n",
      "3223 [D loss: 0.491(R 0.561, F0.421)] [D acc: 0.758(R 0.703, F 0.812)] [G loss: 0.491] [G acc: 0.141]\n",
      "3224 [D loss: 0.475(R 0.458, F0.493)] [D acc: 0.789(R 0.766, F 0.812)] [G loss: 0.475] [G acc: 0.094]\n",
      "3225 [D loss: 0.503(R 0.475, F0.531)] [D acc: 0.727(R 0.703, F 0.750)] [G loss: 0.503] [G acc: 0.094]\n",
      "3226 [D loss: 0.489(R 0.532, F0.446)] [D acc: 0.773(R 0.703, F 0.844)] [G loss: 0.489] [G acc: 0.062]\n",
      "3227 [D loss: 0.606(R 0.635, F0.578)] [D acc: 0.758(R 0.703, F 0.812)] [G loss: 0.606] [G acc: 0.062]\n",
      "3228 [D loss: 0.508(R 0.562, F0.454)] [D acc: 0.742(R 0.609, F 0.875)] [G loss: 0.508] [G acc: 0.156]\n",
      "3229 [D loss: 0.439(R 0.407, F0.470)] [D acc: 0.766(R 0.766, F 0.766)] [G loss: 0.439] [G acc: 0.078]\n",
      "3230 [D loss: 0.508(R 0.566, F0.449)] [D acc: 0.734(R 0.625, F 0.844)] [G loss: 0.508] [G acc: 0.156]\n",
      "3231 [D loss: 0.541(R 0.605, F0.476)] [D acc: 0.711(R 0.656, F 0.766)] [G loss: 0.541] [G acc: 0.141]\n",
      "3232 [D loss: 0.486(R 0.523, F0.448)] [D acc: 0.758(R 0.672, F 0.844)] [G loss: 0.486] [G acc: 0.047]\n",
      "3233 [D loss: 0.510(R 0.451, F0.569)] [D acc: 0.727(R 0.703, F 0.750)] [G loss: 0.510] [G acc: 0.078]\n",
      "3234 [D loss: 0.478(R 0.512, F0.444)] [D acc: 0.781(R 0.641, F 0.922)] [G loss: 0.478] [G acc: 0.141]\n",
      "3235 [D loss: 0.542(R 0.480, F0.603)] [D acc: 0.719(R 0.672, F 0.766)] [G loss: 0.542] [G acc: 0.047]\n",
      "3236 [D loss: 0.565(R 0.735, F0.395)] [D acc: 0.672(R 0.516, F 0.828)] [G loss: 0.565] [G acc: 0.188]\n",
      "3237 [D loss: 0.502(R 0.478, F0.526)] [D acc: 0.781(R 0.734, F 0.828)] [G loss: 0.502] [G acc: 0.125]\n",
      "3238 [D loss: 0.517(R 0.518, F0.515)] [D acc: 0.734(R 0.688, F 0.781)] [G loss: 0.517] [G acc: 0.062]\n",
      "3239 [D loss: 0.494(R 0.562, F0.425)] [D acc: 0.758(R 0.672, F 0.844)] [G loss: 0.494] [G acc: 0.094]\n",
      "3240 [D loss: 0.653(R 0.655, F0.652)] [D acc: 0.711(R 0.609, F 0.812)] [G loss: 0.653] [G acc: 0.062]\n",
      "3241 [D loss: 0.602(R 0.744, F0.460)] [D acc: 0.672(R 0.453, F 0.891)] [G loss: 0.602] [G acc: 0.109]\n",
      "3242 [D loss: 0.479(R 0.488, F0.471)] [D acc: 0.781(R 0.734, F 0.828)] [G loss: 0.479] [G acc: 0.109]\n",
      "3243 [D loss: 0.612(R 0.605, F0.618)] [D acc: 0.695(R 0.594, F 0.797)] [G loss: 0.612] [G acc: 0.047]\n",
      "3244 [D loss: 0.559(R 0.622, F0.496)] [D acc: 0.742(R 0.641, F 0.844)] [G loss: 0.559] [G acc: 0.125]\n",
      "3245 [D loss: 0.506(R 0.514, F0.497)] [D acc: 0.773(R 0.703, F 0.844)] [G loss: 0.506] [G acc: 0.125]\n",
      "3246 [D loss: 0.473(R 0.478, F0.469)] [D acc: 0.789(R 0.719, F 0.859)] [G loss: 0.473] [G acc: 0.062]\n",
      "3247 [D loss: 0.537(R 0.557, F0.517)] [D acc: 0.719(R 0.656, F 0.781)] [G loss: 0.537] [G acc: 0.062]\n",
      "3248 [D loss: 0.544(R 0.655, F0.433)] [D acc: 0.711(R 0.562, F 0.859)] [G loss: 0.544] [G acc: 0.188]\n",
      "3249 [D loss: 0.530(R 0.484, F0.576)] [D acc: 0.742(R 0.719, F 0.766)] [G loss: 0.530] [G acc: 0.141]\n",
      "3250 [D loss: 0.519(R 0.498, F0.540)] [D acc: 0.727(R 0.688, F 0.766)] [G loss: 0.519] [G acc: 0.062]\n",
      "3251 [D loss: 0.519(R 0.535, F0.503)] [D acc: 0.727(R 0.641, F 0.812)] [G loss: 0.519] [G acc: 0.188]\n",
      "3252 [D loss: 0.598(R 0.605, F0.591)] [D acc: 0.680(R 0.594, F 0.766)] [G loss: 0.598] [G acc: 0.156]\n",
      "3253 [D loss: 0.511(R 0.539, F0.484)] [D acc: 0.750(R 0.609, F 0.891)] [G loss: 0.511] [G acc: 0.094]\n",
      "3254 [D loss: 0.578(R 0.588, F0.568)] [D acc: 0.703(R 0.641, F 0.766)] [G loss: 0.578] [G acc: 0.125]\n",
      "3255 [D loss: 0.511(R 0.519, F0.503)] [D acc: 0.742(R 0.703, F 0.781)] [G loss: 0.511] [G acc: 0.125]\n",
      "3256 [D loss: 0.599(R 0.608, F0.591)] [D acc: 0.680(R 0.609, F 0.750)] [G loss: 0.599] [G acc: 0.109]\n",
      "3257 [D loss: 0.515(R 0.597, F0.433)] [D acc: 0.727(R 0.609, F 0.844)] [G loss: 0.515] [G acc: 0.094]\n",
      "3258 [D loss: 0.460(R 0.405, F0.516)] [D acc: 0.758(R 0.734, F 0.781)] [G loss: 0.460] [G acc: 0.172]\n",
      "3259 [D loss: 0.509(R 0.523, F0.494)] [D acc: 0.758(R 0.672, F 0.844)] [G loss: 0.509] [G acc: 0.047]\n",
      "3260 [D loss: 0.593(R 0.612, F0.573)] [D acc: 0.695(R 0.578, F 0.812)] [G loss: 0.593] [G acc: 0.141]\n",
      "3261 [D loss: 0.478(R 0.462, F0.494)] [D acc: 0.719(R 0.734, F 0.703)] [G loss: 0.478] [G acc: 0.078]\n",
      "3262 [D loss: 0.736(R 0.971, F0.502)] [D acc: 0.672(R 0.469, F 0.875)] [G loss: 0.736] [G acc: 0.094]\n",
      "3263 [D loss: 0.560(R 0.555, F0.566)] [D acc: 0.727(R 0.656, F 0.797)] [G loss: 0.560] [G acc: 0.094]\n",
      "3264 [D loss: 0.444(R 0.341, F0.547)] [D acc: 0.812(R 0.875, F 0.750)] [G loss: 0.444] [G acc: 0.062]\n",
      "3265 [D loss: 0.635(R 0.597, F0.673)] [D acc: 0.664(R 0.609, F 0.719)] [G loss: 0.635] [G acc: 0.109]\n",
      "3266 [D loss: 0.572(R 0.568, F0.576)] [D acc: 0.672(R 0.625, F 0.719)] [G loss: 0.572] [G acc: 0.141]\n",
      "3267 [D loss: 0.615(R 0.610, F0.620)] [D acc: 0.695(R 0.609, F 0.781)] [G loss: 0.615] [G acc: 0.078]\n",
      "3268 [D loss: 0.472(R 0.506, F0.437)] [D acc: 0.781(R 0.703, F 0.859)] [G loss: 0.472] [G acc: 0.141]\n",
      "3269 [D loss: 0.521(R 0.538, F0.505)] [D acc: 0.719(R 0.688, F 0.750)] [G loss: 0.521] [G acc: 0.141]\n",
      "3270 [D loss: 0.469(R 0.470, F0.469)] [D acc: 0.766(R 0.734, F 0.797)] [G loss: 0.469] [G acc: 0.016]\n",
      "3271 [D loss: 0.465(R 0.479, F0.451)] [D acc: 0.758(R 0.719, F 0.797)] [G loss: 0.465] [G acc: 0.094]\n",
      "3272 [D loss: 0.514(R 0.443, F0.585)] [D acc: 0.742(R 0.703, F 0.781)] [G loss: 0.514] [G acc: 0.031]\n",
      "3273 [D loss: 0.479(R 0.576, F0.383)] [D acc: 0.789(R 0.656, F 0.922)] [G loss: 0.479] [G acc: 0.000]\n",
      "3274 [D loss: 0.560(R 0.609, F0.510)] [D acc: 0.711(R 0.656, F 0.766)] [G loss: 0.560] [G acc: 0.062]\n",
      "3275 [D loss: 0.451(R 0.439, F0.463)] [D acc: 0.805(R 0.781, F 0.828)] [G loss: 0.451] [G acc: 0.078]\n",
      "3276 [D loss: 0.457(R 0.431, F0.483)] [D acc: 0.805(R 0.766, F 0.844)] [G loss: 0.457] [G acc: 0.109]\n",
      "3277 [D loss: 0.586(R 0.635, F0.537)] [D acc: 0.711(R 0.562, F 0.859)] [G loss: 0.586] [G acc: 0.078]\n",
      "3278 [D loss: 0.571(R 0.561, F0.582)] [D acc: 0.680(R 0.641, F 0.719)] [G loss: 0.571] [G acc: 0.094]\n",
      "3279 [D loss: 0.576(R 0.683, F0.470)] [D acc: 0.727(R 0.594, F 0.859)] [G loss: 0.576] [G acc: 0.125]\n",
      "3280 [D loss: 0.555(R 0.529, F0.582)] [D acc: 0.672(R 0.625, F 0.719)] [G loss: 0.555] [G acc: 0.109]\n",
      "3281 [D loss: 0.498(R 0.520, F0.475)] [D acc: 0.734(R 0.672, F 0.797)] [G loss: 0.498] [G acc: 0.125]\n",
      "3282 [D loss: 0.609(R 0.586, F0.632)] [D acc: 0.664(R 0.641, F 0.688)] [G loss: 0.609] [G acc: 0.156]\n",
      "3283 [D loss: 0.621(R 0.737, F0.506)] [D acc: 0.648(R 0.500, F 0.797)] [G loss: 0.621] [G acc: 0.062]\n",
      "3284 [D loss: 0.468(R 0.399, F0.537)] [D acc: 0.805(R 0.781, F 0.828)] [G loss: 0.468] [G acc: 0.141]\n",
      "3285 [D loss: 0.537(R 0.567, F0.507)] [D acc: 0.664(R 0.547, F 0.781)] [G loss: 0.537] [G acc: 0.141]\n",
      "3286 [D loss: 0.504(R 0.562, F0.445)] [D acc: 0.797(R 0.719, F 0.875)] [G loss: 0.504] [G acc: 0.047]\n",
      "3287 [D loss: 0.494(R 0.506, F0.482)] [D acc: 0.758(R 0.672, F 0.844)] [G loss: 0.494] [G acc: 0.141]\n",
      "3288 [D loss: 0.510(R 0.376, F0.643)] [D acc: 0.750(R 0.797, F 0.703)] [G loss: 0.510] [G acc: 0.047]\n",
      "3289 [D loss: 0.436(R 0.511, F0.360)] [D acc: 0.812(R 0.719, F 0.906)] [G loss: 0.436] [G acc: 0.109]\n",
      "3290 [D loss: 0.439(R 0.391, F0.487)] [D acc: 0.766(R 0.719, F 0.812)] [G loss: 0.439] [G acc: 0.047]\n",
      "3291 [D loss: 0.557(R 0.520, F0.594)] [D acc: 0.758(R 0.734, F 0.781)] [G loss: 0.557] [G acc: 0.078]\n",
      "3292 [D loss: 0.470(R 0.440, F0.500)] [D acc: 0.781(R 0.781, F 0.781)] [G loss: 0.470] [G acc: 0.078]\n",
      "3293 [D loss: 0.534(R 0.584, F0.484)] [D acc: 0.703(R 0.625, F 0.781)] [G loss: 0.534] [G acc: 0.047]\n",
      "3294 [D loss: 0.547(R 0.532, F0.562)] [D acc: 0.695(R 0.672, F 0.719)] [G loss: 0.547] [G acc: 0.078]\n",
      "3295 [D loss: 0.514(R 0.600, F0.427)] [D acc: 0.773(R 0.688, F 0.859)] [G loss: 0.514] [G acc: 0.062]\n",
      "3296 [D loss: 0.491(R 0.463, F0.520)] [D acc: 0.789(R 0.750, F 0.828)] [G loss: 0.491] [G acc: 0.094]\n",
      "3297 [D loss: 0.469(R 0.507, F0.431)] [D acc: 0.789(R 0.688, F 0.891)] [G loss: 0.469] [G acc: 0.094]\n",
      "3298 [D loss: 0.555(R 0.647, F0.464)] [D acc: 0.711(R 0.625, F 0.797)] [G loss: 0.555] [G acc: 0.109]\n",
      "3299 [D loss: 0.549(R 0.578, F0.521)] [D acc: 0.742(R 0.672, F 0.812)] [G loss: 0.549] [G acc: 0.125]\n",
      "INFO:tensorflow:Assets written to: ram://22ecb93a-bbe8-4cbc-8e22-a3cc5f5acdcc/assets\n",
      "INFO:tensorflow:Assets written to: ram://8ca59dc0-935e-46bb-b860-33355b0c3db9/assets\n",
      "INFO:tensorflow:Assets written to: ram://188fafe3-bb78-4bcb-b5de-e55f5027d2c8/assets\n",
      "3300 [D loss: 0.555(R 0.451, F0.658)] [D acc: 0.734(R 0.719, F 0.750)] [G loss: 0.555] [G acc: 0.062]\n",
      "3301 [D loss: 0.456(R 0.523, F0.388)] [D acc: 0.828(R 0.734, F 0.922)] [G loss: 0.456] [G acc: 0.031]\n",
      "3302 [D loss: 0.679(R 0.717, F0.640)] [D acc: 0.594(R 0.484, F 0.703)] [G loss: 0.679] [G acc: 0.062]\n",
      "3303 [D loss: 0.514(R 0.476, F0.551)] [D acc: 0.789(R 0.797, F 0.781)] [G loss: 0.514] [G acc: 0.078]\n",
      "3304 [D loss: 0.539(R 0.575, F0.502)] [D acc: 0.719(R 0.625, F 0.812)] [G loss: 0.539] [G acc: 0.094]\n",
      "3305 [D loss: 0.536(R 0.563, F0.508)] [D acc: 0.750(R 0.641, F 0.859)] [G loss: 0.536] [G acc: 0.188]\n",
      "3306 [D loss: 0.519(R 0.598, F0.440)] [D acc: 0.695(R 0.578, F 0.812)] [G loss: 0.519] [G acc: 0.078]\n",
      "3307 [D loss: 0.509(R 0.534, F0.484)] [D acc: 0.711(R 0.672, F 0.750)] [G loss: 0.509] [G acc: 0.062]\n",
      "3308 [D loss: 0.510(R 0.510, F0.509)] [D acc: 0.734(R 0.688, F 0.781)] [G loss: 0.510] [G acc: 0.047]\n",
      "3309 [D loss: 0.524(R 0.495, F0.552)] [D acc: 0.727(R 0.656, F 0.797)] [G loss: 0.524] [G acc: 0.125]\n",
      "3310 [D loss: 0.548(R 0.645, F0.451)] [D acc: 0.727(R 0.562, F 0.891)] [G loss: 0.548] [G acc: 0.141]\n",
      "3311 [D loss: 0.505(R 0.473, F0.538)] [D acc: 0.750(R 0.688, F 0.812)] [G loss: 0.505] [G acc: 0.125]\n",
      "3312 [D loss: 0.517(R 0.537, F0.497)] [D acc: 0.734(R 0.719, F 0.750)] [G loss: 0.517] [G acc: 0.047]\n",
      "3313 [D loss: 0.495(R 0.513, F0.477)] [D acc: 0.742(R 0.703, F 0.781)] [G loss: 0.495] [G acc: 0.109]\n",
      "3314 [D loss: 0.532(R 0.527, F0.538)] [D acc: 0.688(R 0.625, F 0.750)] [G loss: 0.532] [G acc: 0.047]\n",
      "3315 [D loss: 0.553(R 0.554, F0.552)] [D acc: 0.695(R 0.625, F 0.766)] [G loss: 0.553] [G acc: 0.000]\n",
      "3316 [D loss: 0.543(R 0.618, F0.467)] [D acc: 0.711(R 0.625, F 0.797)] [G loss: 0.543] [G acc: 0.141]\n",
      "3317 [D loss: 0.612(R 0.627, F0.598)] [D acc: 0.711(R 0.625, F 0.797)] [G loss: 0.612] [G acc: 0.062]\n",
      "3318 [D loss: 0.583(R 0.638, F0.528)] [D acc: 0.688(R 0.547, F 0.828)] [G loss: 0.583] [G acc: 0.156]\n",
      "3319 [D loss: 0.534(R 0.499, F0.569)] [D acc: 0.758(R 0.734, F 0.781)] [G loss: 0.534] [G acc: 0.047]\n",
      "3320 [D loss: 0.480(R 0.391, F0.569)] [D acc: 0.812(R 0.812, F 0.812)] [G loss: 0.480] [G acc: 0.047]\n",
      "3321 [D loss: 0.497(R 0.578, F0.416)] [D acc: 0.742(R 0.609, F 0.875)] [G loss: 0.497] [G acc: 0.078]\n",
      "3322 [D loss: 0.514(R 0.569, F0.459)] [D acc: 0.750(R 0.656, F 0.844)] [G loss: 0.514] [G acc: 0.078]\n",
      "3323 [D loss: 0.532(R 0.446, F0.618)] [D acc: 0.719(R 0.703, F 0.734)] [G loss: 0.532] [G acc: 0.062]\n",
      "3324 [D loss: 0.446(R 0.464, F0.428)] [D acc: 0.820(R 0.797, F 0.844)] [G loss: 0.446] [G acc: 0.078]\n",
      "3325 [D loss: 0.749(R 0.574, F0.924)] [D acc: 0.648(R 0.672, F 0.625)] [G loss: 0.749] [G acc: 0.062]\n",
      "3326 [D loss: 0.583(R 0.682, F0.485)] [D acc: 0.688(R 0.578, F 0.797)] [G loss: 0.583] [G acc: 0.062]\n",
      "3327 [D loss: 0.566(R 0.673, F0.459)] [D acc: 0.719(R 0.594, F 0.844)] [G loss: 0.566] [G acc: 0.078]\n",
      "3328 [D loss: 0.542(R 0.626, F0.459)] [D acc: 0.688(R 0.578, F 0.797)] [G loss: 0.542] [G acc: 0.047]\n",
      "3329 [D loss: 0.414(R 0.421, F0.406)] [D acc: 0.797(R 0.734, F 0.859)] [G loss: 0.414] [G acc: 0.188]\n",
      "3330 [D loss: 0.620(R 0.635, F0.606)] [D acc: 0.625(R 0.500, F 0.750)] [G loss: 0.620] [G acc: 0.094]\n",
      "3331 [D loss: 0.525(R 0.565, F0.486)] [D acc: 0.703(R 0.609, F 0.797)] [G loss: 0.525] [G acc: 0.094]\n",
      "3332 [D loss: 0.524(R 0.537, F0.512)] [D acc: 0.750(R 0.672, F 0.828)] [G loss: 0.524] [G acc: 0.125]\n",
      "3333 [D loss: 0.498(R 0.501, F0.495)] [D acc: 0.750(R 0.703, F 0.797)] [G loss: 0.498] [G acc: 0.094]\n",
      "3334 [D loss: 0.509(R 0.560, F0.457)] [D acc: 0.742(R 0.656, F 0.828)] [G loss: 0.509] [G acc: 0.141]\n",
      "3335 [D loss: 0.416(R 0.433, F0.400)] [D acc: 0.812(R 0.766, F 0.859)] [G loss: 0.416] [G acc: 0.172]\n",
      "3336 [D loss: 0.574(R 0.616, F0.531)] [D acc: 0.703(R 0.609, F 0.797)] [G loss: 0.574] [G acc: 0.094]\n",
      "3337 [D loss: 0.477(R 0.483, F0.472)] [D acc: 0.797(R 0.750, F 0.844)] [G loss: 0.477] [G acc: 0.125]\n",
      "3338 [D loss: 0.452(R 0.534, F0.370)] [D acc: 0.781(R 0.641, F 0.922)] [G loss: 0.452] [G acc: 0.062]\n",
      "3339 [D loss: 0.471(R 0.460, F0.482)] [D acc: 0.766(R 0.703, F 0.828)] [G loss: 0.471] [G acc: 0.125]\n",
      "3340 [D loss: 0.577(R 0.614, F0.540)] [D acc: 0.727(R 0.594, F 0.859)] [G loss: 0.577] [G acc: 0.062]\n",
      "3341 [D loss: 0.542(R 0.626, F0.458)] [D acc: 0.688(R 0.578, F 0.797)] [G loss: 0.542] [G acc: 0.141]\n",
      "3342 [D loss: 0.435(R 0.411, F0.459)] [D acc: 0.797(R 0.766, F 0.828)] [G loss: 0.435] [G acc: 0.078]\n",
      "3343 [D loss: 0.531(R 0.595, F0.468)] [D acc: 0.688(R 0.625, F 0.750)] [G loss: 0.531] [G acc: 0.109]\n",
      "3344 [D loss: 0.542(R 0.522, F0.561)] [D acc: 0.719(R 0.703, F 0.734)] [G loss: 0.542] [G acc: 0.141]\n",
      "3345 [D loss: 0.605(R 0.666, F0.544)] [D acc: 0.703(R 0.562, F 0.844)] [G loss: 0.605] [G acc: 0.078]\n",
      "3346 [D loss: 0.500(R 0.494, F0.507)] [D acc: 0.750(R 0.688, F 0.812)] [G loss: 0.500] [G acc: 0.094]\n",
      "3347 [D loss: 0.563(R 0.515, F0.612)] [D acc: 0.703(R 0.688, F 0.719)] [G loss: 0.563] [G acc: 0.109]\n",
      "3348 [D loss: 0.618(R 0.697, F0.538)] [D acc: 0.648(R 0.547, F 0.750)] [G loss: 0.618] [G acc: 0.109]\n",
      "3349 [D loss: 0.524(R 0.524, F0.524)] [D acc: 0.711(R 0.703, F 0.719)] [G loss: 0.524] [G acc: 0.078]\n",
      "3350 [D loss: 0.509(R 0.634, F0.384)] [D acc: 0.742(R 0.594, F 0.891)] [G loss: 0.509] [G acc: 0.062]\n",
      "3351 [D loss: 0.547(R 0.536, F0.559)] [D acc: 0.727(R 0.672, F 0.781)] [G loss: 0.547] [G acc: 0.094]\n",
      "3352 [D loss: 0.514(R 0.362, F0.667)] [D acc: 0.781(R 0.781, F 0.781)] [G loss: 0.514] [G acc: 0.047]\n",
      "3353 [D loss: 0.499(R 0.506, F0.492)] [D acc: 0.773(R 0.656, F 0.891)] [G loss: 0.499] [G acc: 0.078]\n",
      "3354 [D loss: 0.534(R 0.624, F0.444)] [D acc: 0.766(R 0.703, F 0.828)] [G loss: 0.534] [G acc: 0.078]\n",
      "3355 [D loss: 0.508(R 0.606, F0.409)] [D acc: 0.766(R 0.672, F 0.859)] [G loss: 0.508] [G acc: 0.062]\n",
      "3356 [D loss: 0.493(R 0.554, F0.433)] [D acc: 0.750(R 0.641, F 0.859)] [G loss: 0.493] [G acc: 0.125]\n",
      "3357 [D loss: 0.409(R 0.441, F0.377)] [D acc: 0.812(R 0.734, F 0.891)] [G loss: 0.409] [G acc: 0.078]\n",
      "3358 [D loss: 0.472(R 0.416, F0.528)] [D acc: 0.781(R 0.766, F 0.797)] [G loss: 0.472] [G acc: 0.047]\n",
      "3359 [D loss: 0.522(R 0.462, F0.582)] [D acc: 0.789(R 0.750, F 0.828)] [G loss: 0.522] [G acc: 0.125]\n",
      "3360 [D loss: 0.501(R 0.592, F0.409)] [D acc: 0.766(R 0.672, F 0.859)] [G loss: 0.501] [G acc: 0.031]\n",
      "3361 [D loss: 0.567(R 0.586, F0.547)] [D acc: 0.766(R 0.656, F 0.875)] [G loss: 0.567] [G acc: 0.172]\n",
      "3362 [D loss: 0.522(R 0.480, F0.565)] [D acc: 0.750(R 0.719, F 0.781)] [G loss: 0.522] [G acc: 0.047]\n",
      "3363 [D loss: 0.487(R 0.490, F0.483)] [D acc: 0.789(R 0.750, F 0.828)] [G loss: 0.487] [G acc: 0.125]\n",
      "3364 [D loss: 0.580(R 0.480, F0.681)] [D acc: 0.703(R 0.703, F 0.703)] [G loss: 0.580] [G acc: 0.047]\n",
      "3365 [D loss: 0.579(R 0.607, F0.551)] [D acc: 0.711(R 0.641, F 0.781)] [G loss: 0.579] [G acc: 0.047]\n",
      "3366 [D loss: 0.484(R 0.580, F0.388)] [D acc: 0.797(R 0.719, F 0.875)] [G loss: 0.484] [G acc: 0.109]\n",
      "3367 [D loss: 0.524(R 0.546, F0.503)] [D acc: 0.727(R 0.656, F 0.797)] [G loss: 0.524] [G acc: 0.094]\n",
      "3368 [D loss: 0.543(R 0.546, F0.540)] [D acc: 0.727(R 0.656, F 0.797)] [G loss: 0.543] [G acc: 0.016]\n",
      "3369 [D loss: 0.578(R 0.678, F0.478)] [D acc: 0.711(R 0.547, F 0.875)] [G loss: 0.578] [G acc: 0.078]\n",
      "3370 [D loss: 0.512(R 0.530, F0.494)] [D acc: 0.734(R 0.703, F 0.766)] [G loss: 0.512] [G acc: 0.078]\n",
      "3371 [D loss: 0.547(R 0.568, F0.527)] [D acc: 0.734(R 0.688, F 0.781)] [G loss: 0.547] [G acc: 0.141]\n",
      "3372 [D loss: 0.466(R 0.474, F0.457)] [D acc: 0.805(R 0.734, F 0.875)] [G loss: 0.466] [G acc: 0.062]\n",
      "3373 [D loss: 0.504(R 0.485, F0.522)] [D acc: 0.758(R 0.688, F 0.828)] [G loss: 0.504] [G acc: 0.094]\n",
      "3374 [D loss: 0.597(R 0.602, F0.592)] [D acc: 0.664(R 0.578, F 0.750)] [G loss: 0.597] [G acc: 0.125]\n",
      "3375 [D loss: 0.547(R 0.593, F0.500)] [D acc: 0.734(R 0.641, F 0.828)] [G loss: 0.547] [G acc: 0.078]\n",
      "3376 [D loss: 0.460(R 0.486, F0.434)] [D acc: 0.750(R 0.656, F 0.844)] [G loss: 0.460] [G acc: 0.094]\n",
      "3377 [D loss: 0.542(R 0.520, F0.565)] [D acc: 0.727(R 0.719, F 0.734)] [G loss: 0.542] [G acc: 0.047]\n",
      "3378 [D loss: 0.490(R 0.564, F0.415)] [D acc: 0.742(R 0.625, F 0.859)] [G loss: 0.490] [G acc: 0.094]\n",
      "3379 [D loss: 0.470(R 0.488, F0.451)] [D acc: 0.773(R 0.719, F 0.828)] [G loss: 0.470] [G acc: 0.109]\n",
      "3380 [D loss: 0.698(R 0.607, F0.790)] [D acc: 0.680(R 0.594, F 0.766)] [G loss: 0.698] [G acc: 0.078]\n",
      "3381 [D loss: 0.517(R 0.570, F0.463)] [D acc: 0.727(R 0.609, F 0.844)] [G loss: 0.517] [G acc: 0.031]\n",
      "3382 [D loss: 0.532(R 0.643, F0.420)] [D acc: 0.742(R 0.609, F 0.875)] [G loss: 0.532] [G acc: 0.156]\n",
      "3383 [D loss: 0.547(R 0.500, F0.593)] [D acc: 0.797(R 0.766, F 0.828)] [G loss: 0.547] [G acc: 0.141]\n",
      "3384 [D loss: 0.456(R 0.468, F0.443)] [D acc: 0.828(R 0.781, F 0.875)] [G loss: 0.456] [G acc: 0.156]\n",
      "3385 [D loss: 0.571(R 0.601, F0.542)] [D acc: 0.703(R 0.578, F 0.828)] [G loss: 0.571] [G acc: 0.125]\n",
      "3386 [D loss: 0.572(R 0.654, F0.489)] [D acc: 0.711(R 0.609, F 0.812)] [G loss: 0.572] [G acc: 0.156]\n",
      "3387 [D loss: 0.513(R 0.484, F0.541)] [D acc: 0.766(R 0.703, F 0.828)] [G loss: 0.513] [G acc: 0.172]\n",
      "3388 [D loss: 0.500(R 0.515, F0.485)] [D acc: 0.773(R 0.703, F 0.844)] [G loss: 0.500] [G acc: 0.172]\n",
      "3389 [D loss: 0.558(R 0.521, F0.595)] [D acc: 0.727(R 0.719, F 0.734)] [G loss: 0.558] [G acc: 0.156]\n",
      "3390 [D loss: 0.521(R 0.450, F0.591)] [D acc: 0.719(R 0.703, F 0.734)] [G loss: 0.521] [G acc: 0.078]\n",
      "3391 [D loss: 0.582(R 0.566, F0.598)] [D acc: 0.664(R 0.672, F 0.656)] [G loss: 0.582] [G acc: 0.047]\n",
      "3392 [D loss: 0.639(R 0.725, F0.554)] [D acc: 0.656(R 0.547, F 0.766)] [G loss: 0.639] [G acc: 0.078]\n",
      "3393 [D loss: 0.487(R 0.509, F0.466)] [D acc: 0.797(R 0.734, F 0.859)] [G loss: 0.487] [G acc: 0.094]\n",
      "3394 [D loss: 0.698(R 0.526, F0.871)] [D acc: 0.703(R 0.703, F 0.703)] [G loss: 0.698] [G acc: 0.031]\n",
      "3395 [D loss: 0.582(R 0.705, F0.458)] [D acc: 0.695(R 0.500, F 0.891)] [G loss: 0.582] [G acc: 0.094]\n",
      "3396 [D loss: 0.556(R 0.545, F0.568)] [D acc: 0.711(R 0.688, F 0.734)] [G loss: 0.556] [G acc: 0.109]\n",
      "3397 [D loss: 0.532(R 0.618, F0.445)] [D acc: 0.734(R 0.625, F 0.844)] [G loss: 0.532] [G acc: 0.031]\n",
      "3398 [D loss: 0.460(R 0.458, F0.462)] [D acc: 0.781(R 0.766, F 0.797)] [G loss: 0.460] [G acc: 0.141]\n",
      "3399 [D loss: 0.455(R 0.415, F0.495)] [D acc: 0.805(R 0.781, F 0.828)] [G loss: 0.455] [G acc: 0.094]\n",
      "INFO:tensorflow:Assets written to: ram://603f876a-38b9-4308-9964-5397063cdf8e/assets\n",
      "INFO:tensorflow:Assets written to: ram://7da73970-6b02-4c46-bd3c-bad7b92c2bfc/assets\n",
      "INFO:tensorflow:Assets written to: ram://bfd4ef08-e392-4ec2-877c-390c1ec87c82/assets\n",
      "3400 [D loss: 0.611(R 0.524, F0.698)] [D acc: 0.695(R 0.656, F 0.734)] [G loss: 0.611] [G acc: 0.094]\n",
      "3401 [D loss: 0.465(R 0.515, F0.414)] [D acc: 0.797(R 0.688, F 0.906)] [G loss: 0.465] [G acc: 0.172]\n",
      "3402 [D loss: 0.520(R 0.547, F0.493)] [D acc: 0.742(R 0.734, F 0.750)] [G loss: 0.520] [G acc: 0.203]\n",
      "3403 [D loss: 0.539(R 0.573, F0.506)] [D acc: 0.750(R 0.641, F 0.859)] [G loss: 0.539] [G acc: 0.203]\n",
      "3404 [D loss: 0.546(R 0.499, F0.593)] [D acc: 0.711(R 0.719, F 0.703)] [G loss: 0.546] [G acc: 0.047]\n",
      "3405 [D loss: 0.547(R 0.523, F0.572)] [D acc: 0.719(R 0.688, F 0.750)] [G loss: 0.547] [G acc: 0.062]\n",
      "3406 [D loss: 0.564(R 0.579, F0.549)] [D acc: 0.664(R 0.562, F 0.766)] [G loss: 0.564] [G acc: 0.078]\n",
      "3407 [D loss: 0.546(R 0.511, F0.580)] [D acc: 0.742(R 0.750, F 0.734)] [G loss: 0.546] [G acc: 0.078]\n",
      "3408 [D loss: 0.522(R 0.550, F0.495)] [D acc: 0.742(R 0.672, F 0.812)] [G loss: 0.522] [G acc: 0.078]\n",
      "3409 [D loss: 0.466(R 0.399, F0.533)] [D acc: 0.781(R 0.812, F 0.750)] [G loss: 0.466] [G acc: 0.047]\n",
      "3410 [D loss: 0.440(R 0.480, F0.400)] [D acc: 0.781(R 0.688, F 0.875)] [G loss: 0.440] [G acc: 0.078]\n",
      "3411 [D loss: 0.641(R 0.544, F0.738)] [D acc: 0.703(R 0.672, F 0.734)] [G loss: 0.641] [G acc: 0.141]\n",
      "3412 [D loss: 0.526(R 0.659, F0.392)] [D acc: 0.742(R 0.594, F 0.891)] [G loss: 0.526] [G acc: 0.062]\n",
      "3413 [D loss: 0.633(R 0.623, F0.644)] [D acc: 0.641(R 0.594, F 0.688)] [G loss: 0.633] [G acc: 0.078]\n",
      "3414 [D loss: 0.586(R 0.723, F0.449)] [D acc: 0.742(R 0.609, F 0.875)] [G loss: 0.586] [G acc: 0.078]\n",
      "3415 [D loss: 0.436(R 0.434, F0.439)] [D acc: 0.805(R 0.750, F 0.859)] [G loss: 0.436] [G acc: 0.062]\n",
      "3416 [D loss: 0.502(R 0.461, F0.543)] [D acc: 0.781(R 0.750, F 0.812)] [G loss: 0.502] [G acc: 0.016]\n",
      "3417 [D loss: 0.533(R 0.558, F0.508)] [D acc: 0.750(R 0.656, F 0.844)] [G loss: 0.533] [G acc: 0.188]\n",
      "3418 [D loss: 0.467(R 0.543, F0.390)] [D acc: 0.797(R 0.688, F 0.906)] [G loss: 0.467] [G acc: 0.094]\n",
      "3419 [D loss: 0.555(R 0.689, F0.421)] [D acc: 0.711(R 0.578, F 0.844)] [G loss: 0.555] [G acc: 0.156]\n",
      "3420 [D loss: 0.420(R 0.368, F0.471)] [D acc: 0.805(R 0.812, F 0.797)] [G loss: 0.420] [G acc: 0.156]\n",
      "3421 [D loss: 0.529(R 0.414, F0.645)] [D acc: 0.773(R 0.781, F 0.766)] [G loss: 0.529] [G acc: 0.094]\n",
      "3422 [D loss: 0.522(R 0.547, F0.498)] [D acc: 0.695(R 0.672, F 0.719)] [G loss: 0.522] [G acc: 0.156]\n",
      "3423 [D loss: 0.506(R 0.521, F0.492)] [D acc: 0.734(R 0.688, F 0.781)] [G loss: 0.506] [G acc: 0.141]\n",
      "3424 [D loss: 0.474(R 0.413, F0.535)] [D acc: 0.758(R 0.781, F 0.734)] [G loss: 0.474] [G acc: 0.031]\n",
      "3425 [D loss: 0.627(R 0.611, F0.643)] [D acc: 0.688(R 0.672, F 0.703)] [G loss: 0.627] [G acc: 0.094]\n",
      "3426 [D loss: 0.510(R 0.579, F0.442)] [D acc: 0.758(R 0.672, F 0.844)] [G loss: 0.510] [G acc: 0.094]\n",
      "3427 [D loss: 0.546(R 0.542, F0.550)] [D acc: 0.703(R 0.656, F 0.750)] [G loss: 0.546] [G acc: 0.156]\n",
      "3428 [D loss: 0.526(R 0.592, F0.461)] [D acc: 0.750(R 0.641, F 0.859)] [G loss: 0.526] [G acc: 0.062]\n",
      "3429 [D loss: 0.506(R 0.529, F0.482)] [D acc: 0.758(R 0.703, F 0.812)] [G loss: 0.506] [G acc: 0.078]\n",
      "3430 [D loss: 0.518(R 0.585, F0.451)] [D acc: 0.766(R 0.688, F 0.844)] [G loss: 0.518] [G acc: 0.078]\n",
      "3431 [D loss: 0.565(R 0.620, F0.511)] [D acc: 0.742(R 0.656, F 0.828)] [G loss: 0.565] [G acc: 0.109]\n",
      "3432 [D loss: 0.465(R 0.544, F0.386)] [D acc: 0.797(R 0.672, F 0.922)] [G loss: 0.465] [G acc: 0.094]\n",
      "3433 [D loss: 0.509(R 0.498, F0.521)] [D acc: 0.773(R 0.734, F 0.812)] [G loss: 0.509] [G acc: 0.078]\n",
      "3434 [D loss: 0.551(R 0.543, F0.559)] [D acc: 0.688(R 0.656, F 0.719)] [G loss: 0.551] [G acc: 0.062]\n",
      "3435 [D loss: 0.553(R 0.593, F0.513)] [D acc: 0.711(R 0.641, F 0.781)] [G loss: 0.553] [G acc: 0.078]\n",
      "3436 [D loss: 0.531(R 0.542, F0.520)] [D acc: 0.766(R 0.734, F 0.797)] [G loss: 0.531] [G acc: 0.141]\n",
      "3437 [D loss: 0.466(R 0.502, F0.429)] [D acc: 0.758(R 0.656, F 0.859)] [G loss: 0.466] [G acc: 0.109]\n",
      "3438 [D loss: 0.520(R 0.465, F0.576)] [D acc: 0.719(R 0.734, F 0.703)] [G loss: 0.520] [G acc: 0.062]\n",
      "3439 [D loss: 0.458(R 0.509, F0.407)] [D acc: 0.789(R 0.734, F 0.844)] [G loss: 0.458] [G acc: 0.156]\n",
      "3440 [D loss: 0.553(R 0.501, F0.606)] [D acc: 0.719(R 0.703, F 0.734)] [G loss: 0.553] [G acc: 0.109]\n",
      "3441 [D loss: 0.573(R 0.594, F0.551)] [D acc: 0.734(R 0.656, F 0.812)] [G loss: 0.573] [G acc: 0.078]\n",
      "3442 [D loss: 0.547(R 0.630, F0.465)] [D acc: 0.734(R 0.594, F 0.875)] [G loss: 0.547] [G acc: 0.109]\n",
      "3443 [D loss: 0.500(R 0.530, F0.470)] [D acc: 0.797(R 0.719, F 0.875)] [G loss: 0.500] [G acc: 0.094]\n",
      "3444 [D loss: 0.625(R 0.702, F0.548)] [D acc: 0.656(R 0.531, F 0.781)] [G loss: 0.625] [G acc: 0.078]\n",
      "3445 [D loss: 0.550(R 0.569, F0.531)] [D acc: 0.695(R 0.641, F 0.750)] [G loss: 0.550] [G acc: 0.094]\n",
      "3446 [D loss: 0.539(R 0.566, F0.513)] [D acc: 0.734(R 0.625, F 0.844)] [G loss: 0.539] [G acc: 0.172]\n",
      "3447 [D loss: 0.556(R 0.528, F0.584)] [D acc: 0.688(R 0.594, F 0.781)] [G loss: 0.556] [G acc: 0.188]\n",
      "3448 [D loss: 0.504(R 0.412, F0.595)] [D acc: 0.773(R 0.828, F 0.719)] [G loss: 0.504] [G acc: 0.062]\n",
      "3449 [D loss: 0.531(R 0.447, F0.615)] [D acc: 0.758(R 0.781, F 0.734)] [G loss: 0.531] [G acc: 0.094]\n",
      "3450 [D loss: 0.546(R 0.617, F0.475)] [D acc: 0.789(R 0.656, F 0.922)] [G loss: 0.546] [G acc: 0.047]\n",
      "3451 [D loss: 0.424(R 0.403, F0.445)] [D acc: 0.820(R 0.781, F 0.859)] [G loss: 0.424] [G acc: 0.188]\n",
      "3452 [D loss: 0.521(R 0.551, F0.490)] [D acc: 0.758(R 0.688, F 0.828)] [G loss: 0.521] [G acc: 0.078]\n",
      "3453 [D loss: 0.599(R 0.541, F0.656)] [D acc: 0.711(R 0.688, F 0.734)] [G loss: 0.599] [G acc: 0.141]\n",
      "3454 [D loss: 0.604(R 0.700, F0.507)] [D acc: 0.656(R 0.547, F 0.766)] [G loss: 0.604] [G acc: 0.094]\n",
      "3455 [D loss: 0.500(R 0.540, F0.460)] [D acc: 0.766(R 0.672, F 0.859)] [G loss: 0.500] [G acc: 0.109]\n",
      "3456 [D loss: 0.501(R 0.445, F0.556)] [D acc: 0.773(R 0.750, F 0.797)] [G loss: 0.501] [G acc: 0.125]\n",
      "3457 [D loss: 0.541(R 0.613, F0.468)] [D acc: 0.727(R 0.641, F 0.812)] [G loss: 0.541] [G acc: 0.062]\n",
      "3458 [D loss: 0.528(R 0.513, F0.543)] [D acc: 0.766(R 0.719, F 0.812)] [G loss: 0.528] [G acc: 0.078]\n",
      "3459 [D loss: 0.567(R 0.650, F0.484)] [D acc: 0.688(R 0.562, F 0.812)] [G loss: 0.567] [G acc: 0.188]\n",
      "3460 [D loss: 0.462(R 0.467, F0.458)] [D acc: 0.773(R 0.719, F 0.828)] [G loss: 0.462] [G acc: 0.094]\n",
      "3461 [D loss: 0.437(R 0.473, F0.401)] [D acc: 0.773(R 0.688, F 0.859)] [G loss: 0.437] [G acc: 0.094]\n",
      "3462 [D loss: 0.512(R 0.483, F0.540)] [D acc: 0.742(R 0.719, F 0.766)] [G loss: 0.512] [G acc: 0.031]\n",
      "3463 [D loss: 0.589(R 0.655, F0.524)] [D acc: 0.680(R 0.562, F 0.797)] [G loss: 0.589] [G acc: 0.047]\n",
      "3464 [D loss: 0.588(R 0.635, F0.541)] [D acc: 0.695(R 0.625, F 0.766)] [G loss: 0.588] [G acc: 0.094]\n",
      "3465 [D loss: 0.556(R 0.584, F0.527)] [D acc: 0.695(R 0.625, F 0.766)] [G loss: 0.556] [G acc: 0.172]\n",
      "3466 [D loss: 0.512(R 0.568, F0.457)] [D acc: 0.773(R 0.672, F 0.875)] [G loss: 0.512] [G acc: 0.078]\n",
      "3467 [D loss: 0.412(R 0.346, F0.478)] [D acc: 0.836(R 0.797, F 0.875)] [G loss: 0.412] [G acc: 0.062]\n",
      "3468 [D loss: 0.462(R 0.491, F0.434)] [D acc: 0.758(R 0.703, F 0.812)] [G loss: 0.462] [G acc: 0.125]\n",
      "3469 [D loss: 0.599(R 0.530, F0.668)] [D acc: 0.711(R 0.688, F 0.734)] [G loss: 0.599] [G acc: 0.094]\n",
      "3470 [D loss: 0.510(R 0.608, F0.413)] [D acc: 0.758(R 0.625, F 0.891)] [G loss: 0.510] [G acc: 0.094]\n",
      "3471 [D loss: 0.565(R 0.523, F0.607)] [D acc: 0.727(R 0.703, F 0.750)] [G loss: 0.565] [G acc: 0.078]\n",
      "3472 [D loss: 0.607(R 0.654, F0.560)] [D acc: 0.664(R 0.562, F 0.766)] [G loss: 0.607] [G acc: 0.062]\n",
      "3473 [D loss: 0.486(R 0.518, F0.454)] [D acc: 0.758(R 0.672, F 0.844)] [G loss: 0.486] [G acc: 0.094]\n",
      "3474 [D loss: 0.560(R 0.629, F0.490)] [D acc: 0.750(R 0.641, F 0.859)] [G loss: 0.560] [G acc: 0.078]\n",
      "3475 [D loss: 0.528(R 0.561, F0.496)] [D acc: 0.750(R 0.641, F 0.859)] [G loss: 0.528] [G acc: 0.062]\n",
      "3476 [D loss: 0.464(R 0.497, F0.431)] [D acc: 0.750(R 0.656, F 0.844)] [G loss: 0.464] [G acc: 0.094]\n",
      "3477 [D loss: 0.541(R 0.656, F0.426)] [D acc: 0.766(R 0.641, F 0.891)] [G loss: 0.541] [G acc: 0.125]\n",
      "3478 [D loss: 0.555(R 0.508, F0.603)] [D acc: 0.711(R 0.672, F 0.750)] [G loss: 0.555] [G acc: 0.141]\n",
      "3479 [D loss: 0.490(R 0.514, F0.466)] [D acc: 0.773(R 0.672, F 0.875)] [G loss: 0.490] [G acc: 0.109]\n",
      "3480 [D loss: 0.555(R 0.573, F0.536)] [D acc: 0.727(R 0.656, F 0.797)] [G loss: 0.555] [G acc: 0.094]\n",
      "3481 [D loss: 0.514(R 0.544, F0.484)] [D acc: 0.734(R 0.688, F 0.781)] [G loss: 0.514] [G acc: 0.094]\n",
      "3482 [D loss: 0.487(R 0.561, F0.413)] [D acc: 0.773(R 0.641, F 0.906)] [G loss: 0.487] [G acc: 0.047]\n",
      "3483 [D loss: 0.542(R 0.515, F0.569)] [D acc: 0.703(R 0.672, F 0.734)] [G loss: 0.542] [G acc: 0.094]\n",
      "3484 [D loss: 0.472(R 0.420, F0.523)] [D acc: 0.758(R 0.781, F 0.734)] [G loss: 0.472] [G acc: 0.031]\n",
      "3485 [D loss: 0.531(R 0.548, F0.513)] [D acc: 0.727(R 0.641, F 0.812)] [G loss: 0.531] [G acc: 0.062]\n",
      "3486 [D loss: 0.570(R 0.621, F0.519)] [D acc: 0.695(R 0.562, F 0.828)] [G loss: 0.570] [G acc: 0.047]\n",
      "3487 [D loss: 0.549(R 0.597, F0.502)] [D acc: 0.734(R 0.656, F 0.812)] [G loss: 0.549] [G acc: 0.109]\n",
      "3488 [D loss: 0.588(R 0.520, F0.655)] [D acc: 0.719(R 0.688, F 0.750)] [G loss: 0.588] [G acc: 0.125]\n",
      "3489 [D loss: 0.569(R 0.607, F0.532)] [D acc: 0.711(R 0.594, F 0.828)] [G loss: 0.569] [G acc: 0.109]\n",
      "3490 [D loss: 0.493(R 0.487, F0.499)] [D acc: 0.742(R 0.719, F 0.766)] [G loss: 0.493] [G acc: 0.062]\n",
      "3491 [D loss: 0.419(R 0.419, F0.420)] [D acc: 0.812(R 0.766, F 0.859)] [G loss: 0.419] [G acc: 0.094]\n",
      "3492 [D loss: 0.600(R 0.502, F0.698)] [D acc: 0.695(R 0.703, F 0.688)] [G loss: 0.600] [G acc: 0.078]\n",
      "3493 [D loss: 0.507(R 0.644, F0.369)] [D acc: 0.750(R 0.578, F 0.922)] [G loss: 0.507] [G acc: 0.062]\n",
      "3494 [D loss: 0.624(R 0.633, F0.614)] [D acc: 0.703(R 0.641, F 0.766)] [G loss: 0.624] [G acc: 0.031]\n",
      "3495 [D loss: 0.549(R 0.580, F0.518)] [D acc: 0.695(R 0.625, F 0.766)] [G loss: 0.549] [G acc: 0.109]\n",
      "3496 [D loss: 0.526(R 0.612, F0.440)] [D acc: 0.750(R 0.594, F 0.906)] [G loss: 0.526] [G acc: 0.078]\n",
      "3497 [D loss: 0.458(R 0.480, F0.437)] [D acc: 0.797(R 0.719, F 0.875)] [G loss: 0.458] [G acc: 0.094]\n",
      "3498 [D loss: 0.465(R 0.449, F0.481)] [D acc: 0.789(R 0.797, F 0.781)] [G loss: 0.465] [G acc: 0.172]\n",
      "3499 [D loss: 0.473(R 0.454, F0.492)] [D acc: 0.797(R 0.766, F 0.828)] [G loss: 0.473] [G acc: 0.125]\n",
      "INFO:tensorflow:Assets written to: ram://2227b1e5-6304-47f5-8922-ccbb82d899c7/assets\n",
      "INFO:tensorflow:Assets written to: ram://5ea881d6-84ea-473c-bb54-513bd5f9b41f/assets\n",
      "INFO:tensorflow:Assets written to: ram://52fdb700-f6ac-43b0-ad00-165015895369/assets\n",
      "3500 [D loss: 0.557(R 0.520, F0.595)] [D acc: 0.711(R 0.734, F 0.688)] [G loss: 0.557] [G acc: 0.109]\n",
      "3501 [D loss: 0.561(R 0.677, F0.444)] [D acc: 0.711(R 0.562, F 0.859)] [G loss: 0.561] [G acc: 0.078]\n",
      "3502 [D loss: 0.613(R 0.710, F0.516)] [D acc: 0.656(R 0.547, F 0.766)] [G loss: 0.613] [G acc: 0.062]\n",
      "3503 [D loss: 0.564(R 0.537, F0.592)] [D acc: 0.711(R 0.656, F 0.766)] [G loss: 0.564] [G acc: 0.156]\n",
      "3504 [D loss: 0.517(R 0.568, F0.466)] [D acc: 0.766(R 0.641, F 0.891)] [G loss: 0.517] [G acc: 0.078]\n",
      "3505 [D loss: 0.507(R 0.473, F0.541)] [D acc: 0.719(R 0.719, F 0.719)] [G loss: 0.507] [G acc: 0.062]\n",
      "3506 [D loss: 0.561(R 0.516, F0.605)] [D acc: 0.703(R 0.719, F 0.688)] [G loss: 0.561] [G acc: 0.031]\n",
      "3507 [D loss: 0.621(R 0.659, F0.582)] [D acc: 0.703(R 0.609, F 0.797)] [G loss: 0.621] [G acc: 0.094]\n",
      "3508 [D loss: 0.565(R 0.560, F0.570)] [D acc: 0.727(R 0.688, F 0.766)] [G loss: 0.565] [G acc: 0.156]\n",
      "3509 [D loss: 0.516(R 0.608, F0.425)] [D acc: 0.719(R 0.625, F 0.812)] [G loss: 0.516] [G acc: 0.141]\n",
      "3510 [D loss: 0.534(R 0.500, F0.567)] [D acc: 0.758(R 0.703, F 0.812)] [G loss: 0.534] [G acc: 0.125]\n",
      "3511 [D loss: 0.555(R 0.503, F0.606)] [D acc: 0.672(R 0.641, F 0.703)] [G loss: 0.555] [G acc: 0.109]\n",
      "3512 [D loss: 0.482(R 0.457, F0.507)] [D acc: 0.781(R 0.719, F 0.844)] [G loss: 0.482] [G acc: 0.141]\n",
      "3513 [D loss: 0.520(R 0.543, F0.498)] [D acc: 0.727(R 0.672, F 0.781)] [G loss: 0.520] [G acc: 0.141]\n",
      "3514 [D loss: 0.429(R 0.438, F0.419)] [D acc: 0.828(R 0.750, F 0.906)] [G loss: 0.429] [G acc: 0.016]\n",
      "3515 [D loss: 0.537(R 0.430, F0.644)] [D acc: 0.727(R 0.734, F 0.719)] [G loss: 0.537] [G acc: 0.125]\n",
      "3516 [D loss: 0.512(R 0.636, F0.387)] [D acc: 0.695(R 0.531, F 0.859)] [G loss: 0.512] [G acc: 0.016]\n",
      "3517 [D loss: 0.432(R 0.392, F0.471)] [D acc: 0.781(R 0.766, F 0.797)] [G loss: 0.432] [G acc: 0.016]\n",
      "3518 [D loss: 0.541(R 0.463, F0.619)] [D acc: 0.758(R 0.719, F 0.797)] [G loss: 0.541] [G acc: 0.062]\n",
      "3519 [D loss: 0.533(R 0.667, F0.399)] [D acc: 0.773(R 0.672, F 0.875)] [G loss: 0.533] [G acc: 0.094]\n",
      "3520 [D loss: 0.498(R 0.396, F0.600)] [D acc: 0.766(R 0.797, F 0.734)] [G loss: 0.498] [G acc: 0.031]\n",
      "3521 [D loss: 0.475(R 0.562, F0.387)] [D acc: 0.742(R 0.625, F 0.859)] [G loss: 0.475] [G acc: 0.156]\n",
      "3522 [D loss: 0.473(R 0.436, F0.509)] [D acc: 0.742(R 0.734, F 0.750)] [G loss: 0.473] [G acc: 0.156]\n",
      "3523 [D loss: 0.466(R 0.541, F0.391)] [D acc: 0.742(R 0.609, F 0.875)] [G loss: 0.466] [G acc: 0.062]\n",
      "3524 [D loss: 0.539(R 0.476, F0.601)] [D acc: 0.789(R 0.734, F 0.844)] [G loss: 0.539] [G acc: 0.047]\n",
      "3525 [D loss: 0.523(R 0.578, F0.468)] [D acc: 0.750(R 0.609, F 0.891)] [G loss: 0.523] [G acc: 0.047]\n",
      "3526 [D loss: 0.518(R 0.546, F0.490)] [D acc: 0.719(R 0.672, F 0.766)] [G loss: 0.518] [G acc: 0.125]\n",
      "3527 [D loss: 0.450(R 0.491, F0.408)] [D acc: 0.828(R 0.766, F 0.891)] [G loss: 0.450] [G acc: 0.094]\n",
      "3528 [D loss: 0.555(R 0.575, F0.535)] [D acc: 0.688(R 0.641, F 0.734)] [G loss: 0.555] [G acc: 0.094]\n",
      "3529 [D loss: 0.516(R 0.604, F0.428)] [D acc: 0.742(R 0.641, F 0.844)] [G loss: 0.516] [G acc: 0.078]\n",
      "3530 [D loss: 0.545(R 0.592, F0.497)] [D acc: 0.656(R 0.562, F 0.750)] [G loss: 0.545] [G acc: 0.109]\n",
      "3531 [D loss: 0.496(R 0.548, F0.444)] [D acc: 0.734(R 0.609, F 0.859)] [G loss: 0.496] [G acc: 0.047]\n",
      "3532 [D loss: 0.553(R 0.506, F0.601)] [D acc: 0.727(R 0.703, F 0.750)] [G loss: 0.553] [G acc: 0.078]\n",
      "3533 [D loss: 0.575(R 0.673, F0.478)] [D acc: 0.695(R 0.562, F 0.828)] [G loss: 0.575] [G acc: 0.141]\n",
      "3534 [D loss: 0.446(R 0.442, F0.451)] [D acc: 0.789(R 0.734, F 0.844)] [G loss: 0.446] [G acc: 0.078]\n",
      "3535 [D loss: 0.515(R 0.604, F0.426)] [D acc: 0.773(R 0.719, F 0.828)] [G loss: 0.515] [G acc: 0.109]\n",
      "3536 [D loss: 0.529(R 0.506, F0.552)] [D acc: 0.695(R 0.688, F 0.703)] [G loss: 0.529] [G acc: 0.078]\n",
      "3537 [D loss: 0.543(R 0.546, F0.541)] [D acc: 0.711(R 0.688, F 0.734)] [G loss: 0.543] [G acc: 0.094]\n",
      "3538 [D loss: 0.469(R 0.499, F0.440)] [D acc: 0.789(R 0.719, F 0.859)] [G loss: 0.469] [G acc: 0.125]\n",
      "3539 [D loss: 0.628(R 0.632, F0.623)] [D acc: 0.680(R 0.641, F 0.719)] [G loss: 0.628] [G acc: 0.062]\n",
      "3540 [D loss: 0.523(R 0.594, F0.452)] [D acc: 0.742(R 0.641, F 0.844)] [G loss: 0.523] [G acc: 0.109]\n",
      "3541 [D loss: 0.541(R 0.583, F0.500)] [D acc: 0.734(R 0.656, F 0.812)] [G loss: 0.541] [G acc: 0.125]\n",
      "3542 [D loss: 0.521(R 0.561, F0.481)] [D acc: 0.711(R 0.641, F 0.781)] [G loss: 0.521] [G acc: 0.156]\n",
      "3543 [D loss: 0.504(R 0.447, F0.561)] [D acc: 0.766(R 0.703, F 0.828)] [G loss: 0.504] [G acc: 0.047]\n",
      "3544 [D loss: 0.481(R 0.529, F0.434)] [D acc: 0.797(R 0.703, F 0.891)] [G loss: 0.481] [G acc: 0.141]\n",
      "3545 [D loss: 0.566(R 0.463, F0.670)] [D acc: 0.695(R 0.719, F 0.672)] [G loss: 0.566] [G acc: 0.094]\n",
      "3546 [D loss: 0.476(R 0.591, F0.361)] [D acc: 0.766(R 0.625, F 0.906)] [G loss: 0.476] [G acc: 0.016]\n",
      "3547 [D loss: 0.547(R 0.639, F0.455)] [D acc: 0.719(R 0.609, F 0.828)] [G loss: 0.547] [G acc: 0.094]\n",
      "3548 [D loss: 0.518(R 0.527, F0.508)] [D acc: 0.750(R 0.688, F 0.812)] [G loss: 0.518] [G acc: 0.047]\n",
      "3549 [D loss: 0.461(R 0.453, F0.470)] [D acc: 0.789(R 0.734, F 0.844)] [G loss: 0.461] [G acc: 0.109]\n",
      "3550 [D loss: 0.500(R 0.513, F0.487)] [D acc: 0.727(R 0.625, F 0.828)] [G loss: 0.500] [G acc: 0.078]\n",
      "3551 [D loss: 0.549(R 0.607, F0.491)] [D acc: 0.703(R 0.609, F 0.797)] [G loss: 0.549] [G acc: 0.047]\n",
      "3552 [D loss: 0.593(R 0.589, F0.597)] [D acc: 0.695(R 0.609, F 0.781)] [G loss: 0.593] [G acc: 0.094]\n",
      "3553 [D loss: 0.560(R 0.658, F0.462)] [D acc: 0.727(R 0.594, F 0.859)] [G loss: 0.560] [G acc: 0.141]\n",
      "3554 [D loss: 0.560(R 0.437, F0.684)] [D acc: 0.781(R 0.812, F 0.750)] [G loss: 0.560] [G acc: 0.047]\n",
      "3555 [D loss: 0.429(R 0.479, F0.379)] [D acc: 0.828(R 0.766, F 0.891)] [G loss: 0.429] [G acc: 0.078]\n",
      "3556 [D loss: 0.533(R 0.481, F0.585)] [D acc: 0.711(R 0.734, F 0.688)] [G loss: 0.533] [G acc: 0.062]\n",
      "3557 [D loss: 0.518(R 0.565, F0.471)] [D acc: 0.727(R 0.641, F 0.812)] [G loss: 0.518] [G acc: 0.078]\n",
      "3558 [D loss: 0.553(R 0.620, F0.486)] [D acc: 0.742(R 0.656, F 0.828)] [G loss: 0.553] [G acc: 0.062]\n",
      "3559 [D loss: 0.629(R 0.745, F0.513)] [D acc: 0.672(R 0.547, F 0.797)] [G loss: 0.629] [G acc: 0.062]\n",
      "3560 [D loss: 0.642(R 0.655, F0.628)] [D acc: 0.703(R 0.625, F 0.781)] [G loss: 0.642] [G acc: 0.094]\n",
      "3561 [D loss: 0.597(R 0.528, F0.666)] [D acc: 0.734(R 0.703, F 0.766)] [G loss: 0.597] [G acc: 0.109]\n",
      "3562 [D loss: 0.473(R 0.476, F0.470)] [D acc: 0.789(R 0.750, F 0.828)] [G loss: 0.473] [G acc: 0.156]\n",
      "3563 [D loss: 0.491(R 0.470, F0.511)] [D acc: 0.750(R 0.734, F 0.766)] [G loss: 0.491] [G acc: 0.109]\n",
      "3564 [D loss: 0.531(R 0.453, F0.609)] [D acc: 0.734(R 0.703, F 0.766)] [G loss: 0.531] [G acc: 0.141]\n",
      "3565 [D loss: 0.594(R 0.618, F0.570)] [D acc: 0.656(R 0.578, F 0.734)] [G loss: 0.594] [G acc: 0.078]\n",
      "3566 [D loss: 0.528(R 0.591, F0.465)] [D acc: 0.734(R 0.609, F 0.859)] [G loss: 0.528] [G acc: 0.172]\n",
      "3567 [D loss: 0.531(R 0.485, F0.578)] [D acc: 0.742(R 0.719, F 0.766)] [G loss: 0.531] [G acc: 0.062]\n",
      "3568 [D loss: 0.611(R 0.723, F0.500)] [D acc: 0.672(R 0.516, F 0.828)] [G loss: 0.611] [G acc: 0.094]\n",
      "3569 [D loss: 0.485(R 0.501, F0.470)] [D acc: 0.758(R 0.703, F 0.812)] [G loss: 0.485] [G acc: 0.078]\n",
      "3570 [D loss: 0.532(R 0.588, F0.476)] [D acc: 0.742(R 0.641, F 0.844)] [G loss: 0.532] [G acc: 0.094]\n",
      "3571 [D loss: 0.541(R 0.512, F0.570)] [D acc: 0.742(R 0.750, F 0.734)] [G loss: 0.541] [G acc: 0.062]\n",
      "3572 [D loss: 0.500(R 0.519, F0.481)] [D acc: 0.742(R 0.703, F 0.781)] [G loss: 0.500] [G acc: 0.062]\n",
      "3573 [D loss: 0.538(R 0.553, F0.523)] [D acc: 0.766(R 0.688, F 0.844)] [G loss: 0.538] [G acc: 0.109]\n",
      "3574 [D loss: 0.626(R 0.624, F0.629)] [D acc: 0.680(R 0.641, F 0.719)] [G loss: 0.626] [G acc: 0.047]\n",
      "3575 [D loss: 0.567(R 0.624, F0.510)] [D acc: 0.750(R 0.641, F 0.859)] [G loss: 0.567] [G acc: 0.047]\n",
      "3576 [D loss: 0.538(R 0.565, F0.511)] [D acc: 0.695(R 0.609, F 0.781)] [G loss: 0.538] [G acc: 0.062]\n",
      "3577 [D loss: 0.446(R 0.472, F0.420)] [D acc: 0.812(R 0.766, F 0.859)] [G loss: 0.446] [G acc: 0.156]\n",
      "3578 [D loss: 0.542(R 0.625, F0.460)] [D acc: 0.703(R 0.562, F 0.844)] [G loss: 0.542] [G acc: 0.062]\n",
      "3579 [D loss: 0.576(R 0.504, F0.648)] [D acc: 0.688(R 0.625, F 0.750)] [G loss: 0.576] [G acc: 0.094]\n",
      "3580 [D loss: 0.581(R 0.604, F0.558)] [D acc: 0.680(R 0.625, F 0.734)] [G loss: 0.581] [G acc: 0.062]\n",
      "3581 [D loss: 0.520(R 0.608, F0.433)] [D acc: 0.703(R 0.547, F 0.859)] [G loss: 0.520] [G acc: 0.078]\n",
      "3582 [D loss: 0.602(R 0.567, F0.636)] [D acc: 0.734(R 0.672, F 0.797)] [G loss: 0.602] [G acc: 0.125]\n",
      "3583 [D loss: 0.561(R 0.536, F0.586)] [D acc: 0.688(R 0.641, F 0.734)] [G loss: 0.561] [G acc: 0.062]\n",
      "3584 [D loss: 0.518(R 0.571, F0.465)] [D acc: 0.719(R 0.641, F 0.797)] [G loss: 0.518] [G acc: 0.109]\n",
      "3585 [D loss: 0.543(R 0.580, F0.505)] [D acc: 0.727(R 0.672, F 0.781)] [G loss: 0.543] [G acc: 0.125]\n",
      "3586 [D loss: 0.611(R 0.678, F0.544)] [D acc: 0.648(R 0.547, F 0.750)] [G loss: 0.611] [G acc: 0.141]\n",
      "3587 [D loss: 0.606(R 0.593, F0.620)] [D acc: 0.672(R 0.594, F 0.750)] [G loss: 0.606] [G acc: 0.047]\n",
      "3588 [D loss: 0.554(R 0.606, F0.502)] [D acc: 0.711(R 0.594, F 0.828)] [G loss: 0.554] [G acc: 0.062]\n",
      "3589 [D loss: 0.566(R 0.591, F0.540)] [D acc: 0.734(R 0.656, F 0.812)] [G loss: 0.566] [G acc: 0.188]\n",
      "3590 [D loss: 0.497(R 0.482, F0.512)] [D acc: 0.758(R 0.688, F 0.828)] [G loss: 0.497] [G acc: 0.125]\n",
      "3591 [D loss: 0.584(R 0.497, F0.671)] [D acc: 0.734(R 0.734, F 0.734)] [G loss: 0.584] [G acc: 0.031]\n",
      "3592 [D loss: 0.562(R 0.713, F0.411)] [D acc: 0.734(R 0.531, F 0.938)] [G loss: 0.562] [G acc: 0.031]\n",
      "3593 [D loss: 0.644(R 0.724, F0.564)] [D acc: 0.602(R 0.406, F 0.797)] [G loss: 0.644] [G acc: 0.094]\n",
      "3594 [D loss: 0.572(R 0.617, F0.528)] [D acc: 0.680(R 0.578, F 0.781)] [G loss: 0.572] [G acc: 0.172]\n",
      "3595 [D loss: 0.525(R 0.544, F0.507)] [D acc: 0.727(R 0.656, F 0.797)] [G loss: 0.525] [G acc: 0.156]\n",
      "3596 [D loss: 0.510(R 0.507, F0.513)] [D acc: 0.758(R 0.688, F 0.828)] [G loss: 0.510] [G acc: 0.031]\n",
      "3597 [D loss: 0.582(R 0.552, F0.612)] [D acc: 0.727(R 0.641, F 0.812)] [G loss: 0.582] [G acc: 0.062]\n",
      "3598 [D loss: 0.458(R 0.405, F0.510)] [D acc: 0.758(R 0.750, F 0.766)] [G loss: 0.458] [G acc: 0.094]\n",
      "3599 [D loss: 0.532(R 0.618, F0.446)] [D acc: 0.734(R 0.562, F 0.906)] [G loss: 0.532] [G acc: 0.172]\n",
      "INFO:tensorflow:Assets written to: ram://25fd141b-fcd2-495e-85ae-c57c82927ac8/assets\n",
      "INFO:tensorflow:Assets written to: ram://5c6d8bd1-fb6e-448a-a7a5-032b7dedb267/assets\n",
      "INFO:tensorflow:Assets written to: ram://946865b7-4728-42b7-acba-eb973a7b4f1c/assets\n",
      "3600 [D loss: 0.556(R 0.516, F0.596)] [D acc: 0.680(R 0.719, F 0.641)] [G loss: 0.556] [G acc: 0.156]\n",
      "3601 [D loss: 0.537(R 0.603, F0.471)] [D acc: 0.727(R 0.594, F 0.859)] [G loss: 0.537] [G acc: 0.125]\n",
      "3602 [D loss: 0.565(R 0.594, F0.536)] [D acc: 0.688(R 0.594, F 0.781)] [G loss: 0.565] [G acc: 0.172]\n",
      "3603 [D loss: 0.507(R 0.523, F0.491)] [D acc: 0.750(R 0.688, F 0.812)] [G loss: 0.507] [G acc: 0.109]\n",
      "3604 [D loss: 0.590(R 0.501, F0.679)] [D acc: 0.727(R 0.688, F 0.766)] [G loss: 0.590] [G acc: 0.078]\n",
      "3605 [D loss: 0.574(R 0.636, F0.512)] [D acc: 0.703(R 0.594, F 0.812)] [G loss: 0.574] [G acc: 0.031]\n",
      "3606 [D loss: 0.478(R 0.483, F0.473)] [D acc: 0.773(R 0.703, F 0.844)] [G loss: 0.478] [G acc: 0.094]\n",
      "3607 [D loss: 0.489(R 0.471, F0.506)] [D acc: 0.758(R 0.750, F 0.766)] [G loss: 0.489] [G acc: 0.047]\n",
      "3608 [D loss: 0.540(R 0.573, F0.507)] [D acc: 0.742(R 0.703, F 0.781)] [G loss: 0.540] [G acc: 0.031]\n",
      "3609 [D loss: 0.490(R 0.515, F0.465)] [D acc: 0.750(R 0.672, F 0.828)] [G loss: 0.490] [G acc: 0.141]\n",
      "3610 [D loss: 0.527(R 0.526, F0.527)] [D acc: 0.766(R 0.703, F 0.828)] [G loss: 0.527] [G acc: 0.062]\n",
      "3611 [D loss: 0.545(R 0.616, F0.473)] [D acc: 0.773(R 0.688, F 0.859)] [G loss: 0.545] [G acc: 0.078]\n",
      "3612 [D loss: 0.611(R 0.589, F0.632)] [D acc: 0.695(R 0.641, F 0.750)] [G loss: 0.611] [G acc: 0.125]\n",
      "3613 [D loss: 0.570(R 0.491, F0.649)] [D acc: 0.703(R 0.719, F 0.688)] [G loss: 0.570] [G acc: 0.078]\n",
      "3614 [D loss: 0.610(R 0.676, F0.544)] [D acc: 0.664(R 0.562, F 0.766)] [G loss: 0.610] [G acc: 0.078]\n",
      "3615 [D loss: 0.607(R 0.626, F0.587)] [D acc: 0.711(R 0.609, F 0.812)] [G loss: 0.607] [G acc: 0.094]\n",
      "3616 [D loss: 0.479(R 0.539, F0.418)] [D acc: 0.797(R 0.734, F 0.859)] [G loss: 0.479] [G acc: 0.125]\n",
      "3617 [D loss: 0.564(R 0.647, F0.482)] [D acc: 0.719(R 0.609, F 0.828)] [G loss: 0.564] [G acc: 0.078]\n",
      "3618 [D loss: 0.480(R 0.513, F0.447)] [D acc: 0.758(R 0.688, F 0.828)] [G loss: 0.480] [G acc: 0.094]\n",
      "3619 [D loss: 0.633(R 0.547, F0.720)] [D acc: 0.656(R 0.656, F 0.656)] [G loss: 0.633] [G acc: 0.062]\n",
      "3620 [D loss: 0.592(R 0.642, F0.542)] [D acc: 0.672(R 0.578, F 0.766)] [G loss: 0.592] [G acc: 0.078]\n",
      "3621 [D loss: 0.523(R 0.578, F0.469)] [D acc: 0.781(R 0.672, F 0.891)] [G loss: 0.523] [G acc: 0.062]\n",
      "3622 [D loss: 0.520(R 0.527, F0.512)] [D acc: 0.773(R 0.719, F 0.828)] [G loss: 0.520] [G acc: 0.047]\n",
      "3623 [D loss: 0.603(R 0.580, F0.627)] [D acc: 0.680(R 0.609, F 0.750)] [G loss: 0.603] [G acc: 0.078]\n",
      "3624 [D loss: 0.555(R 0.564, F0.547)] [D acc: 0.727(R 0.703, F 0.750)] [G loss: 0.555] [G acc: 0.062]\n",
      "3625 [D loss: 0.560(R 0.545, F0.576)] [D acc: 0.734(R 0.719, F 0.750)] [G loss: 0.560] [G acc: 0.109]\n",
      "3626 [D loss: 0.501(R 0.519, F0.483)] [D acc: 0.766(R 0.672, F 0.859)] [G loss: 0.501] [G acc: 0.094]\n",
      "3627 [D loss: 0.479(R 0.511, F0.447)] [D acc: 0.766(R 0.734, F 0.797)] [G loss: 0.479] [G acc: 0.141]\n",
      "3628 [D loss: 0.444(R 0.405, F0.483)] [D acc: 0.805(R 0.812, F 0.797)] [G loss: 0.444] [G acc: 0.047]\n",
      "3629 [D loss: 0.578(R 0.569, F0.586)] [D acc: 0.719(R 0.672, F 0.766)] [G loss: 0.578] [G acc: 0.109]\n",
      "3630 [D loss: 0.434(R 0.428, F0.441)] [D acc: 0.789(R 0.734, F 0.844)] [G loss: 0.434] [G acc: 0.109]\n",
      "3631 [D loss: 0.513(R 0.437, F0.589)] [D acc: 0.711(R 0.719, F 0.703)] [G loss: 0.513] [G acc: 0.141]\n",
      "3632 [D loss: 0.546(R 0.608, F0.485)] [D acc: 0.766(R 0.641, F 0.891)] [G loss: 0.546] [G acc: 0.094]\n",
      "3633 [D loss: 0.590(R 0.555, F0.625)] [D acc: 0.719(R 0.688, F 0.750)] [G loss: 0.590] [G acc: 0.188]\n",
      "3634 [D loss: 0.534(R 0.528, F0.540)] [D acc: 0.734(R 0.703, F 0.766)] [G loss: 0.534] [G acc: 0.109]\n",
      "3635 [D loss: 0.478(R 0.510, F0.445)] [D acc: 0.766(R 0.719, F 0.812)] [G loss: 0.478] [G acc: 0.188]\n",
      "3636 [D loss: 0.620(R 0.491, F0.749)] [D acc: 0.688(R 0.688, F 0.688)] [G loss: 0.620] [G acc: 0.125]\n",
      "3637 [D loss: 0.537(R 0.621, F0.453)] [D acc: 0.750(R 0.656, F 0.844)] [G loss: 0.537] [G acc: 0.125]\n",
      "3638 [D loss: 0.453(R 0.437, F0.470)] [D acc: 0.797(R 0.750, F 0.844)] [G loss: 0.453] [G acc: 0.062]\n",
      "3639 [D loss: 0.551(R 0.551, F0.550)] [D acc: 0.703(R 0.625, F 0.781)] [G loss: 0.551] [G acc: 0.125]\n",
      "3640 [D loss: 0.457(R 0.438, F0.476)] [D acc: 0.844(R 0.781, F 0.906)] [G loss: 0.457] [G acc: 0.172]\n",
      "3641 [D loss: 0.558(R 0.463, F0.652)] [D acc: 0.672(R 0.734, F 0.609)] [G loss: 0.558] [G acc: 0.078]\n",
      "3642 [D loss: 0.524(R 0.612, F0.437)] [D acc: 0.766(R 0.641, F 0.891)] [G loss: 0.524] [G acc: 0.109]\n",
      "3643 [D loss: 0.407(R 0.378, F0.436)] [D acc: 0.805(R 0.781, F 0.828)] [G loss: 0.407] [G acc: 0.125]\n",
      "3644 [D loss: 0.499(R 0.341, F0.658)] [D acc: 0.781(R 0.812, F 0.750)] [G loss: 0.499] [G acc: 0.125]\n",
      "3645 [D loss: 0.553(R 0.588, F0.519)] [D acc: 0.711(R 0.641, F 0.781)] [G loss: 0.553] [G acc: 0.094]\n",
      "3646 [D loss: 0.581(R 0.628, F0.534)] [D acc: 0.688(R 0.578, F 0.797)] [G loss: 0.581] [G acc: 0.000]\n",
      "3647 [D loss: 0.547(R 0.587, F0.506)] [D acc: 0.688(R 0.609, F 0.766)] [G loss: 0.547] [G acc: 0.094]\n",
      "3648 [D loss: 0.577(R 0.605, F0.549)] [D acc: 0.719(R 0.656, F 0.781)] [G loss: 0.577] [G acc: 0.125]\n",
      "3649 [D loss: 0.542(R 0.596, F0.489)] [D acc: 0.711(R 0.641, F 0.781)] [G loss: 0.542] [G acc: 0.094]\n",
      "3650 [D loss: 0.423(R 0.444, F0.402)] [D acc: 0.836(R 0.750, F 0.922)] [G loss: 0.423] [G acc: 0.156]\n",
      "3651 [D loss: 0.627(R 0.575, F0.680)] [D acc: 0.688(R 0.609, F 0.766)] [G loss: 0.627] [G acc: 0.078]\n",
      "3652 [D loss: 0.574(R 0.622, F0.527)] [D acc: 0.711(R 0.609, F 0.812)] [G loss: 0.574] [G acc: 0.062]\n",
      "3653 [D loss: 0.565(R 0.632, F0.498)] [D acc: 0.703(R 0.625, F 0.781)] [G loss: 0.565] [G acc: 0.172]\n",
      "3654 [D loss: 0.498(R 0.440, F0.556)] [D acc: 0.750(R 0.750, F 0.750)] [G loss: 0.498] [G acc: 0.047]\n",
      "3655 [D loss: 0.493(R 0.499, F0.487)] [D acc: 0.781(R 0.734, F 0.828)] [G loss: 0.493] [G acc: 0.094]\n",
      "3656 [D loss: 0.530(R 0.543, F0.517)] [D acc: 0.719(R 0.656, F 0.781)] [G loss: 0.530] [G acc: 0.078]\n",
      "3657 [D loss: 0.599(R 0.628, F0.570)] [D acc: 0.727(R 0.625, F 0.828)] [G loss: 0.599] [G acc: 0.078]\n",
      "3658 [D loss: 0.553(R 0.582, F0.524)] [D acc: 0.734(R 0.688, F 0.781)] [G loss: 0.553] [G acc: 0.031]\n",
      "3659 [D loss: 0.498(R 0.515, F0.482)] [D acc: 0.773(R 0.703, F 0.844)] [G loss: 0.498] [G acc: 0.078]\n",
      "3660 [D loss: 0.532(R 0.612, F0.453)] [D acc: 0.750(R 0.641, F 0.859)] [G loss: 0.532] [G acc: 0.078]\n",
      "3661 [D loss: 0.598(R 0.563, F0.633)] [D acc: 0.727(R 0.703, F 0.750)] [G loss: 0.598] [G acc: 0.141]\n",
      "3662 [D loss: 0.461(R 0.472, F0.451)] [D acc: 0.789(R 0.719, F 0.859)] [G loss: 0.461] [G acc: 0.109]\n",
      "3663 [D loss: 0.494(R 0.498, F0.491)] [D acc: 0.797(R 0.703, F 0.891)] [G loss: 0.494] [G acc: 0.109]\n",
      "3664 [D loss: 0.419(R 0.450, F0.389)] [D acc: 0.805(R 0.719, F 0.891)] [G loss: 0.419] [G acc: 0.141]\n",
      "3665 [D loss: 0.557(R 0.498, F0.616)] [D acc: 0.711(R 0.703, F 0.719)] [G loss: 0.557] [G acc: 0.125]\n",
      "3666 [D loss: 0.485(R 0.518, F0.452)] [D acc: 0.750(R 0.656, F 0.844)] [G loss: 0.485] [G acc: 0.109]\n",
      "3667 [D loss: 0.519(R 0.447, F0.592)] [D acc: 0.742(R 0.719, F 0.766)] [G loss: 0.519] [G acc: 0.141]\n",
      "3668 [D loss: 0.519(R 0.589, F0.450)] [D acc: 0.719(R 0.672, F 0.766)] [G loss: 0.519] [G acc: 0.031]\n",
      "3669 [D loss: 0.505(R 0.564, F0.446)] [D acc: 0.742(R 0.672, F 0.812)] [G loss: 0.505] [G acc: 0.188]\n",
      "3670 [D loss: 0.579(R 0.462, F0.696)] [D acc: 0.734(R 0.703, F 0.766)] [G loss: 0.579] [G acc: 0.062]\n",
      "3671 [D loss: 0.519(R 0.558, F0.480)] [D acc: 0.734(R 0.656, F 0.812)] [G loss: 0.519] [G acc: 0.078]\n",
      "3672 [D loss: 0.477(R 0.516, F0.438)] [D acc: 0.789(R 0.703, F 0.875)] [G loss: 0.477] [G acc: 0.031]\n",
      "3673 [D loss: 0.441(R 0.480, F0.402)] [D acc: 0.773(R 0.688, F 0.859)] [G loss: 0.441] [G acc: 0.062]\n",
      "3674 [D loss: 0.502(R 0.564, F0.441)] [D acc: 0.711(R 0.656, F 0.766)] [G loss: 0.502] [G acc: 0.141]\n",
      "3675 [D loss: 0.571(R 0.539, F0.602)] [D acc: 0.789(R 0.734, F 0.844)] [G loss: 0.571] [G acc: 0.094]\n",
      "3676 [D loss: 0.598(R 0.612, F0.584)] [D acc: 0.688(R 0.594, F 0.781)] [G loss: 0.598] [G acc: 0.172]\n",
      "3677 [D loss: 0.557(R 0.554, F0.561)] [D acc: 0.703(R 0.641, F 0.766)] [G loss: 0.557] [G acc: 0.141]\n",
      "3678 [D loss: 0.489(R 0.422, F0.556)] [D acc: 0.797(R 0.781, F 0.812)] [G loss: 0.489] [G acc: 0.062]\n",
      "3679 [D loss: 0.545(R 0.555, F0.535)] [D acc: 0.742(R 0.688, F 0.797)] [G loss: 0.545] [G acc: 0.062]\n",
      "3680 [D loss: 0.511(R 0.443, F0.579)] [D acc: 0.719(R 0.719, F 0.719)] [G loss: 0.511] [G acc: 0.094]\n",
      "3681 [D loss: 0.464(R 0.552, F0.376)] [D acc: 0.789(R 0.656, F 0.922)] [G loss: 0.464] [G acc: 0.078]\n",
      "3682 [D loss: 0.492(R 0.367, F0.616)] [D acc: 0.781(R 0.828, F 0.734)] [G loss: 0.492] [G acc: 0.062]\n",
      "3683 [D loss: 0.472(R 0.448, F0.495)] [D acc: 0.734(R 0.688, F 0.781)] [G loss: 0.472] [G acc: 0.078]\n",
      "3684 [D loss: 0.430(R 0.487, F0.372)] [D acc: 0.773(R 0.688, F 0.859)] [G loss: 0.430] [G acc: 0.062]\n",
      "3685 [D loss: 0.599(R 0.640, F0.559)] [D acc: 0.695(R 0.562, F 0.828)] [G loss: 0.599] [G acc: 0.047]\n",
      "3686 [D loss: 0.523(R 0.524, F0.521)] [D acc: 0.766(R 0.750, F 0.781)] [G loss: 0.523] [G acc: 0.094]\n",
      "3687 [D loss: 0.619(R 0.772, F0.466)] [D acc: 0.664(R 0.438, F 0.891)] [G loss: 0.619] [G acc: 0.094]\n",
      "3688 [D loss: 0.491(R 0.529, F0.453)] [D acc: 0.766(R 0.656, F 0.875)] [G loss: 0.491] [G acc: 0.141]\n",
      "3689 [D loss: 0.475(R 0.461, F0.488)] [D acc: 0.758(R 0.688, F 0.828)] [G loss: 0.475] [G acc: 0.219]\n",
      "3690 [D loss: 0.532(R 0.512, F0.552)] [D acc: 0.695(R 0.641, F 0.750)] [G loss: 0.532] [G acc: 0.078]\n",
      "3691 [D loss: 0.533(R 0.509, F0.557)] [D acc: 0.734(R 0.703, F 0.766)] [G loss: 0.533] [G acc: 0.094]\n",
      "3692 [D loss: 0.430(R 0.374, F0.486)] [D acc: 0.805(R 0.781, F 0.828)] [G loss: 0.430] [G acc: 0.062]\n",
      "3693 [D loss: 0.473(R 0.537, F0.408)] [D acc: 0.805(R 0.703, F 0.906)] [G loss: 0.473] [G acc: 0.109]\n",
      "3694 [D loss: 0.527(R 0.571, F0.483)] [D acc: 0.742(R 0.641, F 0.844)] [G loss: 0.527] [G acc: 0.156]\n",
      "3695 [D loss: 0.508(R 0.587, F0.429)] [D acc: 0.742(R 0.625, F 0.859)] [G loss: 0.508] [G acc: 0.078]\n",
      "3696 [D loss: 0.425(R 0.406, F0.444)] [D acc: 0.820(R 0.781, F 0.859)] [G loss: 0.425] [G acc: 0.094]\n",
      "3697 [D loss: 0.543(R 0.622, F0.465)] [D acc: 0.711(R 0.625, F 0.797)] [G loss: 0.543] [G acc: 0.062]\n",
      "3698 [D loss: 0.524(R 0.549, F0.499)] [D acc: 0.711(R 0.625, F 0.797)] [G loss: 0.524] [G acc: 0.062]\n",
      "3699 [D loss: 0.493(R 0.454, F0.533)] [D acc: 0.773(R 0.750, F 0.797)] [G loss: 0.493] [G acc: 0.078]\n",
      "INFO:tensorflow:Assets written to: ram://5d96c08a-75a2-4161-85cd-8c53e0629b35/assets\n",
      "INFO:tensorflow:Assets written to: ram://a4dbc94a-2fd4-4ec4-92a4-4dae762a2c20/assets\n",
      "INFO:tensorflow:Assets written to: ram://47bb104d-323f-474d-82d8-acddaff71bf6/assets\n",
      "3700 [D loss: 0.497(R 0.524, F0.470)] [D acc: 0.734(R 0.625, F 0.844)] [G loss: 0.497] [G acc: 0.094]\n",
      "3701 [D loss: 0.559(R 0.610, F0.509)] [D acc: 0.672(R 0.578, F 0.766)] [G loss: 0.559] [G acc: 0.125]\n",
      "3702 [D loss: 0.540(R 0.483, F0.596)] [D acc: 0.766(R 0.750, F 0.781)] [G loss: 0.540] [G acc: 0.062]\n",
      "3703 [D loss: 0.571(R 0.679, F0.463)] [D acc: 0.719(R 0.625, F 0.812)] [G loss: 0.571] [G acc: 0.109]\n",
      "3704 [D loss: 0.498(R 0.523, F0.472)] [D acc: 0.797(R 0.719, F 0.875)] [G loss: 0.498] [G acc: 0.109]\n",
      "3705 [D loss: 0.564(R 0.519, F0.609)] [D acc: 0.773(R 0.703, F 0.844)] [G loss: 0.564] [G acc: 0.078]\n",
      "3706 [D loss: 0.449(R 0.445, F0.454)] [D acc: 0.750(R 0.719, F 0.781)] [G loss: 0.449] [G acc: 0.047]\n",
      "3707 [D loss: 0.445(R 0.478, F0.413)] [D acc: 0.797(R 0.703, F 0.891)] [G loss: 0.445] [G acc: 0.031]\n",
      "3708 [D loss: 0.647(R 0.583, F0.712)] [D acc: 0.672(R 0.641, F 0.703)] [G loss: 0.647] [G acc: 0.047]\n",
      "3709 [D loss: 0.501(R 0.548, F0.453)] [D acc: 0.727(R 0.656, F 0.797)] [G loss: 0.501] [G acc: 0.047]\n",
      "3710 [D loss: 0.488(R 0.497, F0.478)] [D acc: 0.758(R 0.656, F 0.859)] [G loss: 0.488] [G acc: 0.156]\n",
      "3711 [D loss: 0.583(R 0.648, F0.517)] [D acc: 0.703(R 0.562, F 0.844)] [G loss: 0.583] [G acc: 0.047]\n",
      "3712 [D loss: 0.533(R 0.538, F0.527)] [D acc: 0.742(R 0.672, F 0.812)] [G loss: 0.533] [G acc: 0.062]\n",
      "3713 [D loss: 0.593(R 0.629, F0.558)] [D acc: 0.695(R 0.625, F 0.766)] [G loss: 0.593] [G acc: 0.078]\n",
      "3714 [D loss: 0.581(R 0.563, F0.599)] [D acc: 0.711(R 0.656, F 0.766)] [G loss: 0.581] [G acc: 0.078]\n",
      "3715 [D loss: 0.467(R 0.538, F0.395)] [D acc: 0.750(R 0.656, F 0.844)] [G loss: 0.467] [G acc: 0.156]\n",
      "3716 [D loss: 0.469(R 0.560, F0.378)] [D acc: 0.797(R 0.703, F 0.891)] [G loss: 0.469] [G acc: 0.141]\n",
      "3717 [D loss: 0.454(R 0.465, F0.443)] [D acc: 0.758(R 0.719, F 0.797)] [G loss: 0.454] [G acc: 0.094]\n",
      "3718 [D loss: 0.443(R 0.475, F0.411)] [D acc: 0.781(R 0.719, F 0.844)] [G loss: 0.443] [G acc: 0.172]\n",
      "3719 [D loss: 0.538(R 0.540, F0.537)] [D acc: 0.688(R 0.672, F 0.703)] [G loss: 0.538] [G acc: 0.156]\n",
      "3720 [D loss: 0.618(R 0.674, F0.562)] [D acc: 0.688(R 0.578, F 0.797)] [G loss: 0.618] [G acc: 0.125]\n",
      "3721 [D loss: 0.564(R 0.614, F0.514)] [D acc: 0.750(R 0.688, F 0.812)] [G loss: 0.564] [G acc: 0.109]\n",
      "3722 [D loss: 0.484(R 0.491, F0.478)] [D acc: 0.734(R 0.641, F 0.828)] [G loss: 0.484] [G acc: 0.156]\n",
      "3723 [D loss: 0.532(R 0.492, F0.573)] [D acc: 0.734(R 0.719, F 0.750)] [G loss: 0.532] [G acc: 0.125]\n",
      "3724 [D loss: 0.453(R 0.423, F0.483)] [D acc: 0.766(R 0.750, F 0.781)] [G loss: 0.453] [G acc: 0.078]\n",
      "3725 [D loss: 0.471(R 0.434, F0.509)] [D acc: 0.805(R 0.797, F 0.812)] [G loss: 0.471] [G acc: 0.078]\n",
      "3726 [D loss: 0.476(R 0.521, F0.431)] [D acc: 0.781(R 0.703, F 0.859)] [G loss: 0.476] [G acc: 0.109]\n",
      "3727 [D loss: 0.573(R 0.614, F0.533)] [D acc: 0.695(R 0.594, F 0.797)] [G loss: 0.573] [G acc: 0.078]\n",
      "3728 [D loss: 0.580(R 0.653, F0.506)] [D acc: 0.703(R 0.609, F 0.797)] [G loss: 0.580] [G acc: 0.078]\n",
      "3729 [D loss: 0.491(R 0.534, F0.448)] [D acc: 0.773(R 0.672, F 0.875)] [G loss: 0.491] [G acc: 0.125]\n",
      "3730 [D loss: 0.546(R 0.551, F0.540)] [D acc: 0.688(R 0.609, F 0.766)] [G loss: 0.546] [G acc: 0.141]\n",
      "3731 [D loss: 0.447(R 0.507, F0.388)] [D acc: 0.805(R 0.688, F 0.922)] [G loss: 0.447] [G acc: 0.125]\n",
      "3732 [D loss: 0.525(R 0.417, F0.632)] [D acc: 0.766(R 0.812, F 0.719)] [G loss: 0.525] [G acc: 0.188]\n",
      "3733 [D loss: 0.508(R 0.613, F0.403)] [D acc: 0.766(R 0.641, F 0.891)] [G loss: 0.508] [G acc: 0.094]\n",
      "3734 [D loss: 0.486(R 0.497, F0.474)] [D acc: 0.789(R 0.734, F 0.844)] [G loss: 0.486] [G acc: 0.062]\n",
      "3735 [D loss: 0.474(R 0.450, F0.498)] [D acc: 0.797(R 0.750, F 0.844)] [G loss: 0.474] [G acc: 0.094]\n",
      "3736 [D loss: 0.527(R 0.562, F0.493)] [D acc: 0.711(R 0.625, F 0.797)] [G loss: 0.527] [G acc: 0.062]\n",
      "3737 [D loss: 0.519(R 0.514, F0.524)] [D acc: 0.703(R 0.656, F 0.750)] [G loss: 0.519] [G acc: 0.109]\n",
      "3738 [D loss: 0.518(R 0.541, F0.496)] [D acc: 0.734(R 0.625, F 0.844)] [G loss: 0.518] [G acc: 0.203]\n",
      "3739 [D loss: 0.413(R 0.474, F0.351)] [D acc: 0.805(R 0.719, F 0.891)] [G loss: 0.413] [G acc: 0.250]\n",
      "3740 [D loss: 0.771(R 0.491, F1.050)] [D acc: 0.719(R 0.750, F 0.688)] [G loss: 0.771] [G acc: 0.062]\n",
      "3741 [D loss: 0.570(R 0.733, F0.407)] [D acc: 0.719(R 0.578, F 0.859)] [G loss: 0.570] [G acc: 0.109]\n",
      "3742 [D loss: 0.579(R 0.593, F0.565)] [D acc: 0.719(R 0.641, F 0.797)] [G loss: 0.579] [G acc: 0.062]\n",
      "3743 [D loss: 0.509(R 0.598, F0.421)] [D acc: 0.766(R 0.641, F 0.891)] [G loss: 0.509] [G acc: 0.062]\n",
      "3744 [D loss: 0.501(R 0.514, F0.488)] [D acc: 0.750(R 0.688, F 0.812)] [G loss: 0.501] [G acc: 0.203]\n",
      "3745 [D loss: 0.545(R 0.568, F0.521)] [D acc: 0.703(R 0.641, F 0.766)] [G loss: 0.545] [G acc: 0.078]\n",
      "3746 [D loss: 0.458(R 0.470, F0.446)] [D acc: 0.766(R 0.688, F 0.844)] [G loss: 0.458] [G acc: 0.125]\n",
      "3747 [D loss: 0.502(R 0.503, F0.502)] [D acc: 0.727(R 0.703, F 0.750)] [G loss: 0.502] [G acc: 0.094]\n",
      "3748 [D loss: 0.518(R 0.478, F0.558)] [D acc: 0.742(R 0.703, F 0.781)] [G loss: 0.518] [G acc: 0.094]\n",
      "3749 [D loss: 0.453(R 0.407, F0.499)] [D acc: 0.797(R 0.797, F 0.797)] [G loss: 0.453] [G acc: 0.047]\n",
      "3750 [D loss: 0.576(R 0.598, F0.554)] [D acc: 0.734(R 0.672, F 0.797)] [G loss: 0.576] [G acc: 0.141]\n",
      "3751 [D loss: 0.530(R 0.578, F0.483)] [D acc: 0.742(R 0.672, F 0.812)] [G loss: 0.530] [G acc: 0.031]\n",
      "3752 [D loss: 0.616(R 0.708, F0.524)] [D acc: 0.656(R 0.562, F 0.750)] [G loss: 0.616] [G acc: 0.125]\n",
      "3753 [D loss: 0.435(R 0.488, F0.383)] [D acc: 0.766(R 0.672, F 0.859)] [G loss: 0.435] [G acc: 0.094]\n",
      "3754 [D loss: 0.566(R 0.445, F0.686)] [D acc: 0.703(R 0.703, F 0.703)] [G loss: 0.566] [G acc: 0.031]\n",
      "3755 [D loss: 0.545(R 0.607, F0.483)] [D acc: 0.711(R 0.594, F 0.828)] [G loss: 0.545] [G acc: 0.125]\n",
      "3756 [D loss: 0.571(R 0.545, F0.598)] [D acc: 0.727(R 0.688, F 0.766)] [G loss: 0.571] [G acc: 0.188]\n",
      "3757 [D loss: 0.522(R 0.598, F0.447)] [D acc: 0.750(R 0.625, F 0.875)] [G loss: 0.522] [G acc: 0.031]\n",
      "3758 [D loss: 0.480(R 0.503, F0.457)] [D acc: 0.781(R 0.703, F 0.859)] [G loss: 0.480] [G acc: 0.109]\n",
      "3759 [D loss: 0.473(R 0.478, F0.469)] [D acc: 0.781(R 0.734, F 0.828)] [G loss: 0.473] [G acc: 0.078]\n",
      "3760 [D loss: 0.432(R 0.386, F0.478)] [D acc: 0.789(R 0.781, F 0.797)] [G loss: 0.432] [G acc: 0.141]\n",
      "3761 [D loss: 0.559(R 0.574, F0.544)] [D acc: 0.680(R 0.641, F 0.719)] [G loss: 0.559] [G acc: 0.062]\n",
      "3762 [D loss: 0.640(R 0.640, F0.639)] [D acc: 0.664(R 0.594, F 0.734)] [G loss: 0.640] [G acc: 0.094]\n",
      "3763 [D loss: 0.532(R 0.578, F0.485)] [D acc: 0.727(R 0.609, F 0.844)] [G loss: 0.532] [G acc: 0.047]\n",
      "3764 [D loss: 0.465(R 0.561, F0.369)] [D acc: 0.750(R 0.641, F 0.859)] [G loss: 0.465] [G acc: 0.125]\n",
      "3765 [D loss: 0.515(R 0.629, F0.401)] [D acc: 0.734(R 0.594, F 0.875)] [G loss: 0.515] [G acc: 0.172]\n",
      "3766 [D loss: 0.501(R 0.485, F0.518)] [D acc: 0.758(R 0.734, F 0.781)] [G loss: 0.501] [G acc: 0.125]\n",
      "3767 [D loss: 0.402(R 0.377, F0.428)] [D acc: 0.852(R 0.828, F 0.875)] [G loss: 0.402] [G acc: 0.094]\n",
      "3768 [D loss: 0.549(R 0.491, F0.606)] [D acc: 0.750(R 0.703, F 0.797)] [G loss: 0.549] [G acc: 0.062]\n",
      "3769 [D loss: 0.531(R 0.558, F0.503)] [D acc: 0.727(R 0.656, F 0.797)] [G loss: 0.531] [G acc: 0.109]\n",
      "3770 [D loss: 0.511(R 0.558, F0.464)] [D acc: 0.758(R 0.672, F 0.844)] [G loss: 0.511] [G acc: 0.125]\n",
      "3771 [D loss: 0.521(R 0.573, F0.469)] [D acc: 0.742(R 0.656, F 0.828)] [G loss: 0.521] [G acc: 0.172]\n",
      "3772 [D loss: 0.510(R 0.543, F0.477)] [D acc: 0.742(R 0.641, F 0.844)] [G loss: 0.510] [G acc: 0.125]\n",
      "3773 [D loss: 0.540(R 0.499, F0.581)] [D acc: 0.781(R 0.750, F 0.812)] [G loss: 0.540] [G acc: 0.141]\n",
      "3774 [D loss: 0.587(R 0.494, F0.680)] [D acc: 0.742(R 0.719, F 0.766)] [G loss: 0.587] [G acc: 0.125]\n",
      "3775 [D loss: 0.606(R 0.608, F0.604)] [D acc: 0.719(R 0.656, F 0.781)] [G loss: 0.606] [G acc: 0.141]\n",
      "3776 [D loss: 0.447(R 0.398, F0.497)] [D acc: 0.805(R 0.828, F 0.781)] [G loss: 0.447] [G acc: 0.031]\n",
      "3777 [D loss: 0.483(R 0.567, F0.400)] [D acc: 0.781(R 0.672, F 0.891)] [G loss: 0.483] [G acc: 0.062]\n",
      "3778 [D loss: 0.515(R 0.431, F0.599)] [D acc: 0.773(R 0.812, F 0.734)] [G loss: 0.515] [G acc: 0.031]\n",
      "3779 [D loss: 0.531(R 0.489, F0.572)] [D acc: 0.781(R 0.750, F 0.812)] [G loss: 0.531] [G acc: 0.078]\n",
      "3780 [D loss: 0.590(R 0.700, F0.481)] [D acc: 0.648(R 0.547, F 0.750)] [G loss: 0.590] [G acc: 0.094]\n",
      "3781 [D loss: 0.476(R 0.549, F0.404)] [D acc: 0.766(R 0.688, F 0.844)] [G loss: 0.476] [G acc: 0.172]\n",
      "3782 [D loss: 0.459(R 0.404, F0.513)] [D acc: 0.805(R 0.781, F 0.828)] [G loss: 0.459] [G acc: 0.125]\n",
      "3783 [D loss: 0.510(R 0.545, F0.475)] [D acc: 0.734(R 0.688, F 0.781)] [G loss: 0.510] [G acc: 0.125]\n",
      "3784 [D loss: 0.441(R 0.484, F0.397)] [D acc: 0.812(R 0.734, F 0.891)] [G loss: 0.441] [G acc: 0.094]\n",
      "3785 [D loss: 0.532(R 0.561, F0.502)] [D acc: 0.742(R 0.672, F 0.812)] [G loss: 0.532] [G acc: 0.094]\n",
      "3786 [D loss: 0.421(R 0.383, F0.458)] [D acc: 0.812(R 0.812, F 0.812)] [G loss: 0.421] [G acc: 0.109]\n",
      "3787 [D loss: 0.547(R 0.546, F0.548)] [D acc: 0.758(R 0.688, F 0.828)] [G loss: 0.547] [G acc: 0.125]\n",
      "3788 [D loss: 0.535(R 0.628, F0.442)] [D acc: 0.727(R 0.578, F 0.875)] [G loss: 0.535] [G acc: 0.109]\n",
      "3789 [D loss: 0.579(R 0.553, F0.605)] [D acc: 0.742(R 0.688, F 0.797)] [G loss: 0.579] [G acc: 0.062]\n",
      "3790 [D loss: 0.585(R 0.713, F0.458)] [D acc: 0.742(R 0.625, F 0.859)] [G loss: 0.585] [G acc: 0.062]\n",
      "3791 [D loss: 0.461(R 0.422, F0.500)] [D acc: 0.773(R 0.766, F 0.781)] [G loss: 0.461] [G acc: 0.109]\n",
      "3792 [D loss: 0.467(R 0.484, F0.451)] [D acc: 0.750(R 0.672, F 0.828)] [G loss: 0.467] [G acc: 0.156]\n",
      "3793 [D loss: 0.489(R 0.361, F0.616)] [D acc: 0.789(R 0.781, F 0.797)] [G loss: 0.489] [G acc: 0.094]\n",
      "3794 [D loss: 0.524(R 0.546, F0.502)] [D acc: 0.766(R 0.688, F 0.844)] [G loss: 0.524] [G acc: 0.156]\n",
      "3795 [D loss: 0.536(R 0.619, F0.454)] [D acc: 0.711(R 0.641, F 0.781)] [G loss: 0.536] [G acc: 0.094]\n",
      "3796 [D loss: 0.535(R 0.497, F0.574)] [D acc: 0.750(R 0.719, F 0.781)] [G loss: 0.535] [G acc: 0.047]\n",
      "3797 [D loss: 0.496(R 0.566, F0.426)] [D acc: 0.758(R 0.625, F 0.891)] [G loss: 0.496] [G acc: 0.094]\n",
      "3798 [D loss: 0.460(R 0.487, F0.433)] [D acc: 0.781(R 0.703, F 0.859)] [G loss: 0.460] [G acc: 0.078]\n",
      "3799 [D loss: 0.479(R 0.474, F0.483)] [D acc: 0.805(R 0.781, F 0.828)] [G loss: 0.479] [G acc: 0.125]\n",
      "INFO:tensorflow:Assets written to: ram://f4f0efdc-e7d0-4b4d-b228-8046e929a55f/assets\n",
      "INFO:tensorflow:Assets written to: ram://69739a5a-3d87-42f2-9cbf-d83c118716af/assets\n",
      "INFO:tensorflow:Assets written to: ram://ce7670e3-4cf4-4549-a038-e0ee7cbf6f4a/assets\n",
      "3800 [D loss: 0.563(R 0.617, F0.509)] [D acc: 0.734(R 0.609, F 0.859)] [G loss: 0.563] [G acc: 0.156]\n",
      "3801 [D loss: 0.483(R 0.494, F0.473)] [D acc: 0.773(R 0.688, F 0.859)] [G loss: 0.483] [G acc: 0.078]\n",
      "3802 [D loss: 0.504(R 0.598, F0.409)] [D acc: 0.812(R 0.688, F 0.938)] [G loss: 0.504] [G acc: 0.141]\n",
      "3803 [D loss: 0.516(R 0.541, F0.491)] [D acc: 0.766(R 0.703, F 0.828)] [G loss: 0.516] [G acc: 0.109]\n",
      "3804 [D loss: 0.534(R 0.560, F0.507)] [D acc: 0.680(R 0.562, F 0.797)] [G loss: 0.534] [G acc: 0.125]\n",
      "3805 [D loss: 0.483(R 0.426, F0.540)] [D acc: 0.773(R 0.750, F 0.797)] [G loss: 0.483] [G acc: 0.109]\n",
      "3806 [D loss: 0.532(R 0.612, F0.451)] [D acc: 0.727(R 0.594, F 0.859)] [G loss: 0.532] [G acc: 0.109]\n",
      "3807 [D loss: 0.521(R 0.567, F0.475)] [D acc: 0.719(R 0.688, F 0.750)] [G loss: 0.521] [G acc: 0.156]\n",
      "3808 [D loss: 0.424(R 0.321, F0.527)] [D acc: 0.820(R 0.828, F 0.812)] [G loss: 0.424] [G acc: 0.141]\n",
      "3809 [D loss: 0.566(R 0.619, F0.512)] [D acc: 0.672(R 0.562, F 0.781)] [G loss: 0.566] [G acc: 0.141]\n",
      "3810 [D loss: 0.489(R 0.472, F0.506)] [D acc: 0.742(R 0.703, F 0.781)] [G loss: 0.489] [G acc: 0.031]\n",
      "3811 [D loss: 0.454(R 0.498, F0.410)] [D acc: 0.773(R 0.656, F 0.891)] [G loss: 0.454] [G acc: 0.109]\n",
      "3812 [D loss: 0.511(R 0.547, F0.475)] [D acc: 0.773(R 0.734, F 0.812)] [G loss: 0.511] [G acc: 0.094]\n",
      "3813 [D loss: 0.431(R 0.432, F0.429)] [D acc: 0.781(R 0.750, F 0.812)] [G loss: 0.431] [G acc: 0.062]\n",
      "3814 [D loss: 0.508(R 0.511, F0.505)] [D acc: 0.727(R 0.719, F 0.734)] [G loss: 0.508] [G acc: 0.016]\n",
      "3815 [D loss: 0.433(R 0.466, F0.400)] [D acc: 0.789(R 0.734, F 0.844)] [G loss: 0.433] [G acc: 0.094]\n",
      "3816 [D loss: 0.594(R 0.623, F0.564)] [D acc: 0.672(R 0.594, F 0.750)] [G loss: 0.594] [G acc: 0.172]\n",
      "3817 [D loss: 0.458(R 0.500, F0.416)] [D acc: 0.812(R 0.719, F 0.906)] [G loss: 0.458] [G acc: 0.078]\n",
      "3818 [D loss: 0.671(R 0.689, F0.653)] [D acc: 0.633(R 0.562, F 0.703)] [G loss: 0.671] [G acc: 0.141]\n",
      "3819 [D loss: 0.525(R 0.552, F0.499)] [D acc: 0.758(R 0.703, F 0.812)] [G loss: 0.525] [G acc: 0.062]\n",
      "3820 [D loss: 0.502(R 0.408, F0.596)] [D acc: 0.742(R 0.750, F 0.734)] [G loss: 0.502] [G acc: 0.094]\n",
      "3821 [D loss: 0.454(R 0.506, F0.402)] [D acc: 0.797(R 0.703, F 0.891)] [G loss: 0.454] [G acc: 0.062]\n",
      "3822 [D loss: 0.508(R 0.522, F0.495)] [D acc: 0.773(R 0.688, F 0.859)] [G loss: 0.508] [G acc: 0.141]\n",
      "3823 [D loss: 0.508(R 0.527, F0.490)] [D acc: 0.781(R 0.688, F 0.875)] [G loss: 0.508] [G acc: 0.062]\n",
      "3824 [D loss: 0.473(R 0.465, F0.481)] [D acc: 0.797(R 0.719, F 0.875)] [G loss: 0.473] [G acc: 0.094]\n",
      "3825 [D loss: 0.593(R 0.633, F0.552)] [D acc: 0.711(R 0.625, F 0.797)] [G loss: 0.593] [G acc: 0.016]\n",
      "3826 [D loss: 0.523(R 0.542, F0.503)] [D acc: 0.742(R 0.641, F 0.844)] [G loss: 0.523] [G acc: 0.109]\n",
      "3827 [D loss: 0.458(R 0.492, F0.423)] [D acc: 0.766(R 0.656, F 0.875)] [G loss: 0.458] [G acc: 0.156]\n",
      "3828 [D loss: 0.432(R 0.439, F0.425)] [D acc: 0.805(R 0.750, F 0.859)] [G loss: 0.432] [G acc: 0.141]\n",
      "3829 [D loss: 0.491(R 0.473, F0.508)] [D acc: 0.781(R 0.734, F 0.828)] [G loss: 0.491] [G acc: 0.031]\n",
      "3830 [D loss: 0.483(R 0.489, F0.476)] [D acc: 0.773(R 0.719, F 0.828)] [G loss: 0.483] [G acc: 0.078]\n",
      "3831 [D loss: 0.557(R 0.598, F0.516)] [D acc: 0.703(R 0.594, F 0.812)] [G loss: 0.557] [G acc: 0.125]\n",
      "3832 [D loss: 0.442(R 0.466, F0.418)] [D acc: 0.797(R 0.719, F 0.875)] [G loss: 0.442] [G acc: 0.094]\n",
      "3833 [D loss: 0.540(R 0.531, F0.550)] [D acc: 0.742(R 0.672, F 0.812)] [G loss: 0.540] [G acc: 0.062]\n",
      "3834 [D loss: 0.531(R 0.599, F0.462)] [D acc: 0.711(R 0.609, F 0.812)] [G loss: 0.531] [G acc: 0.109]\n",
      "3835 [D loss: 0.601(R 0.637, F0.566)] [D acc: 0.727(R 0.625, F 0.828)] [G loss: 0.601] [G acc: 0.094]\n",
      "3836 [D loss: 0.548(R 0.549, F0.548)] [D acc: 0.758(R 0.672, F 0.844)] [G loss: 0.548] [G acc: 0.094]\n",
      "3837 [D loss: 0.546(R 0.529, F0.563)] [D acc: 0.719(R 0.625, F 0.812)] [G loss: 0.546] [G acc: 0.156]\n",
      "3838 [D loss: 0.425(R 0.456, F0.393)] [D acc: 0.805(R 0.734, F 0.875)] [G loss: 0.425] [G acc: 0.188]\n",
      "3839 [D loss: 0.452(R 0.480, F0.425)] [D acc: 0.805(R 0.719, F 0.891)] [G loss: 0.452] [G acc: 0.172]\n",
      "3840 [D loss: 0.578(R 0.354, F0.801)] [D acc: 0.773(R 0.812, F 0.734)] [G loss: 0.578] [G acc: 0.094]\n",
      "3841 [D loss: 0.472(R 0.586, F0.358)] [D acc: 0.781(R 0.656, F 0.906)] [G loss: 0.472] [G acc: 0.062]\n",
      "3842 [D loss: 0.453(R 0.532, F0.374)] [D acc: 0.773(R 0.672, F 0.875)] [G loss: 0.453] [G acc: 0.031]\n",
      "3843 [D loss: 0.541(R 0.553, F0.529)] [D acc: 0.719(R 0.688, F 0.750)] [G loss: 0.541] [G acc: 0.062]\n",
      "3844 [D loss: 0.598(R 0.536, F0.661)] [D acc: 0.695(R 0.656, F 0.734)] [G loss: 0.598] [G acc: 0.125]\n",
      "3845 [D loss: 0.496(R 0.503, F0.490)] [D acc: 0.766(R 0.750, F 0.781)] [G loss: 0.496] [G acc: 0.094]\n",
      "3846 [D loss: 0.619(R 0.707, F0.530)] [D acc: 0.664(R 0.562, F 0.766)] [G loss: 0.619] [G acc: 0.156]\n",
      "3847 [D loss: 0.478(R 0.554, F0.402)] [D acc: 0.734(R 0.641, F 0.828)] [G loss: 0.478] [G acc: 0.094]\n",
      "3848 [D loss: 0.511(R 0.494, F0.529)] [D acc: 0.727(R 0.719, F 0.734)] [G loss: 0.511] [G acc: 0.047]\n",
      "3849 [D loss: 0.449(R 0.525, F0.374)] [D acc: 0.828(R 0.750, F 0.906)] [G loss: 0.449] [G acc: 0.078]\n",
      "3850 [D loss: 0.487(R 0.534, F0.440)] [D acc: 0.781(R 0.672, F 0.891)] [G loss: 0.487] [G acc: 0.188]\n",
      "3851 [D loss: 0.521(R 0.541, F0.502)] [D acc: 0.719(R 0.656, F 0.781)] [G loss: 0.521] [G acc: 0.062]\n",
      "3852 [D loss: 0.564(R 0.451, F0.676)] [D acc: 0.719(R 0.766, F 0.672)] [G loss: 0.564] [G acc: 0.141]\n",
      "3853 [D loss: 0.479(R 0.486, F0.472)] [D acc: 0.844(R 0.812, F 0.875)] [G loss: 0.479] [G acc: 0.125]\n",
      "3854 [D loss: 0.457(R 0.457, F0.457)] [D acc: 0.766(R 0.688, F 0.844)] [G loss: 0.457] [G acc: 0.109]\n",
      "3855 [D loss: 0.473(R 0.589, F0.357)] [D acc: 0.789(R 0.672, F 0.906)] [G loss: 0.473] [G acc: 0.141]\n",
      "3856 [D loss: 0.588(R 0.468, F0.709)] [D acc: 0.695(R 0.703, F 0.688)] [G loss: 0.588] [G acc: 0.031]\n",
      "3857 [D loss: 0.535(R 0.620, F0.450)] [D acc: 0.766(R 0.703, F 0.828)] [G loss: 0.535] [G acc: 0.156]\n",
      "3858 [D loss: 0.546(R 0.532, F0.560)] [D acc: 0.727(R 0.703, F 0.750)] [G loss: 0.546] [G acc: 0.094]\n",
      "3859 [D loss: 0.607(R 0.686, F0.527)] [D acc: 0.688(R 0.594, F 0.781)] [G loss: 0.607] [G acc: 0.062]\n",
      "3860 [D loss: 0.473(R 0.546, F0.400)] [D acc: 0.812(R 0.734, F 0.891)] [G loss: 0.473] [G acc: 0.031]\n",
      "3861 [D loss: 0.578(R 0.512, F0.643)] [D acc: 0.703(R 0.703, F 0.703)] [G loss: 0.578] [G acc: 0.094]\n",
      "3862 [D loss: 0.512(R 0.555, F0.469)] [D acc: 0.750(R 0.688, F 0.812)] [G loss: 0.512] [G acc: 0.094]\n",
      "3863 [D loss: 0.489(R 0.547, F0.431)] [D acc: 0.773(R 0.656, F 0.891)] [G loss: 0.489] [G acc: 0.062]\n",
      "3864 [D loss: 0.582(R 0.560, F0.604)] [D acc: 0.734(R 0.688, F 0.781)] [G loss: 0.582] [G acc: 0.078]\n",
      "3865 [D loss: 0.517(R 0.620, F0.415)] [D acc: 0.750(R 0.625, F 0.875)] [G loss: 0.517] [G acc: 0.156]\n",
      "3866 [D loss: 0.425(R 0.409, F0.442)] [D acc: 0.828(R 0.844, F 0.812)] [G loss: 0.425] [G acc: 0.078]\n",
      "3867 [D loss: 0.507(R 0.513, F0.501)] [D acc: 0.727(R 0.672, F 0.781)] [G loss: 0.507] [G acc: 0.062]\n",
      "3868 [D loss: 0.553(R 0.457, F0.649)] [D acc: 0.734(R 0.766, F 0.703)] [G loss: 0.553] [G acc: 0.047]\n",
      "3869 [D loss: 0.631(R 0.684, F0.578)] [D acc: 0.688(R 0.641, F 0.734)] [G loss: 0.631] [G acc: 0.078]\n",
      "3870 [D loss: 0.534(R 0.559, F0.508)] [D acc: 0.695(R 0.641, F 0.750)] [G loss: 0.534] [G acc: 0.078]\n",
      "3871 [D loss: 0.520(R 0.617, F0.423)] [D acc: 0.758(R 0.609, F 0.906)] [G loss: 0.520] [G acc: 0.109]\n",
      "3872 [D loss: 0.466(R 0.462, F0.470)] [D acc: 0.750(R 0.672, F 0.828)] [G loss: 0.466] [G acc: 0.141]\n",
      "3873 [D loss: 0.531(R 0.565, F0.498)] [D acc: 0.703(R 0.578, F 0.828)] [G loss: 0.531] [G acc: 0.078]\n",
      "3874 [D loss: 0.503(R 0.456, F0.550)] [D acc: 0.727(R 0.719, F 0.734)] [G loss: 0.503] [G acc: 0.094]\n",
      "3875 [D loss: 0.485(R 0.551, F0.420)] [D acc: 0.727(R 0.625, F 0.828)] [G loss: 0.485] [G acc: 0.094]\n",
      "3876 [D loss: 0.444(R 0.434, F0.455)] [D acc: 0.820(R 0.797, F 0.844)] [G loss: 0.444] [G acc: 0.062]\n",
      "3877 [D loss: 0.532(R 0.493, F0.570)] [D acc: 0.734(R 0.719, F 0.750)] [G loss: 0.532] [G acc: 0.047]\n",
      "3878 [D loss: 0.527(R 0.532, F0.523)] [D acc: 0.695(R 0.672, F 0.719)] [G loss: 0.527] [G acc: 0.109]\n",
      "3879 [D loss: 0.589(R 0.664, F0.514)] [D acc: 0.695(R 0.609, F 0.781)] [G loss: 0.589] [G acc: 0.031]\n",
      "3880 [D loss: 0.500(R 0.479, F0.521)] [D acc: 0.789(R 0.750, F 0.828)] [G loss: 0.500] [G acc: 0.125]\n",
      "3881 [D loss: 0.514(R 0.539, F0.488)] [D acc: 0.766(R 0.688, F 0.844)] [G loss: 0.514] [G acc: 0.156]\n",
      "3882 [D loss: 0.503(R 0.412, F0.595)] [D acc: 0.719(R 0.750, F 0.688)] [G loss: 0.503] [G acc: 0.078]\n",
      "3883 [D loss: 0.534(R 0.566, F0.502)] [D acc: 0.750(R 0.672, F 0.828)] [G loss: 0.534] [G acc: 0.047]\n",
      "3884 [D loss: 0.538(R 0.520, F0.557)] [D acc: 0.750(R 0.672, F 0.828)] [G loss: 0.538] [G acc: 0.125]\n",
      "3885 [D loss: 0.553(R 0.627, F0.478)] [D acc: 0.711(R 0.625, F 0.797)] [G loss: 0.553] [G acc: 0.156]\n",
      "3886 [D loss: 0.541(R 0.525, F0.557)] [D acc: 0.758(R 0.688, F 0.828)] [G loss: 0.541] [G acc: 0.109]\n",
      "3887 [D loss: 0.545(R 0.673, F0.418)] [D acc: 0.688(R 0.516, F 0.859)] [G loss: 0.545] [G acc: 0.094]\n",
      "3888 [D loss: 0.532(R 0.514, F0.550)] [D acc: 0.750(R 0.719, F 0.781)] [G loss: 0.532] [G acc: 0.094]\n",
      "3889 [D loss: 0.579(R 0.657, F0.500)] [D acc: 0.711(R 0.594, F 0.828)] [G loss: 0.579] [G acc: 0.109]\n",
      "3890 [D loss: 0.520(R 0.402, F0.637)] [D acc: 0.758(R 0.781, F 0.734)] [G loss: 0.520] [G acc: 0.141]\n",
      "3891 [D loss: 0.588(R 0.559, F0.616)] [D acc: 0.656(R 0.625, F 0.688)] [G loss: 0.588] [G acc: 0.078]\n",
      "3892 [D loss: 0.437(R 0.451, F0.424)] [D acc: 0.820(R 0.750, F 0.891)] [G loss: 0.437] [G acc: 0.078]\n",
      "3893 [D loss: 0.485(R 0.483, F0.486)] [D acc: 0.789(R 0.750, F 0.828)] [G loss: 0.485] [G acc: 0.078]\n",
      "3894 [D loss: 0.470(R 0.432, F0.509)] [D acc: 0.805(R 0.734, F 0.875)] [G loss: 0.470] [G acc: 0.156]\n",
      "3895 [D loss: 0.546(R 0.559, F0.533)] [D acc: 0.688(R 0.625, F 0.750)] [G loss: 0.546] [G acc: 0.156]\n",
      "3896 [D loss: 0.574(R 0.681, F0.467)] [D acc: 0.711(R 0.609, F 0.812)] [G loss: 0.574] [G acc: 0.078]\n",
      "3897 [D loss: 0.553(R 0.534, F0.572)] [D acc: 0.719(R 0.656, F 0.781)] [G loss: 0.553] [G acc: 0.031]\n",
      "3898 [D loss: 0.569(R 0.514, F0.624)] [D acc: 0.742(R 0.688, F 0.797)] [G loss: 0.569] [G acc: 0.141]\n",
      "3899 [D loss: 0.552(R 0.536, F0.567)] [D acc: 0.695(R 0.641, F 0.750)] [G loss: 0.552] [G acc: 0.062]\n",
      "INFO:tensorflow:Assets written to: ram://6b387591-c667-459e-867e-6cc09c32aa3e/assets\n",
      "INFO:tensorflow:Assets written to: ram://ce3f6958-c454-4303-acac-c91c0d5c596b/assets\n",
      "INFO:tensorflow:Assets written to: ram://f4445159-6408-4274-9dcb-494c7cf1e590/assets\n",
      "3900 [D loss: 0.559(R 0.629, F0.490)] [D acc: 0.719(R 0.609, F 0.828)] [G loss: 0.559] [G acc: 0.141]\n",
      "3901 [D loss: 0.629(R 0.593, F0.664)] [D acc: 0.641(R 0.609, F 0.672)] [G loss: 0.629] [G acc: 0.094]\n",
      "3902 [D loss: 0.561(R 0.601, F0.522)] [D acc: 0.703(R 0.562, F 0.844)] [G loss: 0.561] [G acc: 0.078]\n",
      "3903 [D loss: 0.568(R 0.681, F0.454)] [D acc: 0.648(R 0.531, F 0.766)] [G loss: 0.568] [G acc: 0.094]\n",
      "3904 [D loss: 0.474(R 0.473, F0.474)] [D acc: 0.742(R 0.719, F 0.766)] [G loss: 0.474] [G acc: 0.109]\n",
      "3905 [D loss: 0.578(R 0.559, F0.598)] [D acc: 0.695(R 0.688, F 0.703)] [G loss: 0.578] [G acc: 0.094]\n",
      "3906 [D loss: 0.564(R 0.585, F0.544)] [D acc: 0.719(R 0.672, F 0.766)] [G loss: 0.564] [G acc: 0.172]\n",
      "3907 [D loss: 0.579(R 0.622, F0.535)] [D acc: 0.695(R 0.625, F 0.766)] [G loss: 0.579] [G acc: 0.125]\n",
      "3908 [D loss: 0.571(R 0.594, F0.548)] [D acc: 0.688(R 0.625, F 0.750)] [G loss: 0.571] [G acc: 0.141]\n",
      "3909 [D loss: 0.534(R 0.518, F0.549)] [D acc: 0.742(R 0.703, F 0.781)] [G loss: 0.534] [G acc: 0.094]\n",
      "3910 [D loss: 0.511(R 0.590, F0.432)] [D acc: 0.734(R 0.609, F 0.859)] [G loss: 0.511] [G acc: 0.031]\n",
      "3911 [D loss: 0.535(R 0.590, F0.481)] [D acc: 0.734(R 0.578, F 0.891)] [G loss: 0.535] [G acc: 0.094]\n",
      "3912 [D loss: 0.568(R 0.500, F0.635)] [D acc: 0.742(R 0.703, F 0.781)] [G loss: 0.568] [G acc: 0.172]\n",
      "3913 [D loss: 0.494(R 0.565, F0.422)] [D acc: 0.789(R 0.688, F 0.891)] [G loss: 0.494] [G acc: 0.141]\n",
      "3914 [D loss: 0.584(R 0.538, F0.630)] [D acc: 0.750(R 0.750, F 0.750)] [G loss: 0.584] [G acc: 0.172]\n",
      "3915 [D loss: 0.640(R 0.641, F0.638)] [D acc: 0.672(R 0.562, F 0.781)] [G loss: 0.640] [G acc: 0.125]\n",
      "3916 [D loss: 0.544(R 0.529, F0.558)] [D acc: 0.711(R 0.625, F 0.797)] [G loss: 0.544] [G acc: 0.078]\n",
      "3917 [D loss: 0.567(R 0.505, F0.628)] [D acc: 0.703(R 0.672, F 0.734)] [G loss: 0.567] [G acc: 0.109]\n",
      "3918 [D loss: 0.617(R 0.702, F0.533)] [D acc: 0.641(R 0.531, F 0.750)] [G loss: 0.617] [G acc: 0.156]\n",
      "3919 [D loss: 0.438(R 0.418, F0.458)] [D acc: 0.797(R 0.781, F 0.812)] [G loss: 0.438] [G acc: 0.156]\n",
      "3920 [D loss: 0.552(R 0.615, F0.489)] [D acc: 0.703(R 0.594, F 0.812)] [G loss: 0.552] [G acc: 0.109]\n",
      "3921 [D loss: 0.660(R 0.417, F0.903)] [D acc: 0.641(R 0.719, F 0.562)] [G loss: 0.660] [G acc: 0.078]\n",
      "3922 [D loss: 0.518(R 0.591, F0.444)] [D acc: 0.766(R 0.656, F 0.875)] [G loss: 0.518] [G acc: 0.047]\n",
      "3923 [D loss: 0.528(R 0.559, F0.497)] [D acc: 0.734(R 0.656, F 0.812)] [G loss: 0.528] [G acc: 0.094]\n",
      "3924 [D loss: 0.562(R 0.662, F0.463)] [D acc: 0.695(R 0.594, F 0.797)] [G loss: 0.562] [G acc: 0.078]\n",
      "3925 [D loss: 0.488(R 0.506, F0.469)] [D acc: 0.695(R 0.641, F 0.750)] [G loss: 0.488] [G acc: 0.125]\n",
      "3926 [D loss: 0.504(R 0.422, F0.586)] [D acc: 0.727(R 0.703, F 0.750)] [G loss: 0.504] [G acc: 0.094]\n",
      "3927 [D loss: 0.546(R 0.625, F0.468)] [D acc: 0.734(R 0.609, F 0.859)] [G loss: 0.546] [G acc: 0.094]\n",
      "3928 [D loss: 0.549(R 0.541, F0.558)] [D acc: 0.680(R 0.609, F 0.750)] [G loss: 0.549] [G acc: 0.109]\n",
      "3929 [D loss: 0.616(R 0.672, F0.561)] [D acc: 0.695(R 0.562, F 0.828)] [G loss: 0.616] [G acc: 0.125]\n",
      "3930 [D loss: 0.565(R 0.613, F0.518)] [D acc: 0.727(R 0.656, F 0.797)] [G loss: 0.565] [G acc: 0.078]\n",
      "3931 [D loss: 0.519(R 0.604, F0.433)] [D acc: 0.742(R 0.641, F 0.844)] [G loss: 0.519] [G acc: 0.094]\n",
      "3932 [D loss: 0.515(R 0.499, F0.530)] [D acc: 0.719(R 0.656, F 0.781)] [G loss: 0.515] [G acc: 0.109]\n",
      "3933 [D loss: 0.547(R 0.613, F0.480)] [D acc: 0.758(R 0.703, F 0.812)] [G loss: 0.547] [G acc: 0.172]\n",
      "3934 [D loss: 0.455(R 0.449, F0.462)] [D acc: 0.766(R 0.750, F 0.781)] [G loss: 0.455] [G acc: 0.047]\n",
      "3935 [D loss: 0.544(R 0.514, F0.574)] [D acc: 0.711(R 0.703, F 0.719)] [G loss: 0.544] [G acc: 0.109]\n",
      "3936 [D loss: 0.550(R 0.681, F0.420)] [D acc: 0.750(R 0.594, F 0.906)] [G loss: 0.550] [G acc: 0.031]\n",
      "3937 [D loss: 0.550(R 0.640, F0.460)] [D acc: 0.711(R 0.578, F 0.844)] [G loss: 0.550] [G acc: 0.109]\n",
      "3938 [D loss: 0.459(R 0.440, F0.478)] [D acc: 0.781(R 0.766, F 0.797)] [G loss: 0.459] [G acc: 0.109]\n",
      "3939 [D loss: 0.503(R 0.537, F0.468)] [D acc: 0.711(R 0.656, F 0.766)] [G loss: 0.503] [G acc: 0.109]\n",
      "3940 [D loss: 0.583(R 0.653, F0.512)] [D acc: 0.688(R 0.594, F 0.781)] [G loss: 0.583] [G acc: 0.062]\n",
      "3941 [D loss: 0.604(R 0.583, F0.624)] [D acc: 0.680(R 0.609, F 0.750)] [G loss: 0.604] [G acc: 0.141]\n",
      "3942 [D loss: 0.523(R 0.511, F0.535)] [D acc: 0.781(R 0.719, F 0.844)] [G loss: 0.523] [G acc: 0.062]\n",
      "3943 [D loss: 0.469(R 0.426, F0.513)] [D acc: 0.773(R 0.766, F 0.781)] [G loss: 0.469] [G acc: 0.156]\n",
      "3944 [D loss: 0.513(R 0.473, F0.554)] [D acc: 0.758(R 0.734, F 0.781)] [G loss: 0.513] [G acc: 0.094]\n",
      "3945 [D loss: 0.599(R 0.736, F0.462)] [D acc: 0.703(R 0.516, F 0.891)] [G loss: 0.599] [G acc: 0.156]\n",
      "3946 [D loss: 0.600(R 0.655, F0.546)] [D acc: 0.688(R 0.641, F 0.734)] [G loss: 0.600] [G acc: 0.094]\n",
      "3947 [D loss: 0.507(R 0.557, F0.457)] [D acc: 0.750(R 0.672, F 0.828)] [G loss: 0.507] [G acc: 0.094]\n",
      "3948 [D loss: 0.584(R 0.597, F0.571)] [D acc: 0.695(R 0.594, F 0.797)] [G loss: 0.584] [G acc: 0.047]\n",
      "3949 [D loss: 0.525(R 0.474, F0.575)] [D acc: 0.758(R 0.750, F 0.766)] [G loss: 0.525] [G acc: 0.109]\n",
      "3950 [D loss: 0.616(R 0.543, F0.690)] [D acc: 0.680(R 0.656, F 0.703)] [G loss: 0.616] [G acc: 0.094]\n",
      "3951 [D loss: 0.549(R 0.601, F0.497)] [D acc: 0.703(R 0.562, F 0.844)] [G loss: 0.549] [G acc: 0.078]\n",
      "3952 [D loss: 0.518(R 0.481, F0.556)] [D acc: 0.727(R 0.688, F 0.766)] [G loss: 0.518] [G acc: 0.109]\n",
      "3953 [D loss: 0.563(R 0.646, F0.480)] [D acc: 0.672(R 0.562, F 0.781)] [G loss: 0.563] [G acc: 0.062]\n",
      "3954 [D loss: 0.563(R 0.576, F0.549)] [D acc: 0.727(R 0.672, F 0.781)] [G loss: 0.563] [G acc: 0.062]\n",
      "3955 [D loss: 0.502(R 0.512, F0.492)] [D acc: 0.750(R 0.672, F 0.828)] [G loss: 0.502] [G acc: 0.109]\n",
      "3956 [D loss: 0.635(R 0.383, F0.887)] [D acc: 0.750(R 0.766, F 0.734)] [G loss: 0.635] [G acc: 0.094]\n",
      "3957 [D loss: 0.452(R 0.497, F0.407)] [D acc: 0.789(R 0.703, F 0.875)] [G loss: 0.452] [G acc: 0.078]\n",
      "3958 [D loss: 0.589(R 0.752, F0.427)] [D acc: 0.688(R 0.531, F 0.844)] [G loss: 0.589] [G acc: 0.062]\n",
      "3959 [D loss: 0.507(R 0.398, F0.616)] [D acc: 0.812(R 0.781, F 0.844)] [G loss: 0.507] [G acc: 0.062]\n",
      "3960 [D loss: 0.499(R 0.508, F0.490)] [D acc: 0.781(R 0.703, F 0.859)] [G loss: 0.499] [G acc: 0.094]\n",
      "3961 [D loss: 0.511(R 0.569, F0.453)] [D acc: 0.750(R 0.609, F 0.891)] [G loss: 0.511] [G acc: 0.062]\n",
      "3962 [D loss: 0.598(R 0.629, F0.567)] [D acc: 0.664(R 0.547, F 0.781)] [G loss: 0.598] [G acc: 0.125]\n",
      "3963 [D loss: 0.497(R 0.514, F0.480)] [D acc: 0.781(R 0.750, F 0.812)] [G loss: 0.497] [G acc: 0.156]\n",
      "3964 [D loss: 0.462(R 0.534, F0.391)] [D acc: 0.781(R 0.688, F 0.875)] [G loss: 0.462] [G acc: 0.016]\n",
      "3965 [D loss: 0.600(R 0.616, F0.585)] [D acc: 0.664(R 0.578, F 0.750)] [G loss: 0.600] [G acc: 0.047]\n",
      "3966 [D loss: 0.530(R 0.625, F0.436)] [D acc: 0.742(R 0.609, F 0.875)] [G loss: 0.530] [G acc: 0.125]\n",
      "3967 [D loss: 0.566(R 0.461, F0.672)] [D acc: 0.711(R 0.656, F 0.766)] [G loss: 0.566] [G acc: 0.062]\n",
      "3968 [D loss: 0.543(R 0.548, F0.538)] [D acc: 0.750(R 0.609, F 0.891)] [G loss: 0.543] [G acc: 0.109]\n",
      "3969 [D loss: 0.532(R 0.592, F0.473)] [D acc: 0.727(R 0.656, F 0.797)] [G loss: 0.532] [G acc: 0.109]\n",
      "3970 [D loss: 0.489(R 0.499, F0.478)] [D acc: 0.734(R 0.672, F 0.797)] [G loss: 0.489] [G acc: 0.156]\n",
      "3971 [D loss: 0.530(R 0.517, F0.543)] [D acc: 0.742(R 0.703, F 0.781)] [G loss: 0.530] [G acc: 0.031]\n",
      "3972 [D loss: 0.593(R 0.626, F0.560)] [D acc: 0.695(R 0.578, F 0.812)] [G loss: 0.593] [G acc: 0.078]\n",
      "3973 [D loss: 0.554(R 0.631, F0.478)] [D acc: 0.695(R 0.578, F 0.812)] [G loss: 0.554] [G acc: 0.047]\n",
      "3974 [D loss: 0.603(R 0.622, F0.584)] [D acc: 0.664(R 0.578, F 0.750)] [G loss: 0.603] [G acc: 0.094]\n",
      "3975 [D loss: 0.526(R 0.618, F0.434)] [D acc: 0.742(R 0.594, F 0.891)] [G loss: 0.526] [G acc: 0.078]\n",
      "3976 [D loss: 0.523(R 0.517, F0.529)] [D acc: 0.758(R 0.719, F 0.797)] [G loss: 0.523] [G acc: 0.078]\n",
      "3977 [D loss: 0.481(R 0.499, F0.463)] [D acc: 0.758(R 0.703, F 0.812)] [G loss: 0.481] [G acc: 0.047]\n",
      "3978 [D loss: 0.429(R 0.452, F0.405)] [D acc: 0.773(R 0.688, F 0.859)] [G loss: 0.429] [G acc: 0.062]\n",
      "3979 [D loss: 0.534(R 0.501, F0.567)] [D acc: 0.742(R 0.719, F 0.766)] [G loss: 0.534] [G acc: 0.109]\n",
      "3980 [D loss: 0.527(R 0.502, F0.551)] [D acc: 0.703(R 0.688, F 0.719)] [G loss: 0.527] [G acc: 0.047]\n",
      "3981 [D loss: 0.558(R 0.614, F0.503)] [D acc: 0.680(R 0.578, F 0.781)] [G loss: 0.558] [G acc: 0.078]\n",
      "3982 [D loss: 0.509(R 0.482, F0.536)] [D acc: 0.766(R 0.719, F 0.812)] [G loss: 0.509] [G acc: 0.156]\n",
      "3983 [D loss: 0.575(R 0.635, F0.516)] [D acc: 0.711(R 0.594, F 0.828)] [G loss: 0.575] [G acc: 0.094]\n",
      "3984 [D loss: 0.554(R 0.573, F0.535)] [D acc: 0.734(R 0.656, F 0.812)] [G loss: 0.554] [G acc: 0.078]\n",
      "3985 [D loss: 0.505(R 0.554, F0.455)] [D acc: 0.766(R 0.688, F 0.844)] [G loss: 0.505] [G acc: 0.156]\n",
      "3986 [D loss: 0.595(R 0.578, F0.612)] [D acc: 0.727(R 0.703, F 0.750)] [G loss: 0.595] [G acc: 0.141]\n",
      "3987 [D loss: 0.511(R 0.573, F0.450)] [D acc: 0.781(R 0.688, F 0.875)] [G loss: 0.511] [G acc: 0.109]\n",
      "3988 [D loss: 0.455(R 0.415, F0.495)] [D acc: 0.789(R 0.734, F 0.844)] [G loss: 0.455] [G acc: 0.125]\n",
      "3989 [D loss: 0.557(R 0.525, F0.588)] [D acc: 0.703(R 0.641, F 0.766)] [G loss: 0.557] [G acc: 0.047]\n",
      "3990 [D loss: 0.557(R 0.526, F0.587)] [D acc: 0.758(R 0.703, F 0.812)] [G loss: 0.557] [G acc: 0.062]\n",
      "3991 [D loss: 0.439(R 0.396, F0.482)] [D acc: 0.781(R 0.781, F 0.781)] [G loss: 0.439] [G acc: 0.062]\n",
      "3992 [D loss: 0.507(R 0.630, F0.383)] [D acc: 0.781(R 0.672, F 0.891)] [G loss: 0.507] [G acc: 0.047]\n",
      "3993 [D loss: 0.486(R 0.455, F0.517)] [D acc: 0.797(R 0.750, F 0.844)] [G loss: 0.486] [G acc: 0.062]\n",
      "3994 [D loss: 0.498(R 0.526, F0.471)] [D acc: 0.742(R 0.656, F 0.828)] [G loss: 0.498] [G acc: 0.078]\n",
      "3995 [D loss: 0.408(R 0.371, F0.444)] [D acc: 0.789(R 0.766, F 0.812)] [G loss: 0.408] [G acc: 0.125]\n",
      "3996 [D loss: 0.493(R 0.510, F0.475)] [D acc: 0.789(R 0.750, F 0.828)] [G loss: 0.493] [G acc: 0.062]\n",
      "3997 [D loss: 0.494(R 0.478, F0.510)] [D acc: 0.758(R 0.703, F 0.812)] [G loss: 0.494] [G acc: 0.078]\n",
      "3998 [D loss: 0.514(R 0.543, F0.484)] [D acc: 0.734(R 0.641, F 0.828)] [G loss: 0.514] [G acc: 0.094]\n",
      "3999 [D loss: 0.517(R 0.456, F0.578)] [D acc: 0.711(R 0.703, F 0.719)] [G loss: 0.517] [G acc: 0.078]\n",
      "INFO:tensorflow:Assets written to: ram://f39ada9a-efd8-439c-bef3-c10d7f9db0af/assets\n",
      "INFO:tensorflow:Assets written to: ram://95024fa6-2590-43d3-b133-0e52a73efc87/assets\n",
      "INFO:tensorflow:Assets written to: ram://c5fe3a1f-ecd7-486b-81bb-b9158809d103/assets\n",
      "4000 [D loss: 0.587(R 0.702, F0.472)] [D acc: 0.664(R 0.562, F 0.766)] [G loss: 0.587] [G acc: 0.094]\n",
      "4001 [D loss: 0.604(R 0.496, F0.712)] [D acc: 0.719(R 0.703, F 0.734)] [G loss: 0.604] [G acc: 0.047]\n",
      "4002 [D loss: 0.420(R 0.485, F0.355)] [D acc: 0.844(R 0.719, F 0.969)] [G loss: 0.420] [G acc: 0.062]\n",
      "4003 [D loss: 0.468(R 0.470, F0.465)] [D acc: 0.734(R 0.656, F 0.812)] [G loss: 0.468] [G acc: 0.094]\n",
      "4004 [D loss: 0.641(R 0.508, F0.774)] [D acc: 0.664(R 0.672, F 0.656)] [G loss: 0.641] [G acc: 0.094]\n",
      "4005 [D loss: 0.479(R 0.484, F0.474)] [D acc: 0.781(R 0.719, F 0.844)] [G loss: 0.479] [G acc: 0.141]\n",
      "4006 [D loss: 0.527(R 0.518, F0.537)] [D acc: 0.758(R 0.688, F 0.828)] [G loss: 0.527] [G acc: 0.125]\n",
      "4007 [D loss: 0.562(R 0.628, F0.496)] [D acc: 0.742(R 0.656, F 0.828)] [G loss: 0.562] [G acc: 0.141]\n",
      "4008 [D loss: 0.447(R 0.432, F0.463)] [D acc: 0.766(R 0.719, F 0.812)] [G loss: 0.447] [G acc: 0.125]\n",
      "4009 [D loss: 0.474(R 0.498, F0.449)] [D acc: 0.766(R 0.734, F 0.797)] [G loss: 0.474] [G acc: 0.062]\n",
      "4010 [D loss: 0.674(R 0.734, F0.614)] [D acc: 0.641(R 0.516, F 0.766)] [G loss: 0.674] [G acc: 0.078]\n",
      "4011 [D loss: 0.555(R 0.623, F0.487)] [D acc: 0.781(R 0.703, F 0.859)] [G loss: 0.555] [G acc: 0.016]\n",
      "4012 [D loss: 0.511(R 0.571, F0.451)] [D acc: 0.750(R 0.672, F 0.828)] [G loss: 0.511] [G acc: 0.047]\n",
      "4013 [D loss: 0.537(R 0.505, F0.569)] [D acc: 0.719(R 0.719, F 0.719)] [G loss: 0.537] [G acc: 0.062]\n",
      "4014 [D loss: 0.523(R 0.514, F0.533)] [D acc: 0.711(R 0.672, F 0.750)] [G loss: 0.523] [G acc: 0.047]\n",
      "4015 [D loss: 0.512(R 0.546, F0.479)] [D acc: 0.727(R 0.625, F 0.828)] [G loss: 0.512] [G acc: 0.156]\n",
      "4016 [D loss: 0.508(R 0.529, F0.486)] [D acc: 0.773(R 0.703, F 0.844)] [G loss: 0.508] [G acc: 0.078]\n",
      "4017 [D loss: 0.472(R 0.502, F0.443)] [D acc: 0.781(R 0.719, F 0.844)] [G loss: 0.472] [G acc: 0.156]\n",
      "4018 [D loss: 0.716(R 0.549, F0.883)] [D acc: 0.633(R 0.625, F 0.641)] [G loss: 0.716] [G acc: 0.062]\n",
      "4019 [D loss: 0.576(R 0.656, F0.496)] [D acc: 0.641(R 0.547, F 0.734)] [G loss: 0.576] [G acc: 0.062]\n",
      "4020 [D loss: 0.528(R 0.577, F0.480)] [D acc: 0.766(R 0.688, F 0.844)] [G loss: 0.528] [G acc: 0.094]\n",
      "4021 [D loss: 0.484(R 0.413, F0.555)] [D acc: 0.750(R 0.766, F 0.734)] [G loss: 0.484] [G acc: 0.094]\n",
      "4022 [D loss: 0.518(R 0.579, F0.458)] [D acc: 0.750(R 0.656, F 0.844)] [G loss: 0.518] [G acc: 0.031]\n",
      "4023 [D loss: 0.504(R 0.540, F0.467)] [D acc: 0.734(R 0.641, F 0.828)] [G loss: 0.504] [G acc: 0.031]\n",
      "4024 [D loss: 0.567(R 0.555, F0.578)] [D acc: 0.680(R 0.656, F 0.703)] [G loss: 0.567] [G acc: 0.125]\n",
      "4025 [D loss: 0.460(R 0.493, F0.428)] [D acc: 0.781(R 0.688, F 0.875)] [G loss: 0.460] [G acc: 0.078]\n",
      "4026 [D loss: 0.535(R 0.476, F0.595)] [D acc: 0.750(R 0.734, F 0.766)] [G loss: 0.535] [G acc: 0.062]\n",
      "4027 [D loss: 0.508(R 0.561, F0.454)] [D acc: 0.758(R 0.656, F 0.859)] [G loss: 0.508] [G acc: 0.125]\n",
      "4028 [D loss: 0.566(R 0.510, F0.622)] [D acc: 0.719(R 0.672, F 0.766)] [G loss: 0.566] [G acc: 0.094]\n",
      "4029 [D loss: 0.575(R 0.620, F0.531)] [D acc: 0.688(R 0.594, F 0.781)] [G loss: 0.575] [G acc: 0.062]\n",
      "4030 [D loss: 0.537(R 0.612, F0.461)] [D acc: 0.750(R 0.641, F 0.859)] [G loss: 0.537] [G acc: 0.062]\n",
      "4031 [D loss: 0.558(R 0.593, F0.523)] [D acc: 0.727(R 0.641, F 0.812)] [G loss: 0.558] [G acc: 0.109]\n",
      "4032 [D loss: 0.419(R 0.403, F0.435)] [D acc: 0.805(R 0.734, F 0.875)] [G loss: 0.419] [G acc: 0.062]\n",
      "4033 [D loss: 0.609(R 0.487, F0.732)] [D acc: 0.719(R 0.734, F 0.703)] [G loss: 0.609] [G acc: 0.125]\n",
      "4034 [D loss: 0.516(R 0.593, F0.438)] [D acc: 0.750(R 0.625, F 0.875)] [G loss: 0.516] [G acc: 0.094]\n",
      "4035 [D loss: 0.588(R 0.653, F0.524)] [D acc: 0.648(R 0.547, F 0.750)] [G loss: 0.588] [G acc: 0.109]\n",
      "4036 [D loss: 0.483(R 0.421, F0.544)] [D acc: 0.727(R 0.688, F 0.766)] [G loss: 0.483] [G acc: 0.047]\n",
      "4037 [D loss: 0.421(R 0.476, F0.366)] [D acc: 0.805(R 0.734, F 0.875)] [G loss: 0.421] [G acc: 0.078]\n",
      "4038 [D loss: 0.477(R 0.476, F0.477)] [D acc: 0.773(R 0.703, F 0.844)] [G loss: 0.477] [G acc: 0.047]\n",
      "4039 [D loss: 0.472(R 0.460, F0.484)] [D acc: 0.758(R 0.703, F 0.812)] [G loss: 0.472] [G acc: 0.062]\n",
      "4040 [D loss: 0.495(R 0.532, F0.457)] [D acc: 0.742(R 0.656, F 0.828)] [G loss: 0.495] [G acc: 0.094]\n",
      "4041 [D loss: 0.466(R 0.474, F0.459)] [D acc: 0.789(R 0.734, F 0.844)] [G loss: 0.466] [G acc: 0.125]\n",
      "4042 [D loss: 0.496(R 0.449, F0.543)] [D acc: 0.766(R 0.797, F 0.734)] [G loss: 0.496] [G acc: 0.016]\n",
      "4043 [D loss: 0.520(R 0.668, F0.373)] [D acc: 0.734(R 0.562, F 0.906)] [G loss: 0.520] [G acc: 0.031]\n",
      "4044 [D loss: 0.525(R 0.548, F0.502)] [D acc: 0.734(R 0.656, F 0.812)] [G loss: 0.525] [G acc: 0.062]\n",
      "4045 [D loss: 0.597(R 0.599, F0.596)] [D acc: 0.680(R 0.609, F 0.750)] [G loss: 0.597] [G acc: 0.078]\n",
      "4046 [D loss: 0.426(R 0.511, F0.341)] [D acc: 0.773(R 0.672, F 0.875)] [G loss: 0.426] [G acc: 0.078]\n",
      "4047 [D loss: 0.451(R 0.412, F0.489)] [D acc: 0.750(R 0.750, F 0.750)] [G loss: 0.451] [G acc: 0.094]\n",
      "4048 [D loss: 0.415(R 0.501, F0.328)] [D acc: 0.805(R 0.672, F 0.938)] [G loss: 0.415] [G acc: 0.094]\n",
      "4049 [D loss: 0.458(R 0.352, F0.563)] [D acc: 0.758(R 0.781, F 0.734)] [G loss: 0.458] [G acc: 0.125]\n",
      "4050 [D loss: 0.591(R 0.644, F0.537)] [D acc: 0.719(R 0.594, F 0.844)] [G loss: 0.591] [G acc: 0.156]\n",
      "4051 [D loss: 0.546(R 0.598, F0.494)] [D acc: 0.727(R 0.609, F 0.844)] [G loss: 0.546] [G acc: 0.094]\n",
      "4052 [D loss: 0.626(R 0.750, F0.502)] [D acc: 0.641(R 0.500, F 0.781)] [G loss: 0.626] [G acc: 0.109]\n",
      "4053 [D loss: 0.535(R 0.590, F0.481)] [D acc: 0.703(R 0.594, F 0.812)] [G loss: 0.535] [G acc: 0.141]\n",
      "4054 [D loss: 0.475(R 0.373, F0.577)] [D acc: 0.766(R 0.781, F 0.750)] [G loss: 0.475] [G acc: 0.141]\n",
      "4055 [D loss: 0.550(R 0.490, F0.609)] [D acc: 0.734(R 0.672, F 0.797)] [G loss: 0.550] [G acc: 0.109]\n",
      "4056 [D loss: 0.553(R 0.630, F0.477)] [D acc: 0.703(R 0.609, F 0.797)] [G loss: 0.553] [G acc: 0.125]\n",
      "4057 [D loss: 0.492(R 0.501, F0.483)] [D acc: 0.758(R 0.719, F 0.797)] [G loss: 0.492] [G acc: 0.125]\n",
      "4058 [D loss: 0.620(R 0.639, F0.601)] [D acc: 0.656(R 0.641, F 0.672)] [G loss: 0.620] [G acc: 0.094]\n",
      "4059 [D loss: 0.644(R 0.541, F0.747)] [D acc: 0.727(R 0.641, F 0.812)] [G loss: 0.644] [G acc: 0.156]\n",
      "4060 [D loss: 0.478(R 0.490, F0.466)] [D acc: 0.750(R 0.688, F 0.812)] [G loss: 0.478] [G acc: 0.078]\n",
      "4061 [D loss: 0.477(R 0.453, F0.501)] [D acc: 0.781(R 0.750, F 0.812)] [G loss: 0.477] [G acc: 0.047]\n",
      "4062 [D loss: 0.464(R 0.543, F0.385)] [D acc: 0.766(R 0.656, F 0.875)] [G loss: 0.464] [G acc: 0.047]\n",
      "4063 [D loss: 0.485(R 0.503, F0.467)] [D acc: 0.789(R 0.719, F 0.859)] [G loss: 0.485] [G acc: 0.125]\n",
      "4064 [D loss: 0.556(R 0.474, F0.637)] [D acc: 0.758(R 0.703, F 0.812)] [G loss: 0.556] [G acc: 0.078]\n",
      "4065 [D loss: 0.532(R 0.476, F0.588)] [D acc: 0.719(R 0.688, F 0.750)] [G loss: 0.532] [G acc: 0.109]\n",
      "4066 [D loss: 0.469(R 0.544, F0.394)] [D acc: 0.781(R 0.656, F 0.906)] [G loss: 0.469] [G acc: 0.125]\n",
      "4067 [D loss: 0.539(R 0.469, F0.609)] [D acc: 0.703(R 0.719, F 0.688)] [G loss: 0.539] [G acc: 0.078]\n",
      "4068 [D loss: 0.478(R 0.564, F0.392)] [D acc: 0.742(R 0.625, F 0.859)] [G loss: 0.478] [G acc: 0.109]\n",
      "4069 [D loss: 0.665(R 0.598, F0.733)] [D acc: 0.648(R 0.625, F 0.672)] [G loss: 0.665] [G acc: 0.062]\n",
      "4070 [D loss: 0.600(R 0.616, F0.584)] [D acc: 0.656(R 0.609, F 0.703)] [G loss: 0.600] [G acc: 0.078]\n",
      "4071 [D loss: 0.488(R 0.482, F0.493)] [D acc: 0.766(R 0.734, F 0.797)] [G loss: 0.488] [G acc: 0.141]\n",
      "4072 [D loss: 0.433(R 0.416, F0.450)] [D acc: 0.789(R 0.781, F 0.797)] [G loss: 0.433] [G acc: 0.078]\n",
      "4073 [D loss: 0.588(R 0.660, F0.515)] [D acc: 0.703(R 0.578, F 0.828)] [G loss: 0.588] [G acc: 0.078]\n",
      "4074 [D loss: 0.538(R 0.575, F0.501)] [D acc: 0.750(R 0.672, F 0.828)] [G loss: 0.538] [G acc: 0.125]\n",
      "4075 [D loss: 0.474(R 0.479, F0.469)] [D acc: 0.742(R 0.656, F 0.828)] [G loss: 0.474] [G acc: 0.156]\n",
      "4076 [D loss: 1.073(R 0.400, F1.745)] [D acc: 0.617(R 0.781, F 0.453)] [G loss: 1.073] [G acc: 0.094]\n",
      "4077 [D loss: 0.525(R 0.670, F0.381)] [D acc: 0.719(R 0.547, F 0.891)] [G loss: 0.525] [G acc: 0.141]\n",
      "4078 [D loss: 0.451(R 0.522, F0.380)] [D acc: 0.805(R 0.672, F 0.938)] [G loss: 0.451] [G acc: 0.062]\n",
      "4079 [D loss: 0.540(R 0.644, F0.436)] [D acc: 0.695(R 0.562, F 0.828)] [G loss: 0.540] [G acc: 0.203]\n",
      "4080 [D loss: 0.563(R 0.541, F0.585)] [D acc: 0.711(R 0.719, F 0.703)] [G loss: 0.563] [G acc: 0.125]\n",
      "4081 [D loss: 0.532(R 0.595, F0.470)] [D acc: 0.727(R 0.656, F 0.797)] [G loss: 0.532] [G acc: 0.047]\n",
      "4082 [D loss: 0.601(R 0.546, F0.656)] [D acc: 0.648(R 0.609, F 0.688)] [G loss: 0.601] [G acc: 0.094]\n",
      "4083 [D loss: 0.464(R 0.452, F0.475)] [D acc: 0.797(R 0.766, F 0.828)] [G loss: 0.464] [G acc: 0.094]\n",
      "4084 [D loss: 0.539(R 0.559, F0.519)] [D acc: 0.727(R 0.672, F 0.781)] [G loss: 0.539] [G acc: 0.109]\n",
      "4085 [D loss: 0.594(R 0.587, F0.601)] [D acc: 0.672(R 0.609, F 0.734)] [G loss: 0.594] [G acc: 0.062]\n",
      "4086 [D loss: 0.467(R 0.498, F0.436)] [D acc: 0.773(R 0.703, F 0.844)] [G loss: 0.467] [G acc: 0.141]\n",
      "4087 [D loss: 0.504(R 0.515, F0.494)] [D acc: 0.758(R 0.703, F 0.812)] [G loss: 0.504] [G acc: 0.062]\n",
      "4088 [D loss: 0.493(R 0.505, F0.482)] [D acc: 0.719(R 0.656, F 0.781)] [G loss: 0.493] [G acc: 0.094]\n",
      "4089 [D loss: 0.504(R 0.546, F0.462)] [D acc: 0.773(R 0.672, F 0.875)] [G loss: 0.504] [G acc: 0.047]\n",
      "4090 [D loss: 0.541(R 0.591, F0.491)] [D acc: 0.773(R 0.719, F 0.828)] [G loss: 0.541] [G acc: 0.125]\n",
      "4091 [D loss: 0.447(R 0.414, F0.481)] [D acc: 0.812(R 0.797, F 0.828)] [G loss: 0.447] [G acc: 0.156]\n",
      "4092 [D loss: 0.465(R 0.462, F0.468)] [D acc: 0.781(R 0.766, F 0.797)] [G loss: 0.465] [G acc: 0.141]\n",
      "4093 [D loss: 0.494(R 0.533, F0.455)] [D acc: 0.734(R 0.641, F 0.828)] [G loss: 0.494] [G acc: 0.094]\n",
      "4094 [D loss: 0.547(R 0.474, F0.620)] [D acc: 0.695(R 0.750, F 0.641)] [G loss: 0.547] [G acc: 0.094]\n",
      "4095 [D loss: 0.515(R 0.442, F0.587)] [D acc: 0.742(R 0.734, F 0.750)] [G loss: 0.515] [G acc: 0.031]\n",
      "4096 [D loss: 0.500(R 0.642, F0.358)] [D acc: 0.734(R 0.594, F 0.875)] [G loss: 0.500] [G acc: 0.016]\n",
      "4097 [D loss: 0.532(R 0.588, F0.475)] [D acc: 0.688(R 0.641, F 0.734)] [G loss: 0.532] [G acc: 0.109]\n",
      "4098 [D loss: 0.614(R 0.607, F0.621)] [D acc: 0.688(R 0.641, F 0.734)] [G loss: 0.614] [G acc: 0.156]\n",
      "4099 [D loss: 0.609(R 0.740, F0.477)] [D acc: 0.664(R 0.500, F 0.828)] [G loss: 0.609] [G acc: 0.141]\n",
      "INFO:tensorflow:Assets written to: ram://3e4e178a-8d8f-41c2-9e74-ee143e1d851f/assets\n",
      "INFO:tensorflow:Assets written to: ram://ab81d9a5-29c2-466f-b74b-db853b94c998/assets\n",
      "INFO:tensorflow:Assets written to: ram://4688025f-9857-47b6-9464-17f0efc97a9b/assets\n",
      "4100 [D loss: 0.518(R 0.517, F0.519)] [D acc: 0.766(R 0.688, F 0.844)] [G loss: 0.518] [G acc: 0.062]\n",
      "4101 [D loss: 0.573(R 0.641, F0.505)] [D acc: 0.680(R 0.594, F 0.766)] [G loss: 0.573] [G acc: 0.078]\n",
      "4102 [D loss: 0.445(R 0.461, F0.429)] [D acc: 0.797(R 0.703, F 0.891)] [G loss: 0.445] [G acc: 0.109]\n",
      "4103 [D loss: 0.543(R 0.493, F0.594)] [D acc: 0.703(R 0.719, F 0.688)] [G loss: 0.543] [G acc: 0.031]\n",
      "4104 [D loss: 0.499(R 0.374, F0.625)] [D acc: 0.758(R 0.797, F 0.719)] [G loss: 0.499] [G acc: 0.031]\n",
      "4105 [D loss: 0.496(R 0.594, F0.397)] [D acc: 0.750(R 0.625, F 0.875)] [G loss: 0.496] [G acc: 0.203]\n",
      "4106 [D loss: 0.509(R 0.475, F0.543)] [D acc: 0.742(R 0.719, F 0.766)] [G loss: 0.509] [G acc: 0.078]\n",
      "4107 [D loss: 0.453(R 0.497, F0.409)] [D acc: 0.781(R 0.719, F 0.844)] [G loss: 0.453] [G acc: 0.141]\n",
      "4108 [D loss: 0.507(R 0.547, F0.466)] [D acc: 0.742(R 0.656, F 0.828)] [G loss: 0.507] [G acc: 0.141]\n",
      "4109 [D loss: 0.545(R 0.589, F0.502)] [D acc: 0.727(R 0.672, F 0.781)] [G loss: 0.545] [G acc: 0.141]\n",
      "4110 [D loss: 0.523(R 0.523, F0.523)] [D acc: 0.734(R 0.688, F 0.781)] [G loss: 0.523] [G acc: 0.078]\n",
      "4111 [D loss: 0.515(R 0.500, F0.531)] [D acc: 0.773(R 0.750, F 0.797)] [G loss: 0.515] [G acc: 0.172]\n",
      "4112 [D loss: 0.617(R 0.719, F0.515)] [D acc: 0.672(R 0.578, F 0.766)] [G loss: 0.617] [G acc: 0.062]\n",
      "4113 [D loss: 0.465(R 0.471, F0.460)] [D acc: 0.773(R 0.719, F 0.828)] [G loss: 0.465] [G acc: 0.062]\n",
      "4114 [D loss: 0.508(R 0.474, F0.543)] [D acc: 0.766(R 0.703, F 0.828)] [G loss: 0.508] [G acc: 0.078]\n",
      "4115 [D loss: 0.563(R 0.553, F0.573)] [D acc: 0.719(R 0.625, F 0.812)] [G loss: 0.563] [G acc: 0.078]\n",
      "4116 [D loss: 0.540(R 0.633, F0.446)] [D acc: 0.719(R 0.578, F 0.859)] [G loss: 0.540] [G acc: 0.172]\n",
      "4117 [D loss: 0.474(R 0.403, F0.546)] [D acc: 0.852(R 0.844, F 0.859)] [G loss: 0.474] [G acc: 0.016]\n",
      "4118 [D loss: 0.562(R 0.572, F0.551)] [D acc: 0.703(R 0.625, F 0.781)] [G loss: 0.562] [G acc: 0.062]\n",
      "4119 [D loss: 0.429(R 0.461, F0.397)] [D acc: 0.805(R 0.734, F 0.875)] [G loss: 0.429] [G acc: 0.078]\n",
      "4120 [D loss: 0.559(R 0.594, F0.525)] [D acc: 0.695(R 0.609, F 0.781)] [G loss: 0.559] [G acc: 0.047]\n",
      "4121 [D loss: 0.533(R 0.677, F0.388)] [D acc: 0.758(R 0.609, F 0.906)] [G loss: 0.533] [G acc: 0.094]\n",
      "4122 [D loss: 0.586(R 0.462, F0.710)] [D acc: 0.727(R 0.719, F 0.734)] [G loss: 0.586] [G acc: 0.031]\n",
      "4123 [D loss: 0.491(R 0.537, F0.446)] [D acc: 0.750(R 0.734, F 0.766)] [G loss: 0.491] [G acc: 0.109]\n",
      "4124 [D loss: 0.519(R 0.621, F0.418)] [D acc: 0.773(R 0.641, F 0.906)] [G loss: 0.519] [G acc: 0.109]\n",
      "4125 [D loss: 0.548(R 0.549, F0.548)] [D acc: 0.680(R 0.625, F 0.734)] [G loss: 0.548] [G acc: 0.109]\n",
      "4126 [D loss: 0.486(R 0.485, F0.486)] [D acc: 0.781(R 0.703, F 0.859)] [G loss: 0.486] [G acc: 0.047]\n",
      "4127 [D loss: 0.510(R 0.461, F0.560)] [D acc: 0.734(R 0.703, F 0.766)] [G loss: 0.510] [G acc: 0.062]\n",
      "4128 [D loss: 0.594(R 0.673, F0.515)] [D acc: 0.711(R 0.656, F 0.766)] [G loss: 0.594] [G acc: 0.094]\n",
      "4129 [D loss: 0.484(R 0.542, F0.426)] [D acc: 0.773(R 0.719, F 0.828)] [G loss: 0.484] [G acc: 0.078]\n",
      "4130 [D loss: 0.544(R 0.511, F0.577)] [D acc: 0.695(R 0.672, F 0.719)] [G loss: 0.544] [G acc: 0.094]\n",
      "4131 [D loss: 0.475(R 0.509, F0.442)] [D acc: 0.766(R 0.688, F 0.844)] [G loss: 0.475] [G acc: 0.109]\n",
      "4132 [D loss: 0.539(R 0.477, F0.600)] [D acc: 0.727(R 0.734, F 0.719)] [G loss: 0.539] [G acc: 0.062]\n",
      "4133 [D loss: 0.572(R 0.649, F0.496)] [D acc: 0.727(R 0.625, F 0.828)] [G loss: 0.572] [G acc: 0.109]\n",
      "4134 [D loss: 0.441(R 0.496, F0.387)] [D acc: 0.828(R 0.750, F 0.906)] [G loss: 0.441] [G acc: 0.031]\n",
      "4135 [D loss: 0.474(R 0.539, F0.409)] [D acc: 0.750(R 0.641, F 0.859)] [G loss: 0.474] [G acc: 0.094]\n",
      "4136 [D loss: 0.538(R 0.541, F0.536)] [D acc: 0.727(R 0.672, F 0.781)] [G loss: 0.538] [G acc: 0.109]\n",
      "4137 [D loss: 0.566(R 0.522, F0.611)] [D acc: 0.695(R 0.688, F 0.703)] [G loss: 0.566] [G acc: 0.016]\n",
      "4138 [D loss: 0.448(R 0.525, F0.370)] [D acc: 0.797(R 0.688, F 0.906)] [G loss: 0.448] [G acc: 0.016]\n",
      "4139 [D loss: 0.610(R 0.725, F0.496)] [D acc: 0.695(R 0.562, F 0.828)] [G loss: 0.610] [G acc: 0.125]\n",
      "4140 [D loss: 0.553(R 0.422, F0.684)] [D acc: 0.742(R 0.719, F 0.766)] [G loss: 0.553] [G acc: 0.078]\n",
      "4141 [D loss: 0.477(R 0.470, F0.484)] [D acc: 0.727(R 0.641, F 0.812)] [G loss: 0.477] [G acc: 0.031]\n",
      "4142 [D loss: 0.486(R 0.474, F0.497)] [D acc: 0.727(R 0.688, F 0.766)] [G loss: 0.486] [G acc: 0.078]\n",
      "4143 [D loss: 0.499(R 0.458, F0.539)] [D acc: 0.773(R 0.750, F 0.797)] [G loss: 0.499] [G acc: 0.094]\n",
      "4144 [D loss: 0.576(R 0.496, F0.656)] [D acc: 0.742(R 0.719, F 0.766)] [G loss: 0.576] [G acc: 0.078]\n",
      "4145 [D loss: 0.580(R 0.729, F0.431)] [D acc: 0.695(R 0.531, F 0.859)] [G loss: 0.580] [G acc: 0.078]\n",
      "4146 [D loss: 0.485(R 0.517, F0.454)] [D acc: 0.812(R 0.750, F 0.875)] [G loss: 0.485] [G acc: 0.062]\n",
      "4147 [D loss: 0.533(R 0.598, F0.468)] [D acc: 0.688(R 0.609, F 0.766)] [G loss: 0.533] [G acc: 0.125]\n",
      "4148 [D loss: 0.554(R 0.460, F0.647)] [D acc: 0.750(R 0.750, F 0.750)] [G loss: 0.554] [G acc: 0.094]\n",
      "4149 [D loss: 0.539(R 0.576, F0.502)] [D acc: 0.695(R 0.594, F 0.797)] [G loss: 0.539] [G acc: 0.172]\n",
      "4150 [D loss: 0.578(R 0.720, F0.436)] [D acc: 0.703(R 0.531, F 0.875)] [G loss: 0.578] [G acc: 0.125]\n",
      "4151 [D loss: 0.505(R 0.494, F0.516)] [D acc: 0.758(R 0.703, F 0.812)] [G loss: 0.505] [G acc: 0.109]\n",
      "4152 [D loss: 0.497(R 0.496, F0.498)] [D acc: 0.758(R 0.719, F 0.797)] [G loss: 0.497] [G acc: 0.141]\n",
      "4153 [D loss: 0.526(R 0.534, F0.518)] [D acc: 0.758(R 0.688, F 0.828)] [G loss: 0.526] [G acc: 0.000]\n",
      "4154 [D loss: 0.467(R 0.522, F0.413)] [D acc: 0.766(R 0.688, F 0.844)] [G loss: 0.467] [G acc: 0.031]\n",
      "4155 [D loss: 0.548(R 0.387, F0.709)] [D acc: 0.820(R 0.828, F 0.812)] [G loss: 0.548] [G acc: 0.094]\n",
      "4156 [D loss: 0.572(R 0.608, F0.535)] [D acc: 0.711(R 0.609, F 0.812)] [G loss: 0.572] [G acc: 0.062]\n",
      "4157 [D loss: 0.548(R 0.424, F0.672)] [D acc: 0.750(R 0.781, F 0.719)] [G loss: 0.548] [G acc: 0.078]\n",
      "4158 [D loss: 0.504(R 0.625, F0.383)] [D acc: 0.766(R 0.625, F 0.906)] [G loss: 0.504] [G acc: 0.172]\n",
      "4159 [D loss: 0.437(R 0.475, F0.400)] [D acc: 0.797(R 0.750, F 0.844)] [G loss: 0.437] [G acc: 0.094]\n",
      "4160 [D loss: 0.480(R 0.491, F0.469)] [D acc: 0.781(R 0.750, F 0.812)] [G loss: 0.480] [G acc: 0.188]\n",
      "4161 [D loss: 0.554(R 0.616, F0.491)] [D acc: 0.703(R 0.609, F 0.797)] [G loss: 0.554] [G acc: 0.141]\n",
      "4162 [D loss: 0.433(R 0.423, F0.442)] [D acc: 0.789(R 0.719, F 0.859)] [G loss: 0.433] [G acc: 0.188]\n",
      "4163 [D loss: 0.516(R 0.416, F0.615)] [D acc: 0.703(R 0.766, F 0.641)] [G loss: 0.516] [G acc: 0.125]\n",
      "4164 [D loss: 0.423(R 0.447, F0.399)] [D acc: 0.820(R 0.781, F 0.859)] [G loss: 0.423] [G acc: 0.078]\n",
      "4165 [D loss: 0.606(R 0.544, F0.669)] [D acc: 0.688(R 0.656, F 0.719)] [G loss: 0.606] [G acc: 0.047]\n",
      "4166 [D loss: 0.509(R 0.618, F0.401)] [D acc: 0.781(R 0.656, F 0.906)] [G loss: 0.509] [G acc: 0.047]\n",
      "4167 [D loss: 0.434(R 0.440, F0.429)] [D acc: 0.773(R 0.688, F 0.859)] [G loss: 0.434] [G acc: 0.125]\n",
      "4168 [D loss: 0.521(R 0.493, F0.549)] [D acc: 0.750(R 0.703, F 0.797)] [G loss: 0.521] [G acc: 0.094]\n",
      "4169 [D loss: 0.530(R 0.510, F0.550)] [D acc: 0.734(R 0.688, F 0.781)] [G loss: 0.530] [G acc: 0.141]\n",
      "4170 [D loss: 0.563(R 0.557, F0.569)] [D acc: 0.734(R 0.641, F 0.828)] [G loss: 0.563] [G acc: 0.062]\n",
      "4171 [D loss: 0.519(R 0.637, F0.402)] [D acc: 0.766(R 0.656, F 0.875)] [G loss: 0.519] [G acc: 0.078]\n",
      "4172 [D loss: 0.540(R 0.597, F0.484)] [D acc: 0.742(R 0.688, F 0.797)] [G loss: 0.540] [G acc: 0.125]\n",
      "4173 [D loss: 0.445(R 0.492, F0.398)] [D acc: 0.797(R 0.703, F 0.891)] [G loss: 0.445] [G acc: 0.125]\n",
      "4174 [D loss: 0.463(R 0.481, F0.446)] [D acc: 0.781(R 0.719, F 0.844)] [G loss: 0.463] [G acc: 0.094]\n",
      "4175 [D loss: 0.622(R 0.610, F0.634)] [D acc: 0.680(R 0.656, F 0.703)] [G loss: 0.622] [G acc: 0.078]\n",
      "4176 [D loss: 0.510(R 0.571, F0.449)] [D acc: 0.750(R 0.641, F 0.859)] [G loss: 0.510] [G acc: 0.141]\n",
      "4177 [D loss: 0.480(R 0.455, F0.505)] [D acc: 0.789(R 0.703, F 0.875)] [G loss: 0.480] [G acc: 0.109]\n",
      "4178 [D loss: 0.421(R 0.446, F0.396)] [D acc: 0.852(R 0.781, F 0.922)] [G loss: 0.421] [G acc: 0.125]\n",
      "4179 [D loss: 0.575(R 0.503, F0.648)] [D acc: 0.688(R 0.641, F 0.734)] [G loss: 0.575] [G acc: 0.125]\n",
      "4180 [D loss: 0.531(R 0.546, F0.516)] [D acc: 0.727(R 0.656, F 0.797)] [G loss: 0.531] [G acc: 0.031]\n",
      "4181 [D loss: 0.788(R 0.897, F0.679)] [D acc: 0.617(R 0.500, F 0.734)] [G loss: 0.788] [G acc: 0.078]\n",
      "4182 [D loss: 0.529(R 0.576, F0.483)] [D acc: 0.734(R 0.672, F 0.797)] [G loss: 0.529] [G acc: 0.078]\n",
      "4183 [D loss: 0.457(R 0.496, F0.418)] [D acc: 0.789(R 0.703, F 0.875)] [G loss: 0.457] [G acc: 0.109]\n",
      "4184 [D loss: 0.504(R 0.373, F0.635)] [D acc: 0.766(R 0.781, F 0.750)] [G loss: 0.504] [G acc: 0.062]\n",
      "4185 [D loss: 0.559(R 0.645, F0.473)] [D acc: 0.781(R 0.688, F 0.875)] [G loss: 0.559] [G acc: 0.078]\n",
      "4186 [D loss: 0.524(R 0.587, F0.461)] [D acc: 0.695(R 0.609, F 0.781)] [G loss: 0.524] [G acc: 0.047]\n",
      "4187 [D loss: 0.536(R 0.465, F0.607)] [D acc: 0.719(R 0.734, F 0.703)] [G loss: 0.536] [G acc: 0.078]\n",
      "4188 [D loss: 0.581(R 0.637, F0.524)] [D acc: 0.711(R 0.609, F 0.812)] [G loss: 0.581] [G acc: 0.094]\n",
      "4189 [D loss: 0.461(R 0.523, F0.398)] [D acc: 0.820(R 0.734, F 0.906)] [G loss: 0.461] [G acc: 0.141]\n",
      "4190 [D loss: 0.404(R 0.328, F0.479)] [D acc: 0.820(R 0.812, F 0.828)] [G loss: 0.404] [G acc: 0.109]\n",
      "4191 [D loss: 0.466(R 0.551, F0.381)] [D acc: 0.742(R 0.609, F 0.875)] [G loss: 0.466] [G acc: 0.109]\n",
      "4192 [D loss: 0.494(R 0.516, F0.471)] [D acc: 0.758(R 0.703, F 0.812)] [G loss: 0.494] [G acc: 0.094]\n",
      "4193 [D loss: 0.508(R 0.583, F0.433)] [D acc: 0.766(R 0.656, F 0.875)] [G loss: 0.508] [G acc: 0.062]\n",
      "4194 [D loss: 0.394(R 0.427, F0.361)] [D acc: 0.805(R 0.734, F 0.875)] [G loss: 0.394] [G acc: 0.062]\n",
      "4195 [D loss: 0.447(R 0.446, F0.447)] [D acc: 0.789(R 0.750, F 0.828)] [G loss: 0.447] [G acc: 0.031]\n",
      "4196 [D loss: 0.515(R 0.433, F0.597)] [D acc: 0.758(R 0.781, F 0.734)] [G loss: 0.515] [G acc: 0.031]\n",
      "4197 [D loss: 0.421(R 0.504, F0.338)] [D acc: 0.805(R 0.719, F 0.891)] [G loss: 0.421] [G acc: 0.078]\n",
      "4198 [D loss: 0.522(R 0.436, F0.608)] [D acc: 0.742(R 0.734, F 0.750)] [G loss: 0.522] [G acc: 0.031]\n",
      "4199 [D loss: 0.567(R 0.697, F0.438)] [D acc: 0.734(R 0.609, F 0.859)] [G loss: 0.567] [G acc: 0.125]\n",
      "INFO:tensorflow:Assets written to: ram://20a3025b-a6f8-4f56-8d06-20b77ed1af3b/assets\n",
      "INFO:tensorflow:Assets written to: ram://5447fb5d-3f1c-43e2-b004-ed136f6dab59/assets\n",
      "INFO:tensorflow:Assets written to: ram://2f40344f-70d2-45ee-8abc-cfb7c9768678/assets\n",
      "4200 [D loss: 0.591(R 0.582, F0.600)] [D acc: 0.734(R 0.703, F 0.766)] [G loss: 0.591] [G acc: 0.016]\n",
      "4201 [D loss: 0.521(R 0.542, F0.499)] [D acc: 0.734(R 0.719, F 0.750)] [G loss: 0.521] [G acc: 0.188]\n",
      "4202 [D loss: 0.601(R 0.654, F0.548)] [D acc: 0.656(R 0.547, F 0.766)] [G loss: 0.601] [G acc: 0.125]\n",
      "4203 [D loss: 0.572(R 0.607, F0.536)] [D acc: 0.680(R 0.625, F 0.734)] [G loss: 0.572] [G acc: 0.109]\n",
      "4204 [D loss: 0.555(R 0.574, F0.535)] [D acc: 0.711(R 0.672, F 0.750)] [G loss: 0.555] [G acc: 0.109]\n",
      "4205 [D loss: 0.408(R 0.376, F0.439)] [D acc: 0.859(R 0.859, F 0.859)] [G loss: 0.408] [G acc: 0.062]\n",
      "4206 [D loss: 0.407(R 0.351, F0.464)] [D acc: 0.820(R 0.812, F 0.828)] [G loss: 0.407] [G acc: 0.062]\n",
      "4207 [D loss: 0.449(R 0.388, F0.509)] [D acc: 0.812(R 0.844, F 0.781)] [G loss: 0.449] [G acc: 0.109]\n",
      "4208 [D loss: 0.589(R 0.570, F0.608)] [D acc: 0.742(R 0.703, F 0.781)] [G loss: 0.589] [G acc: 0.078]\n",
      "4209 [D loss: 0.494(R 0.659, F0.330)] [D acc: 0.734(R 0.516, F 0.953)] [G loss: 0.494] [G acc: 0.031]\n",
      "4210 [D loss: 0.570(R 0.558, F0.582)] [D acc: 0.711(R 0.625, F 0.797)] [G loss: 0.570] [G acc: 0.219]\n",
      "4211 [D loss: 0.471(R 0.486, F0.455)] [D acc: 0.781(R 0.719, F 0.844)] [G loss: 0.471] [G acc: 0.156]\n",
      "4212 [D loss: 0.568(R 0.567, F0.568)] [D acc: 0.680(R 0.609, F 0.750)] [G loss: 0.568] [G acc: 0.078]\n",
      "4213 [D loss: 0.527(R 0.493, F0.562)] [D acc: 0.695(R 0.703, F 0.688)] [G loss: 0.527] [G acc: 0.078]\n",
      "4214 [D loss: 0.514(R 0.594, F0.434)] [D acc: 0.742(R 0.688, F 0.797)] [G loss: 0.514] [G acc: 0.141]\n",
      "4215 [D loss: 0.562(R 0.570, F0.555)] [D acc: 0.688(R 0.656, F 0.719)] [G loss: 0.562] [G acc: 0.094]\n",
      "4216 [D loss: 0.521(R 0.537, F0.505)] [D acc: 0.742(R 0.688, F 0.797)] [G loss: 0.521] [G acc: 0.062]\n",
      "4217 [D loss: 0.465(R 0.503, F0.428)] [D acc: 0.797(R 0.703, F 0.891)] [G loss: 0.465] [G acc: 0.078]\n",
      "4218 [D loss: 0.542(R 0.513, F0.572)] [D acc: 0.734(R 0.688, F 0.781)] [G loss: 0.542] [G acc: 0.094]\n",
      "4219 [D loss: 0.512(R 0.466, F0.558)] [D acc: 0.734(R 0.703, F 0.766)] [G loss: 0.512] [G acc: 0.078]\n",
      "4220 [D loss: 0.570(R 0.595, F0.546)] [D acc: 0.742(R 0.641, F 0.844)] [G loss: 0.570] [G acc: 0.172]\n",
      "4221 [D loss: 0.542(R 0.526, F0.557)] [D acc: 0.766(R 0.750, F 0.781)] [G loss: 0.542] [G acc: 0.156]\n",
      "4222 [D loss: 0.571(R 0.650, F0.493)] [D acc: 0.734(R 0.609, F 0.859)] [G loss: 0.571] [G acc: 0.078]\n",
      "4223 [D loss: 0.536(R 0.546, F0.525)] [D acc: 0.742(R 0.641, F 0.844)] [G loss: 0.536] [G acc: 0.125]\n",
      "4224 [D loss: 0.461(R 0.494, F0.428)] [D acc: 0.805(R 0.719, F 0.891)] [G loss: 0.461] [G acc: 0.078]\n",
      "4225 [D loss: 0.472(R 0.469, F0.474)] [D acc: 0.719(R 0.688, F 0.750)] [G loss: 0.472] [G acc: 0.203]\n",
      "4226 [D loss: 0.438(R 0.440, F0.436)] [D acc: 0.750(R 0.703, F 0.797)] [G loss: 0.438] [G acc: 0.094]\n",
      "4227 [D loss: 0.452(R 0.401, F0.504)] [D acc: 0.820(R 0.828, F 0.812)] [G loss: 0.452] [G acc: 0.047]\n",
      "4228 [D loss: 0.518(R 0.519, F0.517)] [D acc: 0.781(R 0.703, F 0.859)] [G loss: 0.518] [G acc: 0.047]\n",
      "4229 [D loss: 0.529(R 0.594, F0.464)] [D acc: 0.766(R 0.656, F 0.875)] [G loss: 0.529] [G acc: 0.125]\n",
      "4230 [D loss: 0.481(R 0.478, F0.483)] [D acc: 0.789(R 0.719, F 0.859)] [G loss: 0.481] [G acc: 0.109]\n",
      "4231 [D loss: 0.529(R 0.432, F0.625)] [D acc: 0.734(R 0.766, F 0.703)] [G loss: 0.529] [G acc: 0.125]\n",
      "4232 [D loss: 0.563(R 0.667, F0.460)] [D acc: 0.703(R 0.594, F 0.812)] [G loss: 0.563] [G acc: 0.094]\n",
      "4233 [D loss: 0.511(R 0.585, F0.436)] [D acc: 0.727(R 0.641, F 0.812)] [G loss: 0.511] [G acc: 0.109]\n",
      "4234 [D loss: 0.503(R 0.600, F0.406)] [D acc: 0.758(R 0.672, F 0.844)] [G loss: 0.503] [G acc: 0.094]\n",
      "4235 [D loss: 0.488(R 0.460, F0.516)] [D acc: 0.766(R 0.750, F 0.781)] [G loss: 0.488] [G acc: 0.047]\n",
      "4236 [D loss: 0.480(R 0.466, F0.495)] [D acc: 0.750(R 0.688, F 0.812)] [G loss: 0.480] [G acc: 0.094]\n",
      "4237 [D loss: 0.541(R 0.555, F0.528)] [D acc: 0.734(R 0.656, F 0.812)] [G loss: 0.541] [G acc: 0.094]\n",
      "4238 [D loss: 0.445(R 0.318, F0.572)] [D acc: 0.805(R 0.828, F 0.781)] [G loss: 0.445] [G acc: 0.031]\n",
      "4239 [D loss: 0.524(R 0.622, F0.426)] [D acc: 0.758(R 0.625, F 0.891)] [G loss: 0.524] [G acc: 0.094]\n",
      "4240 [D loss: 0.512(R 0.589, F0.436)] [D acc: 0.734(R 0.641, F 0.828)] [G loss: 0.512] [G acc: 0.125]\n",
      "4241 [D loss: 0.503(R 0.477, F0.529)] [D acc: 0.758(R 0.750, F 0.766)] [G loss: 0.503] [G acc: 0.031]\n",
      "4242 [D loss: 0.514(R 0.637, F0.390)] [D acc: 0.758(R 0.609, F 0.906)] [G loss: 0.514] [G acc: 0.031]\n",
      "4243 [D loss: 0.508(R 0.430, F0.586)] [D acc: 0.766(R 0.719, F 0.812)] [G loss: 0.508] [G acc: 0.078]\n",
      "4244 [D loss: 0.571(R 0.547, F0.594)] [D acc: 0.758(R 0.688, F 0.828)] [G loss: 0.571] [G acc: 0.078]\n",
      "4245 [D loss: 0.509(R 0.538, F0.481)] [D acc: 0.789(R 0.703, F 0.875)] [G loss: 0.509] [G acc: 0.078]\n",
      "4246 [D loss: 0.633(R 0.470, F0.796)] [D acc: 0.766(R 0.719, F 0.812)] [G loss: 0.633] [G acc: 0.078]\n",
      "4247 [D loss: 0.461(R 0.453, F0.468)] [D acc: 0.789(R 0.734, F 0.844)] [G loss: 0.461] [G acc: 0.125]\n",
      "4248 [D loss: 0.577(R 0.637, F0.517)] [D acc: 0.680(R 0.578, F 0.781)] [G loss: 0.577] [G acc: 0.062]\n",
      "4249 [D loss: 0.454(R 0.452, F0.455)] [D acc: 0.742(R 0.703, F 0.781)] [G loss: 0.454] [G acc: 0.141]\n",
      "4250 [D loss: 0.546(R 0.549, F0.542)] [D acc: 0.766(R 0.703, F 0.828)] [G loss: 0.546] [G acc: 0.062]\n",
      "4251 [D loss: 0.537(R 0.585, F0.489)] [D acc: 0.719(R 0.625, F 0.812)] [G loss: 0.537] [G acc: 0.094]\n",
      "4252 [D loss: 0.526(R 0.516, F0.535)] [D acc: 0.703(R 0.672, F 0.734)] [G loss: 0.526] [G acc: 0.094]\n",
      "4253 [D loss: 0.614(R 0.569, F0.659)] [D acc: 0.703(R 0.734, F 0.672)] [G loss: 0.614] [G acc: 0.109]\n",
      "4254 [D loss: 0.621(R 0.646, F0.595)] [D acc: 0.672(R 0.594, F 0.750)] [G loss: 0.621] [G acc: 0.078]\n",
      "4255 [D loss: 0.482(R 0.579, F0.384)] [D acc: 0.750(R 0.609, F 0.891)] [G loss: 0.482] [G acc: 0.078]\n",
      "4256 [D loss: 0.503(R 0.566, F0.439)] [D acc: 0.727(R 0.609, F 0.844)] [G loss: 0.503] [G acc: 0.141]\n",
      "4257 [D loss: 0.469(R 0.438, F0.499)] [D acc: 0.797(R 0.781, F 0.812)] [G loss: 0.469] [G acc: 0.094]\n",
      "4258 [D loss: 0.448(R 0.479, F0.417)] [D acc: 0.781(R 0.719, F 0.844)] [G loss: 0.448] [G acc: 0.172]\n",
      "4259 [D loss: 0.486(R 0.421, F0.551)] [D acc: 0.695(R 0.734, F 0.656)] [G loss: 0.486] [G acc: 0.109]\n",
      "4260 [D loss: 0.541(R 0.608, F0.473)] [D acc: 0.750(R 0.672, F 0.828)] [G loss: 0.541] [G acc: 0.078]\n",
      "4261 [D loss: 0.542(R 0.595, F0.489)] [D acc: 0.766(R 0.703, F 0.828)] [G loss: 0.542] [G acc: 0.172]\n",
      "4262 [D loss: 0.508(R 0.538, F0.478)] [D acc: 0.719(R 0.609, F 0.828)] [G loss: 0.508] [G acc: 0.078]\n",
      "4263 [D loss: 0.572(R 0.643, F0.500)] [D acc: 0.656(R 0.609, F 0.703)] [G loss: 0.572] [G acc: 0.047]\n",
      "4264 [D loss: 0.611(R 0.446, F0.777)] [D acc: 0.758(R 0.719, F 0.797)] [G loss: 0.611] [G acc: 0.031]\n",
      "4265 [D loss: 0.570(R 0.724, F0.416)] [D acc: 0.727(R 0.609, F 0.844)] [G loss: 0.570] [G acc: 0.031]\n",
      "4266 [D loss: 0.655(R 0.706, F0.604)] [D acc: 0.609(R 0.516, F 0.703)] [G loss: 0.655] [G acc: 0.156]\n",
      "4267 [D loss: 0.475(R 0.533, F0.416)] [D acc: 0.797(R 0.703, F 0.891)] [G loss: 0.475] [G acc: 0.047]\n",
      "4268 [D loss: 0.483(R 0.489, F0.478)] [D acc: 0.789(R 0.750, F 0.828)] [G loss: 0.483] [G acc: 0.094]\n",
      "4269 [D loss: 0.514(R 0.529, F0.499)] [D acc: 0.773(R 0.719, F 0.828)] [G loss: 0.514] [G acc: 0.078]\n",
      "4270 [D loss: 0.488(R 0.511, F0.465)] [D acc: 0.758(R 0.719, F 0.797)] [G loss: 0.488] [G acc: 0.141]\n",
      "4271 [D loss: 0.504(R 0.458, F0.551)] [D acc: 0.750(R 0.719, F 0.781)] [G loss: 0.504] [G acc: 0.109]\n",
      "4272 [D loss: 0.621(R 0.773, F0.468)] [D acc: 0.656(R 0.484, F 0.828)] [G loss: 0.621] [G acc: 0.156]\n",
      "4273 [D loss: 0.498(R 0.406, F0.589)] [D acc: 0.773(R 0.750, F 0.797)] [G loss: 0.498] [G acc: 0.094]\n",
      "4274 [D loss: 0.493(R 0.595, F0.391)] [D acc: 0.742(R 0.578, F 0.906)] [G loss: 0.493] [G acc: 0.109]\n",
      "4275 [D loss: 0.467(R 0.446, F0.488)] [D acc: 0.789(R 0.703, F 0.875)] [G loss: 0.467] [G acc: 0.047]\n",
      "4276 [D loss: 0.532(R 0.613, F0.451)] [D acc: 0.742(R 0.672, F 0.812)] [G loss: 0.532] [G acc: 0.125]\n",
      "4277 [D loss: 0.531(R 0.630, F0.431)] [D acc: 0.672(R 0.547, F 0.797)] [G loss: 0.531] [G acc: 0.078]\n",
      "4278 [D loss: 0.527(R 0.493, F0.560)] [D acc: 0.727(R 0.719, F 0.734)] [G loss: 0.527] [G acc: 0.078]\n",
      "4279 [D loss: 0.476(R 0.535, F0.418)] [D acc: 0.742(R 0.641, F 0.844)] [G loss: 0.476] [G acc: 0.188]\n",
      "4280 [D loss: 0.491(R 0.538, F0.444)] [D acc: 0.734(R 0.672, F 0.797)] [G loss: 0.491] [G acc: 0.047]\n",
      "4281 [D loss: 0.468(R 0.499, F0.436)] [D acc: 0.766(R 0.672, F 0.859)] [G loss: 0.468] [G acc: 0.078]\n",
      "4282 [D loss: 0.473(R 0.420, F0.525)] [D acc: 0.789(R 0.734, F 0.844)] [G loss: 0.473] [G acc: 0.031]\n",
      "4283 [D loss: 0.523(R 0.514, F0.532)] [D acc: 0.734(R 0.656, F 0.812)] [G loss: 0.523] [G acc: 0.031]\n",
      "4284 [D loss: 0.516(R 0.642, F0.391)] [D acc: 0.750(R 0.578, F 0.922)] [G loss: 0.516] [G acc: 0.109]\n",
      "4285 [D loss: 0.559(R 0.522, F0.596)] [D acc: 0.711(R 0.672, F 0.750)] [G loss: 0.559] [G acc: 0.094]\n",
      "4286 [D loss: 0.457(R 0.495, F0.419)] [D acc: 0.812(R 0.750, F 0.875)] [G loss: 0.457] [G acc: 0.047]\n",
      "4287 [D loss: 0.604(R 0.604, F0.603)] [D acc: 0.711(R 0.656, F 0.766)] [G loss: 0.604] [G acc: 0.125]\n",
      "4288 [D loss: 0.529(R 0.554, F0.503)] [D acc: 0.758(R 0.688, F 0.828)] [G loss: 0.529] [G acc: 0.062]\n",
      "4289 [D loss: 0.588(R 0.705, F0.471)] [D acc: 0.688(R 0.562, F 0.812)] [G loss: 0.588] [G acc: 0.094]\n",
      "4290 [D loss: 0.492(R 0.485, F0.499)] [D acc: 0.742(R 0.719, F 0.766)] [G loss: 0.492] [G acc: 0.062]\n",
      "4291 [D loss: 0.428(R 0.471, F0.385)] [D acc: 0.812(R 0.734, F 0.891)] [G loss: 0.428] [G acc: 0.141]\n",
      "4292 [D loss: 0.579(R 0.588, F0.570)] [D acc: 0.719(R 0.672, F 0.766)] [G loss: 0.579] [G acc: 0.078]\n",
      "4293 [D loss: 0.495(R 0.520, F0.471)] [D acc: 0.727(R 0.656, F 0.797)] [G loss: 0.495] [G acc: 0.172]\n",
      "4294 [D loss: 0.476(R 0.451, F0.501)] [D acc: 0.766(R 0.688, F 0.844)] [G loss: 0.476] [G acc: 0.031]\n",
      "4295 [D loss: 0.595(R 0.697, F0.493)] [D acc: 0.688(R 0.547, F 0.828)] [G loss: 0.595] [G acc: 0.094]\n",
      "4296 [D loss: 0.440(R 0.465, F0.415)] [D acc: 0.828(R 0.781, F 0.875)] [G loss: 0.440] [G acc: 0.109]\n",
      "4297 [D loss: 0.486(R 0.410, F0.563)] [D acc: 0.742(R 0.703, F 0.781)] [G loss: 0.486] [G acc: 0.156]\n",
      "4298 [D loss: 0.534(R 0.653, F0.414)] [D acc: 0.727(R 0.578, F 0.875)] [G loss: 0.534] [G acc: 0.047]\n",
      "4299 [D loss: 0.514(R 0.506, F0.522)] [D acc: 0.750(R 0.719, F 0.781)] [G loss: 0.514] [G acc: 0.125]\n",
      "INFO:tensorflow:Assets written to: ram://56277f90-8dd3-48a5-9667-972689899f14/assets\n",
      "INFO:tensorflow:Assets written to: ram://37edcc65-b841-4bbc-8489-adce7c1fd668/assets\n",
      "INFO:tensorflow:Assets written to: ram://92ac59a0-9d40-4d12-972c-5a986dfba31f/assets\n",
      "4300 [D loss: 0.622(R 0.728, F0.515)] [D acc: 0.641(R 0.500, F 0.781)] [G loss: 0.622] [G acc: 0.094]\n",
      "4301 [D loss: 0.561(R 0.558, F0.563)] [D acc: 0.727(R 0.703, F 0.750)] [G loss: 0.561] [G acc: 0.047]\n",
      "4302 [D loss: 0.541(R 0.515, F0.567)] [D acc: 0.719(R 0.641, F 0.797)] [G loss: 0.541] [G acc: 0.078]\n",
      "4303 [D loss: 0.533(R 0.535, F0.532)] [D acc: 0.734(R 0.703, F 0.766)] [G loss: 0.533] [G acc: 0.172]\n",
      "4304 [D loss: 0.534(R 0.534, F0.534)] [D acc: 0.773(R 0.719, F 0.828)] [G loss: 0.534] [G acc: 0.078]\n",
      "4305 [D loss: 0.558(R 0.646, F0.469)] [D acc: 0.695(R 0.609, F 0.781)] [G loss: 0.558] [G acc: 0.109]\n",
      "4306 [D loss: 0.494(R 0.463, F0.525)] [D acc: 0.766(R 0.734, F 0.797)] [G loss: 0.494] [G acc: 0.047]\n",
      "4307 [D loss: 0.533(R 0.605, F0.461)] [D acc: 0.711(R 0.609, F 0.812)] [G loss: 0.533] [G acc: 0.047]\n",
      "4308 [D loss: 0.477(R 0.507, F0.448)] [D acc: 0.797(R 0.734, F 0.859)] [G loss: 0.477] [G acc: 0.094]\n",
      "4309 [D loss: 0.570(R 0.497, F0.643)] [D acc: 0.688(R 0.719, F 0.656)] [G loss: 0.570] [G acc: 0.062]\n",
      "4310 [D loss: 0.463(R 0.473, F0.452)] [D acc: 0.820(R 0.766, F 0.875)] [G loss: 0.463] [G acc: 0.062]\n",
      "4311 [D loss: 0.479(R 0.473, F0.485)] [D acc: 0.734(R 0.672, F 0.797)] [G loss: 0.479] [G acc: 0.047]\n",
      "4312 [D loss: 0.460(R 0.498, F0.423)] [D acc: 0.766(R 0.672, F 0.859)] [G loss: 0.460] [G acc: 0.062]\n",
      "4313 [D loss: 0.531(R 0.489, F0.572)] [D acc: 0.711(R 0.703, F 0.719)] [G loss: 0.531] [G acc: 0.109]\n",
      "4314 [D loss: 0.526(R 0.540, F0.513)] [D acc: 0.750(R 0.625, F 0.875)] [G loss: 0.526] [G acc: 0.109]\n",
      "4315 [D loss: 0.510(R 0.457, F0.563)] [D acc: 0.734(R 0.766, F 0.703)] [G loss: 0.510] [G acc: 0.062]\n",
      "4316 [D loss: 0.562(R 0.566, F0.558)] [D acc: 0.703(R 0.641, F 0.766)] [G loss: 0.562] [G acc: 0.062]\n",
      "4317 [D loss: 0.538(R 0.623, F0.453)] [D acc: 0.734(R 0.625, F 0.844)] [G loss: 0.538] [G acc: 0.031]\n",
      "4318 [D loss: 0.525(R 0.518, F0.532)] [D acc: 0.773(R 0.703, F 0.844)] [G loss: 0.525] [G acc: 0.125]\n",
      "4319 [D loss: 0.681(R 0.580, F0.782)] [D acc: 0.648(R 0.656, F 0.641)] [G loss: 0.681] [G acc: 0.062]\n",
      "4320 [D loss: 0.594(R 0.745, F0.444)] [D acc: 0.680(R 0.500, F 0.859)] [G loss: 0.594] [G acc: 0.141]\n",
      "4321 [D loss: 0.478(R 0.419, F0.536)] [D acc: 0.789(R 0.766, F 0.812)] [G loss: 0.478] [G acc: 0.047]\n",
      "4322 [D loss: 0.421(R 0.441, F0.401)] [D acc: 0.828(R 0.797, F 0.859)] [G loss: 0.421] [G acc: 0.141]\n",
      "4323 [D loss: 0.492(R 0.533, F0.452)] [D acc: 0.758(R 0.656, F 0.859)] [G loss: 0.492] [G acc: 0.078]\n",
      "4324 [D loss: 0.487(R 0.496, F0.477)] [D acc: 0.750(R 0.688, F 0.812)] [G loss: 0.487] [G acc: 0.062]\n",
      "4325 [D loss: 0.536(R 0.620, F0.451)] [D acc: 0.742(R 0.625, F 0.859)] [G loss: 0.536] [G acc: 0.141]\n",
      "4326 [D loss: 0.554(R 0.442, F0.666)] [D acc: 0.719(R 0.734, F 0.703)] [G loss: 0.554] [G acc: 0.094]\n",
      "4327 [D loss: 0.497(R 0.533, F0.462)] [D acc: 0.773(R 0.656, F 0.891)] [G loss: 0.497] [G acc: 0.094]\n",
      "4328 [D loss: 0.465(R 0.430, F0.501)] [D acc: 0.781(R 0.734, F 0.828)] [G loss: 0.465] [G acc: 0.031]\n",
      "4329 [D loss: 0.456(R 0.390, F0.522)] [D acc: 0.812(R 0.797, F 0.828)] [G loss: 0.456] [G acc: 0.078]\n",
      "4330 [D loss: 0.624(R 0.667, F0.580)] [D acc: 0.672(R 0.562, F 0.781)] [G loss: 0.624] [G acc: 0.094]\n",
      "4331 [D loss: 0.470(R 0.493, F0.446)] [D acc: 0.812(R 0.719, F 0.906)] [G loss: 0.470] [G acc: 0.062]\n",
      "4332 [D loss: 0.488(R 0.478, F0.498)] [D acc: 0.812(R 0.797, F 0.828)] [G loss: 0.488] [G acc: 0.031]\n",
      "4333 [D loss: 0.512(R 0.557, F0.467)] [D acc: 0.773(R 0.672, F 0.875)] [G loss: 0.512] [G acc: 0.062]\n",
      "4334 [D loss: 0.528(R 0.539, F0.517)] [D acc: 0.750(R 0.703, F 0.797)] [G loss: 0.528] [G acc: 0.078]\n",
      "4335 [D loss: 0.454(R 0.450, F0.459)] [D acc: 0.758(R 0.734, F 0.781)] [G loss: 0.454] [G acc: 0.141]\n",
      "4336 [D loss: 0.588(R 0.585, F0.592)] [D acc: 0.680(R 0.594, F 0.766)] [G loss: 0.588] [G acc: 0.062]\n",
      "4337 [D loss: 0.549(R 0.573, F0.524)] [D acc: 0.711(R 0.609, F 0.812)] [G loss: 0.549] [G acc: 0.125]\n",
      "4338 [D loss: 0.492(R 0.526, F0.459)] [D acc: 0.758(R 0.688, F 0.828)] [G loss: 0.492] [G acc: 0.109]\n",
      "4339 [D loss: 0.537(R 0.568, F0.506)] [D acc: 0.758(R 0.703, F 0.812)] [G loss: 0.537] [G acc: 0.047]\n",
      "4340 [D loss: 0.497(R 0.475, F0.519)] [D acc: 0.797(R 0.734, F 0.859)] [G loss: 0.497] [G acc: 0.125]\n",
      "4341 [D loss: 0.486(R 0.508, F0.464)] [D acc: 0.742(R 0.688, F 0.797)] [G loss: 0.486] [G acc: 0.094]\n",
      "4342 [D loss: 0.503(R 0.488, F0.517)] [D acc: 0.742(R 0.719, F 0.766)] [G loss: 0.503] [G acc: 0.078]\n",
      "4343 [D loss: 0.633(R 0.616, F0.650)] [D acc: 0.688(R 0.625, F 0.750)] [G loss: 0.633] [G acc: 0.031]\n",
      "4344 [D loss: 0.501(R 0.609, F0.394)] [D acc: 0.797(R 0.672, F 0.922)] [G loss: 0.501] [G acc: 0.094]\n",
      "4345 [D loss: 0.588(R 0.621, F0.555)] [D acc: 0.688(R 0.578, F 0.797)] [G loss: 0.588] [G acc: 0.047]\n",
      "4346 [D loss: 0.491(R 0.611, F0.371)] [D acc: 0.719(R 0.562, F 0.875)] [G loss: 0.491] [G acc: 0.078]\n",
      "4347 [D loss: 0.477(R 0.356, F0.597)] [D acc: 0.773(R 0.797, F 0.750)] [G loss: 0.477] [G acc: 0.062]\n",
      "4348 [D loss: 0.501(R 0.549, F0.452)] [D acc: 0.750(R 0.656, F 0.844)] [G loss: 0.501] [G acc: 0.062]\n",
      "4349 [D loss: 0.506(R 0.503, F0.510)] [D acc: 0.742(R 0.641, F 0.844)] [G loss: 0.506] [G acc: 0.062]\n",
      "4350 [D loss: 0.571(R 0.546, F0.597)] [D acc: 0.727(R 0.703, F 0.750)] [G loss: 0.571] [G acc: 0.047]\n",
      "4351 [D loss: 0.467(R 0.529, F0.406)] [D acc: 0.734(R 0.641, F 0.828)] [G loss: 0.467] [G acc: 0.094]\n",
      "4352 [D loss: 0.481(R 0.505, F0.456)] [D acc: 0.758(R 0.688, F 0.828)] [G loss: 0.481] [G acc: 0.062]\n",
      "4353 [D loss: 0.561(R 0.568, F0.554)] [D acc: 0.711(R 0.641, F 0.781)] [G loss: 0.561] [G acc: 0.125]\n",
      "4354 [D loss: 0.503(R 0.538, F0.469)] [D acc: 0.711(R 0.641, F 0.781)] [G loss: 0.503] [G acc: 0.125]\n",
      "4355 [D loss: 0.589(R 0.632, F0.547)] [D acc: 0.695(R 0.609, F 0.781)] [G loss: 0.589] [G acc: 0.156]\n",
      "4356 [D loss: 0.558(R 0.533, F0.583)] [D acc: 0.719(R 0.625, F 0.812)] [G loss: 0.558] [G acc: 0.047]\n",
      "4357 [D loss: 0.467(R 0.452, F0.482)] [D acc: 0.766(R 0.672, F 0.859)] [G loss: 0.467] [G acc: 0.141]\n",
      "4358 [D loss: 0.497(R 0.473, F0.520)] [D acc: 0.750(R 0.703, F 0.797)] [G loss: 0.497] [G acc: 0.188]\n",
      "4359 [D loss: 0.517(R 0.479, F0.556)] [D acc: 0.734(R 0.703, F 0.766)] [G loss: 0.517] [G acc: 0.094]\n",
      "4360 [D loss: 0.426(R 0.439, F0.414)] [D acc: 0.797(R 0.750, F 0.844)] [G loss: 0.426] [G acc: 0.109]\n",
      "4361 [D loss: 0.523(R 0.577, F0.469)] [D acc: 0.773(R 0.656, F 0.891)] [G loss: 0.523] [G acc: 0.047]\n",
      "4362 [D loss: 0.524(R 0.616, F0.433)] [D acc: 0.734(R 0.609, F 0.859)] [G loss: 0.524] [G acc: 0.062]\n",
      "4363 [D loss: 0.475(R 0.487, F0.462)] [D acc: 0.781(R 0.703, F 0.859)] [G loss: 0.475] [G acc: 0.078]\n",
      "4364 [D loss: 0.511(R 0.584, F0.438)] [D acc: 0.742(R 0.641, F 0.844)] [G loss: 0.511] [G acc: 0.031]\n",
      "4365 [D loss: 0.500(R 0.364, F0.635)] [D acc: 0.805(R 0.797, F 0.812)] [G loss: 0.500] [G acc: 0.031]\n",
      "4366 [D loss: 0.688(R 0.715, F0.661)] [D acc: 0.641(R 0.562, F 0.719)] [G loss: 0.688] [G acc: 0.047]\n",
      "4367 [D loss: 0.520(R 0.680, F0.360)] [D acc: 0.773(R 0.625, F 0.922)] [G loss: 0.520] [G acc: 0.062]\n",
      "4368 [D loss: 0.421(R 0.419, F0.423)] [D acc: 0.805(R 0.750, F 0.859)] [G loss: 0.421] [G acc: 0.078]\n",
      "4369 [D loss: 0.499(R 0.513, F0.485)] [D acc: 0.758(R 0.688, F 0.828)] [G loss: 0.499] [G acc: 0.125]\n",
      "4370 [D loss: 0.471(R 0.403, F0.539)] [D acc: 0.742(R 0.781, F 0.703)] [G loss: 0.471] [G acc: 0.078]\n",
      "4371 [D loss: 0.493(R 0.555, F0.431)] [D acc: 0.742(R 0.656, F 0.828)] [G loss: 0.493] [G acc: 0.141]\n",
      "4372 [D loss: 0.481(R 0.350, F0.611)] [D acc: 0.820(R 0.828, F 0.812)] [G loss: 0.481] [G acc: 0.062]\n",
      "4373 [D loss: 0.592(R 0.755, F0.429)] [D acc: 0.719(R 0.578, F 0.859)] [G loss: 0.592] [G acc: 0.094]\n",
      "4374 [D loss: 0.555(R 0.616, F0.495)] [D acc: 0.703(R 0.609, F 0.797)] [G loss: 0.555] [G acc: 0.031]\n",
      "4375 [D loss: 0.477(R 0.561, F0.392)] [D acc: 0.805(R 0.703, F 0.906)] [G loss: 0.477] [G acc: 0.047]\n",
      "4376 [D loss: 0.570(R 0.674, F0.465)] [D acc: 0.688(R 0.562, F 0.812)] [G loss: 0.570] [G acc: 0.047]\n",
      "4377 [D loss: 0.499(R 0.578, F0.421)] [D acc: 0.750(R 0.625, F 0.875)] [G loss: 0.499] [G acc: 0.078]\n",
      "4378 [D loss: 0.649(R 0.667, F0.630)] [D acc: 0.680(R 0.562, F 0.797)] [G loss: 0.649] [G acc: 0.109]\n",
      "4379 [D loss: 0.530(R 0.490, F0.569)] [D acc: 0.750(R 0.688, F 0.812)] [G loss: 0.530] [G acc: 0.109]\n",
      "4380 [D loss: 0.640(R 0.828, F0.452)] [D acc: 0.656(R 0.469, F 0.844)] [G loss: 0.640] [G acc: 0.047]\n",
      "4381 [D loss: 0.527(R 0.525, F0.530)] [D acc: 0.727(R 0.672, F 0.781)] [G loss: 0.527] [G acc: 0.094]\n",
      "4382 [D loss: 0.508(R 0.439, F0.577)] [D acc: 0.773(R 0.688, F 0.859)] [G loss: 0.508] [G acc: 0.062]\n",
      "4383 [D loss: 0.535(R 0.612, F0.458)] [D acc: 0.727(R 0.641, F 0.812)] [G loss: 0.535] [G acc: 0.094]\n",
      "4384 [D loss: 0.527(R 0.446, F0.608)] [D acc: 0.773(R 0.766, F 0.781)] [G loss: 0.527] [G acc: 0.047]\n",
      "4385 [D loss: 0.515(R 0.511, F0.518)] [D acc: 0.750(R 0.703, F 0.797)] [G loss: 0.515] [G acc: 0.156]\n",
      "4386 [D loss: 0.515(R 0.590, F0.439)] [D acc: 0.727(R 0.641, F 0.812)] [G loss: 0.515] [G acc: 0.141]\n",
      "4387 [D loss: 0.509(R 0.569, F0.449)] [D acc: 0.766(R 0.688, F 0.844)] [G loss: 0.509] [G acc: 0.062]\n",
      "4388 [D loss: 0.513(R 0.354, F0.672)] [D acc: 0.766(R 0.750, F 0.781)] [G loss: 0.513] [G acc: 0.109]\n",
      "4389 [D loss: 0.558(R 0.649, F0.467)] [D acc: 0.719(R 0.594, F 0.844)] [G loss: 0.558] [G acc: 0.094]\n",
      "4390 [D loss: 0.469(R 0.560, F0.379)] [D acc: 0.773(R 0.672, F 0.875)] [G loss: 0.469] [G acc: 0.109]\n",
      "4391 [D loss: 0.549(R 0.461, F0.637)] [D acc: 0.766(R 0.750, F 0.781)] [G loss: 0.549] [G acc: 0.141]\n",
      "4392 [D loss: 0.544(R 0.487, F0.602)] [D acc: 0.758(R 0.734, F 0.781)] [G loss: 0.544] [G acc: 0.078]\n",
      "4393 [D loss: 0.562(R 0.699, F0.425)] [D acc: 0.633(R 0.469, F 0.797)] [G loss: 0.562] [G acc: 0.172]\n",
      "4394 [D loss: 0.469(R 0.460, F0.478)] [D acc: 0.828(R 0.750, F 0.906)] [G loss: 0.469] [G acc: 0.094]\n",
      "4395 [D loss: 0.458(R 0.405, F0.512)] [D acc: 0.781(R 0.766, F 0.797)] [G loss: 0.458] [G acc: 0.016]\n",
      "4396 [D loss: 0.640(R 0.515, F0.766)] [D acc: 0.719(R 0.688, F 0.750)] [G loss: 0.640] [G acc: 0.062]\n",
      "4397 [D loss: 0.544(R 0.614, F0.473)] [D acc: 0.742(R 0.641, F 0.844)] [G loss: 0.544] [G acc: 0.047]\n",
      "4398 [D loss: 0.572(R 0.702, F0.443)] [D acc: 0.727(R 0.562, F 0.891)] [G loss: 0.572] [G acc: 0.062]\n",
      "4399 [D loss: 0.534(R 0.550, F0.519)] [D acc: 0.742(R 0.672, F 0.812)] [G loss: 0.534] [G acc: 0.016]\n",
      "INFO:tensorflow:Assets written to: ram://5372c29c-2c6e-4cbf-976e-0aa2e34c277d/assets\n",
      "INFO:tensorflow:Assets written to: ram://c2debe38-7e1b-4183-9fae-98d882b38e5a/assets\n",
      "INFO:tensorflow:Assets written to: ram://c623eebe-8232-4aa8-bf97-169ec3f24dd9/assets\n",
      "4400 [D loss: 0.443(R 0.370, F0.516)] [D acc: 0.789(R 0.812, F 0.766)] [G loss: 0.443] [G acc: 0.062]\n",
      "4401 [D loss: 0.469(R 0.541, F0.396)] [D acc: 0.773(R 0.672, F 0.875)] [G loss: 0.469] [G acc: 0.031]\n",
      "4402 [D loss: 0.517(R 0.506, F0.529)] [D acc: 0.750(R 0.734, F 0.766)] [G loss: 0.517] [G acc: 0.125]\n",
      "4403 [D loss: 0.580(R 0.567, F0.593)] [D acc: 0.711(R 0.656, F 0.766)] [G loss: 0.580] [G acc: 0.109]\n",
      "4404 [D loss: 0.516(R 0.566, F0.466)] [D acc: 0.742(R 0.641, F 0.844)] [G loss: 0.516] [G acc: 0.078]\n",
      "4405 [D loss: 0.582(R 0.561, F0.602)] [D acc: 0.734(R 0.672, F 0.797)] [G loss: 0.582] [G acc: 0.125]\n",
      "4406 [D loss: 0.514(R 0.570, F0.457)] [D acc: 0.773(R 0.641, F 0.906)] [G loss: 0.514] [G acc: 0.141]\n",
      "4407 [D loss: 0.602(R 0.543, F0.660)] [D acc: 0.688(R 0.672, F 0.703)] [G loss: 0.602] [G acc: 0.031]\n",
      "4408 [D loss: 0.460(R 0.512, F0.409)] [D acc: 0.797(R 0.703, F 0.891)] [G loss: 0.460] [G acc: 0.047]\n",
      "4409 [D loss: 0.519(R 0.590, F0.448)] [D acc: 0.773(R 0.641, F 0.906)] [G loss: 0.519] [G acc: 0.141]\n",
      "4410 [D loss: 0.442(R 0.441, F0.444)] [D acc: 0.781(R 0.719, F 0.844)] [G loss: 0.442] [G acc: 0.094]\n",
      "4411 [D loss: 0.518(R 0.451, F0.586)] [D acc: 0.742(R 0.734, F 0.750)] [G loss: 0.518] [G acc: 0.109]\n",
      "4412 [D loss: 0.571(R 0.669, F0.473)] [D acc: 0.711(R 0.625, F 0.797)] [G loss: 0.571] [G acc: 0.125]\n",
      "4413 [D loss: 0.535(R 0.515, F0.556)] [D acc: 0.719(R 0.672, F 0.766)] [G loss: 0.535] [G acc: 0.047]\n",
      "4414 [D loss: 0.529(R 0.628, F0.429)] [D acc: 0.758(R 0.672, F 0.844)] [G loss: 0.529] [G acc: 0.109]\n",
      "4415 [D loss: 0.554(R 0.536, F0.572)] [D acc: 0.719(R 0.688, F 0.750)] [G loss: 0.554] [G acc: 0.047]\n",
      "4416 [D loss: 0.475(R 0.473, F0.477)] [D acc: 0.727(R 0.703, F 0.750)] [G loss: 0.475] [G acc: 0.219]\n",
      "4417 [D loss: 0.404(R 0.360, F0.449)] [D acc: 0.797(R 0.797, F 0.797)] [G loss: 0.404] [G acc: 0.141]\n",
      "4418 [D loss: 0.554(R 0.556, F0.552)] [D acc: 0.734(R 0.703, F 0.766)] [G loss: 0.554] [G acc: 0.125]\n",
      "4419 [D loss: 0.557(R 0.673, F0.442)] [D acc: 0.711(R 0.594, F 0.828)] [G loss: 0.557] [G acc: 0.078]\n",
      "4420 [D loss: 0.610(R 0.649, F0.571)] [D acc: 0.664(R 0.594, F 0.734)] [G loss: 0.610] [G acc: 0.062]\n",
      "4421 [D loss: 0.560(R 0.674, F0.445)] [D acc: 0.672(R 0.562, F 0.781)] [G loss: 0.560] [G acc: 0.141]\n",
      "4422 [D loss: 0.534(R 0.498, F0.569)] [D acc: 0.703(R 0.672, F 0.734)] [G loss: 0.534] [G acc: 0.078]\n",
      "4423 [D loss: 0.536(R 0.657, F0.416)] [D acc: 0.789(R 0.656, F 0.922)] [G loss: 0.536] [G acc: 0.078]\n",
      "4424 [D loss: 0.426(R 0.465, F0.386)] [D acc: 0.828(R 0.781, F 0.875)] [G loss: 0.426] [G acc: 0.188]\n",
      "4425 [D loss: 0.526(R 0.504, F0.547)] [D acc: 0.727(R 0.703, F 0.750)] [G loss: 0.526] [G acc: 0.094]\n",
      "4426 [D loss: 0.431(R 0.346, F0.516)] [D acc: 0.820(R 0.828, F 0.812)] [G loss: 0.431] [G acc: 0.047]\n",
      "4427 [D loss: 0.600(R 0.579, F0.620)] [D acc: 0.742(R 0.672, F 0.812)] [G loss: 0.600] [G acc: 0.062]\n",
      "4428 [D loss: 0.666(R 0.752, F0.580)] [D acc: 0.633(R 0.500, F 0.766)] [G loss: 0.666] [G acc: 0.094]\n",
      "4429 [D loss: 0.581(R 0.675, F0.486)] [D acc: 0.695(R 0.578, F 0.812)] [G loss: 0.581] [G acc: 0.062]\n",
      "4430 [D loss: 0.454(R 0.523, F0.386)] [D acc: 0.812(R 0.703, F 0.922)] [G loss: 0.454] [G acc: 0.125]\n",
      "4431 [D loss: 0.448(R 0.378, F0.517)] [D acc: 0.773(R 0.750, F 0.797)] [G loss: 0.448] [G acc: 0.109]\n",
      "4432 [D loss: 0.523(R 0.550, F0.495)] [D acc: 0.742(R 0.656, F 0.828)] [G loss: 0.523] [G acc: 0.062]\n",
      "4433 [D loss: 0.506(R 0.380, F0.632)] [D acc: 0.742(R 0.766, F 0.719)] [G loss: 0.506] [G acc: 0.188]\n",
      "4434 [D loss: 0.494(R 0.537, F0.452)] [D acc: 0.750(R 0.688, F 0.812)] [G loss: 0.494] [G acc: 0.062]\n",
      "4435 [D loss: 0.511(R 0.554, F0.469)] [D acc: 0.773(R 0.672, F 0.875)] [G loss: 0.511] [G acc: 0.109]\n",
      "4436 [D loss: 0.500(R 0.534, F0.466)] [D acc: 0.773(R 0.672, F 0.875)] [G loss: 0.500] [G acc: 0.078]\n",
      "4437 [D loss: 0.558(R 0.574, F0.542)] [D acc: 0.711(R 0.641, F 0.781)] [G loss: 0.558] [G acc: 0.109]\n",
      "4438 [D loss: 0.563(R 0.563, F0.562)] [D acc: 0.688(R 0.672, F 0.703)] [G loss: 0.563] [G acc: 0.062]\n",
      "4439 [D loss: 0.597(R 0.625, F0.568)] [D acc: 0.758(R 0.672, F 0.844)] [G loss: 0.597] [G acc: 0.109]\n",
      "4440 [D loss: 0.578(R 0.608, F0.548)] [D acc: 0.711(R 0.641, F 0.781)] [G loss: 0.578] [G acc: 0.078]\n",
      "4441 [D loss: 0.480(R 0.504, F0.456)] [D acc: 0.742(R 0.641, F 0.844)] [G loss: 0.480] [G acc: 0.078]\n",
      "4442 [D loss: 0.503(R 0.569, F0.436)] [D acc: 0.750(R 0.656, F 0.844)] [G loss: 0.503] [G acc: 0.125]\n",
      "4443 [D loss: 0.463(R 0.425, F0.500)] [D acc: 0.797(R 0.797, F 0.797)] [G loss: 0.463] [G acc: 0.125]\n",
      "4444 [D loss: 0.515(R 0.425, F0.606)] [D acc: 0.781(R 0.766, F 0.797)] [G loss: 0.515] [G acc: 0.094]\n",
      "4445 [D loss: 0.562(R 0.650, F0.474)] [D acc: 0.711(R 0.609, F 0.812)] [G loss: 0.562] [G acc: 0.125]\n",
      "4446 [D loss: 0.535(R 0.512, F0.558)] [D acc: 0.711(R 0.688, F 0.734)] [G loss: 0.535] [G acc: 0.125]\n",
      "4447 [D loss: 0.538(R 0.514, F0.562)] [D acc: 0.727(R 0.656, F 0.797)] [G loss: 0.538] [G acc: 0.094]\n",
      "4448 [D loss: 0.547(R 0.594, F0.500)] [D acc: 0.727(R 0.656, F 0.797)] [G loss: 0.547] [G acc: 0.016]\n",
      "4449 [D loss: 0.630(R 0.631, F0.630)] [D acc: 0.695(R 0.578, F 0.812)] [G loss: 0.630] [G acc: 0.047]\n",
      "4450 [D loss: 0.538(R 0.558, F0.517)] [D acc: 0.711(R 0.625, F 0.797)] [G loss: 0.538] [G acc: 0.031]\n",
      "4451 [D loss: 0.564(R 0.674, F0.454)] [D acc: 0.680(R 0.516, F 0.844)] [G loss: 0.564] [G acc: 0.031]\n",
      "4452 [D loss: 0.537(R 0.553, F0.520)] [D acc: 0.727(R 0.656, F 0.797)] [G loss: 0.537] [G acc: 0.125]\n",
      "4453 [D loss: 0.604(R 0.558, F0.649)] [D acc: 0.703(R 0.688, F 0.719)] [G loss: 0.604] [G acc: 0.078]\n",
      "4454 [D loss: 0.469(R 0.497, F0.442)] [D acc: 0.773(R 0.672, F 0.875)] [G loss: 0.469] [G acc: 0.094]\n",
      "4455 [D loss: 0.512(R 0.583, F0.441)] [D acc: 0.734(R 0.562, F 0.906)] [G loss: 0.512] [G acc: 0.078]\n",
      "4456 [D loss: 0.513(R 0.528, F0.499)] [D acc: 0.742(R 0.625, F 0.859)] [G loss: 0.513] [G acc: 0.078]\n",
      "4457 [D loss: 0.457(R 0.529, F0.385)] [D acc: 0.766(R 0.656, F 0.875)] [G loss: 0.457] [G acc: 0.078]\n",
      "4458 [D loss: 0.499(R 0.520, F0.477)] [D acc: 0.742(R 0.672, F 0.812)] [G loss: 0.499] [G acc: 0.047]\n",
      "4459 [D loss: 0.565(R 0.656, F0.475)] [D acc: 0.695(R 0.547, F 0.844)] [G loss: 0.565] [G acc: 0.188]\n",
      "4460 [D loss: 0.462(R 0.459, F0.464)] [D acc: 0.766(R 0.719, F 0.812)] [G loss: 0.462] [G acc: 0.047]\n",
      "4461 [D loss: 0.569(R 0.484, F0.654)] [D acc: 0.758(R 0.750, F 0.766)] [G loss: 0.569] [G acc: 0.078]\n",
      "4462 [D loss: 0.409(R 0.461, F0.356)] [D acc: 0.797(R 0.703, F 0.891)] [G loss: 0.409] [G acc: 0.062]\n",
      "4463 [D loss: 0.546(R 0.524, F0.568)] [D acc: 0.719(R 0.688, F 0.750)] [G loss: 0.546] [G acc: 0.094]\n",
      "4464 [D loss: 0.460(R 0.499, F0.421)] [D acc: 0.805(R 0.703, F 0.906)] [G loss: 0.460] [G acc: 0.047]\n",
      "4465 [D loss: 0.540(R 0.541, F0.538)] [D acc: 0.688(R 0.641, F 0.734)] [G loss: 0.540] [G acc: 0.062]\n",
      "4466 [D loss: 0.564(R 0.661, F0.468)] [D acc: 0.703(R 0.531, F 0.875)] [G loss: 0.564] [G acc: 0.078]\n",
      "4467 [D loss: 0.507(R 0.577, F0.437)] [D acc: 0.734(R 0.625, F 0.844)] [G loss: 0.507] [G acc: 0.141]\n",
      "4468 [D loss: 0.507(R 0.460, F0.554)] [D acc: 0.781(R 0.750, F 0.812)] [G loss: 0.507] [G acc: 0.078]\n",
      "4469 [D loss: 0.561(R 0.601, F0.521)] [D acc: 0.688(R 0.625, F 0.750)] [G loss: 0.561] [G acc: 0.094]\n",
      "4470 [D loss: 0.481(R 0.629, F0.334)] [D acc: 0.812(R 0.688, F 0.938)] [G loss: 0.481] [G acc: 0.062]\n",
      "4471 [D loss: 0.564(R 0.591, F0.538)] [D acc: 0.758(R 0.734, F 0.781)] [G loss: 0.564] [G acc: 0.094]\n",
      "4472 [D loss: 0.441(R 0.504, F0.378)] [D acc: 0.812(R 0.703, F 0.922)] [G loss: 0.441] [G acc: 0.219]\n",
      "4473 [D loss: 0.485(R 0.440, F0.529)] [D acc: 0.750(R 0.750, F 0.750)] [G loss: 0.485] [G acc: 0.203]\n",
      "4474 [D loss: 0.538(R 0.371, F0.706)] [D acc: 0.719(R 0.781, F 0.656)] [G loss: 0.538] [G acc: 0.031]\n",
      "4475 [D loss: 0.581(R 0.593, F0.569)] [D acc: 0.734(R 0.641, F 0.828)] [G loss: 0.581] [G acc: 0.109]\n",
      "4476 [D loss: 0.501(R 0.527, F0.475)] [D acc: 0.766(R 0.688, F 0.844)] [G loss: 0.501] [G acc: 0.125]\n",
      "4477 [D loss: 0.537(R 0.516, F0.558)] [D acc: 0.734(R 0.719, F 0.750)] [G loss: 0.537] [G acc: 0.125]\n",
      "4478 [D loss: 0.549(R 0.673, F0.425)] [D acc: 0.734(R 0.625, F 0.844)] [G loss: 0.549] [G acc: 0.125]\n",
      "4479 [D loss: 0.427(R 0.438, F0.415)] [D acc: 0.820(R 0.766, F 0.875)] [G loss: 0.427] [G acc: 0.141]\n",
      "4480 [D loss: 0.498(R 0.527, F0.468)] [D acc: 0.758(R 0.703, F 0.812)] [G loss: 0.498] [G acc: 0.094]\n",
      "4481 [D loss: 0.476(R 0.410, F0.542)] [D acc: 0.773(R 0.703, F 0.844)] [G loss: 0.476] [G acc: 0.078]\n",
      "4482 [D loss: 0.441(R 0.532, F0.349)] [D acc: 0.781(R 0.672, F 0.891)] [G loss: 0.441] [G acc: 0.062]\n",
      "4483 [D loss: 0.508(R 0.560, F0.455)] [D acc: 0.758(R 0.656, F 0.859)] [G loss: 0.508] [G acc: 0.078]\n",
      "4484 [D loss: 0.583(R 0.418, F0.748)] [D acc: 0.727(R 0.766, F 0.688)] [G loss: 0.583] [G acc: 0.047]\n",
      "4485 [D loss: 0.588(R 0.743, F0.433)] [D acc: 0.688(R 0.500, F 0.875)] [G loss: 0.588] [G acc: 0.062]\n",
      "4486 [D loss: 0.542(R 0.594, F0.491)] [D acc: 0.734(R 0.672, F 0.797)] [G loss: 0.542] [G acc: 0.031]\n",
      "4487 [D loss: 0.537(R 0.536, F0.538)] [D acc: 0.727(R 0.656, F 0.797)] [G loss: 0.537] [G acc: 0.031]\n",
      "4488 [D loss: 0.558(R 0.634, F0.482)] [D acc: 0.695(R 0.641, F 0.750)] [G loss: 0.558] [G acc: 0.156]\n",
      "4489 [D loss: 0.430(R 0.446, F0.414)] [D acc: 0.820(R 0.750, F 0.891)] [G loss: 0.430] [G acc: 0.156]\n",
      "4490 [D loss: 0.522(R 0.523, F0.520)] [D acc: 0.711(R 0.656, F 0.766)] [G loss: 0.522] [G acc: 0.125]\n",
      "4491 [D loss: 0.558(R 0.528, F0.588)] [D acc: 0.734(R 0.688, F 0.781)] [G loss: 0.558] [G acc: 0.094]\n",
      "4492 [D loss: 0.510(R 0.532, F0.487)] [D acc: 0.750(R 0.641, F 0.859)] [G loss: 0.510] [G acc: 0.156]\n",
      "4493 [D loss: 0.403(R 0.374, F0.431)] [D acc: 0.828(R 0.797, F 0.859)] [G loss: 0.403] [G acc: 0.047]\n",
      "4494 [D loss: 0.569(R 0.512, F0.625)] [D acc: 0.742(R 0.719, F 0.766)] [G loss: 0.569] [G acc: 0.109]\n",
      "4495 [D loss: 0.488(R 0.557, F0.419)] [D acc: 0.750(R 0.688, F 0.812)] [G loss: 0.488] [G acc: 0.062]\n",
      "4496 [D loss: 0.573(R 0.538, F0.608)] [D acc: 0.703(R 0.641, F 0.766)] [G loss: 0.573] [G acc: 0.078]\n",
      "4497 [D loss: 0.499(R 0.577, F0.420)] [D acc: 0.758(R 0.688, F 0.828)] [G loss: 0.499] [G acc: 0.062]\n",
      "4498 [D loss: 0.572(R 0.633, F0.511)] [D acc: 0.789(R 0.750, F 0.828)] [G loss: 0.572] [G acc: 0.125]\n",
      "4499 [D loss: 0.500(R 0.540, F0.460)] [D acc: 0.742(R 0.672, F 0.812)] [G loss: 0.500] [G acc: 0.047]\n",
      "INFO:tensorflow:Assets written to: ram://3e734de7-ec2c-4ff4-9a6c-f7b929bae06f/assets\n",
      "INFO:tensorflow:Assets written to: ram://bfdaf92e-cb3a-491d-af25-a47e15b3db1e/assets\n",
      "INFO:tensorflow:Assets written to: ram://cc9dc0c5-a5ea-4840-b090-776a819095f9/assets\n",
      "4500 [D loss: 0.518(R 0.537, F0.500)] [D acc: 0.742(R 0.734, F 0.750)] [G loss: 0.518] [G acc: 0.016]\n",
      "4501 [D loss: 0.508(R 0.412, F0.605)] [D acc: 0.773(R 0.750, F 0.797)] [G loss: 0.508] [G acc: 0.062]\n",
      "4502 [D loss: 0.501(R 0.602, F0.400)] [D acc: 0.797(R 0.688, F 0.906)] [G loss: 0.501] [G acc: 0.078]\n",
      "4503 [D loss: 0.542(R 0.567, F0.517)] [D acc: 0.711(R 0.625, F 0.797)] [G loss: 0.542] [G acc: 0.109]\n",
      "4504 [D loss: 0.461(R 0.487, F0.435)] [D acc: 0.789(R 0.719, F 0.859)] [G loss: 0.461] [G acc: 0.094]\n",
      "4505 [D loss: 0.416(R 0.419, F0.412)] [D acc: 0.820(R 0.781, F 0.859)] [G loss: 0.416] [G acc: 0.125]\n",
      "4506 [D loss: 0.537(R 0.538, F0.537)] [D acc: 0.727(R 0.672, F 0.781)] [G loss: 0.537] [G acc: 0.062]\n",
      "4507 [D loss: 0.461(R 0.588, F0.333)] [D acc: 0.781(R 0.625, F 0.938)] [G loss: 0.461] [G acc: 0.016]\n",
      "4508 [D loss: 0.507(R 0.531, F0.482)] [D acc: 0.742(R 0.656, F 0.828)] [G loss: 0.507] [G acc: 0.078]\n",
      "4509 [D loss: 0.436(R 0.459, F0.414)] [D acc: 0.797(R 0.734, F 0.859)] [G loss: 0.436] [G acc: 0.062]\n",
      "4510 [D loss: 0.443(R 0.521, F0.365)] [D acc: 0.789(R 0.688, F 0.891)] [G loss: 0.443] [G acc: 0.141]\n",
      "4511 [D loss: 0.498(R 0.497, F0.500)] [D acc: 0.734(R 0.703, F 0.766)] [G loss: 0.498] [G acc: 0.047]\n",
      "4512 [D loss: 0.463(R 0.531, F0.396)] [D acc: 0.758(R 0.656, F 0.859)] [G loss: 0.463] [G acc: 0.000]\n",
      "4513 [D loss: 0.495(R 0.539, F0.451)] [D acc: 0.750(R 0.672, F 0.828)] [G loss: 0.495] [G acc: 0.078]\n",
      "4514 [D loss: 0.520(R 0.496, F0.544)] [D acc: 0.734(R 0.688, F 0.781)] [G loss: 0.520] [G acc: 0.125]\n",
      "4515 [D loss: 0.522(R 0.541, F0.504)] [D acc: 0.727(R 0.656, F 0.797)] [G loss: 0.522] [G acc: 0.031]\n",
      "4516 [D loss: 0.551(R 0.619, F0.484)] [D acc: 0.727(R 0.656, F 0.797)] [G loss: 0.551] [G acc: 0.109]\n",
      "4517 [D loss: 0.509(R 0.537, F0.481)] [D acc: 0.766(R 0.672, F 0.859)] [G loss: 0.509] [G acc: 0.078]\n",
      "4518 [D loss: 0.464(R 0.484, F0.444)] [D acc: 0.781(R 0.672, F 0.891)] [G loss: 0.464] [G acc: 0.094]\n",
      "4519 [D loss: 0.497(R 0.568, F0.425)] [D acc: 0.766(R 0.656, F 0.875)] [G loss: 0.497] [G acc: 0.141]\n",
      "4520 [D loss: 0.404(R 0.400, F0.408)] [D acc: 0.844(R 0.812, F 0.875)] [G loss: 0.404] [G acc: 0.047]\n",
      "4521 [D loss: 0.537(R 0.413, F0.662)] [D acc: 0.781(R 0.766, F 0.797)] [G loss: 0.537] [G acc: 0.062]\n",
      "4522 [D loss: 0.567(R 0.719, F0.414)] [D acc: 0.688(R 0.516, F 0.859)] [G loss: 0.567] [G acc: 0.094]\n",
      "4523 [D loss: 0.391(R 0.401, F0.381)] [D acc: 0.859(R 0.828, F 0.891)] [G loss: 0.391] [G acc: 0.125]\n",
      "4524 [D loss: 0.462(R 0.542, F0.383)] [D acc: 0.797(R 0.672, F 0.922)] [G loss: 0.462] [G acc: 0.109]\n",
      "4525 [D loss: 0.485(R 0.436, F0.534)] [D acc: 0.750(R 0.703, F 0.797)] [G loss: 0.485] [G acc: 0.094]\n",
      "4526 [D loss: 0.343(R 0.268, F0.417)] [D acc: 0.875(R 0.875, F 0.875)] [G loss: 0.343] [G acc: 0.078]\n",
      "4527 [D loss: 0.472(R 0.517, F0.426)] [D acc: 0.805(R 0.734, F 0.875)] [G loss: 0.472] [G acc: 0.109]\n",
      "4528 [D loss: 0.450(R 0.393, F0.507)] [D acc: 0.797(R 0.812, F 0.781)] [G loss: 0.450] [G acc: 0.062]\n",
      "4529 [D loss: 0.617(R 0.647, F0.588)] [D acc: 0.711(R 0.594, F 0.828)] [G loss: 0.617] [G acc: 0.031]\n",
      "4530 [D loss: 0.499(R 0.605, F0.393)] [D acc: 0.750(R 0.609, F 0.891)] [G loss: 0.499] [G acc: 0.031]\n",
      "4531 [D loss: 0.432(R 0.534, F0.329)] [D acc: 0.828(R 0.703, F 0.953)] [G loss: 0.432] [G acc: 0.109]\n",
      "4532 [D loss: 0.496(R 0.503, F0.488)] [D acc: 0.758(R 0.703, F 0.812)] [G loss: 0.496] [G acc: 0.078]\n",
      "4533 [D loss: 0.464(R 0.564, F0.364)] [D acc: 0.805(R 0.719, F 0.891)] [G loss: 0.464] [G acc: 0.109]\n",
      "4534 [D loss: 0.432(R 0.404, F0.459)] [D acc: 0.812(R 0.797, F 0.828)] [G loss: 0.432] [G acc: 0.047]\n",
      "4535 [D loss: 0.397(R 0.375, F0.419)] [D acc: 0.805(R 0.734, F 0.875)] [G loss: 0.397] [G acc: 0.047]\n",
      "4536 [D loss: 0.420(R 0.400, F0.441)] [D acc: 0.828(R 0.812, F 0.844)] [G loss: 0.420] [G acc: 0.109]\n",
      "4537 [D loss: 0.466(R 0.449, F0.483)] [D acc: 0.758(R 0.766, F 0.750)] [G loss: 0.466] [G acc: 0.078]\n",
      "4538 [D loss: 0.561(R 0.650, F0.473)] [D acc: 0.750(R 0.656, F 0.844)] [G loss: 0.561] [G acc: 0.156]\n",
      "4539 [D loss: 0.492(R 0.511, F0.473)] [D acc: 0.773(R 0.688, F 0.859)] [G loss: 0.492] [G acc: 0.047]\n",
      "4540 [D loss: 0.532(R 0.569, F0.495)] [D acc: 0.750(R 0.719, F 0.781)] [G loss: 0.532] [G acc: 0.047]\n",
      "4541 [D loss: 0.486(R 0.558, F0.414)] [D acc: 0.742(R 0.641, F 0.844)] [G loss: 0.486] [G acc: 0.125]\n",
      "4542 [D loss: 0.519(R 0.495, F0.543)] [D acc: 0.742(R 0.719, F 0.766)] [G loss: 0.519] [G acc: 0.062]\n",
      "4543 [D loss: 0.481(R 0.534, F0.428)] [D acc: 0.781(R 0.672, F 0.891)] [G loss: 0.481] [G acc: 0.094]\n",
      "4544 [D loss: 0.531(R 0.631, F0.431)] [D acc: 0.766(R 0.656, F 0.875)] [G loss: 0.531] [G acc: 0.109]\n",
      "4545 [D loss: 0.534(R 0.586, F0.483)] [D acc: 0.711(R 0.625, F 0.797)] [G loss: 0.534] [G acc: 0.094]\n",
      "4546 [D loss: 0.530(R 0.580, F0.481)] [D acc: 0.781(R 0.719, F 0.844)] [G loss: 0.530] [G acc: 0.109]\n",
      "4547 [D loss: 0.418(R 0.388, F0.447)] [D acc: 0.805(R 0.797, F 0.812)] [G loss: 0.418] [G acc: 0.062]\n",
      "4548 [D loss: 0.469(R 0.571, F0.367)] [D acc: 0.797(R 0.672, F 0.922)] [G loss: 0.469] [G acc: 0.031]\n",
      "4549 [D loss: 0.564(R 0.576, F0.552)] [D acc: 0.781(R 0.734, F 0.828)] [G loss: 0.564] [G acc: 0.156]\n",
      "4550 [D loss: 0.562(R 0.617, F0.507)] [D acc: 0.688(R 0.609, F 0.766)] [G loss: 0.562] [G acc: 0.062]\n",
      "4551 [D loss: 0.454(R 0.406, F0.502)] [D acc: 0.773(R 0.797, F 0.750)] [G loss: 0.454] [G acc: 0.094]\n",
      "4552 [D loss: 0.448(R 0.414, F0.481)] [D acc: 0.781(R 0.750, F 0.812)] [G loss: 0.448] [G acc: 0.094]\n",
      "4553 [D loss: 0.540(R 0.533, F0.548)] [D acc: 0.734(R 0.688, F 0.781)] [G loss: 0.540] [G acc: 0.047]\n",
      "4554 [D loss: 0.498(R 0.545, F0.452)] [D acc: 0.742(R 0.688, F 0.797)] [G loss: 0.498] [G acc: 0.078]\n",
      "4555 [D loss: 0.530(R 0.569, F0.490)] [D acc: 0.695(R 0.594, F 0.797)] [G loss: 0.530] [G acc: 0.125]\n",
      "4556 [D loss: 0.594(R 0.622, F0.566)] [D acc: 0.711(R 0.625, F 0.797)] [G loss: 0.594] [G acc: 0.094]\n",
      "4557 [D loss: 0.516(R 0.662, F0.370)] [D acc: 0.773(R 0.672, F 0.875)] [G loss: 0.516] [G acc: 0.109]\n",
      "4558 [D loss: 0.415(R 0.414, F0.416)] [D acc: 0.797(R 0.734, F 0.859)] [G loss: 0.415] [G acc: 0.109]\n",
      "4559 [D loss: 0.502(R 0.381, F0.623)] [D acc: 0.758(R 0.828, F 0.688)] [G loss: 0.502] [G acc: 0.016]\n",
      "4560 [D loss: 0.457(R 0.515, F0.398)] [D acc: 0.781(R 0.703, F 0.859)] [G loss: 0.457] [G acc: 0.094]\n",
      "4561 [D loss: 0.509(R 0.532, F0.486)] [D acc: 0.758(R 0.656, F 0.859)] [G loss: 0.509] [G acc: 0.156]\n",
      "4562 [D loss: 0.522(R 0.596, F0.448)] [D acc: 0.773(R 0.656, F 0.891)] [G loss: 0.522] [G acc: 0.016]\n",
      "4563 [D loss: 0.492(R 0.500, F0.484)] [D acc: 0.766(R 0.703, F 0.828)] [G loss: 0.492] [G acc: 0.141]\n",
      "4564 [D loss: 0.457(R 0.500, F0.415)] [D acc: 0.773(R 0.719, F 0.828)] [G loss: 0.457] [G acc: 0.062]\n",
      "4565 [D loss: 0.466(R 0.466, F0.465)] [D acc: 0.789(R 0.734, F 0.844)] [G loss: 0.466] [G acc: 0.062]\n",
      "4566 [D loss: 0.547(R 0.537, F0.558)] [D acc: 0.734(R 0.703, F 0.766)] [G loss: 0.547] [G acc: 0.062]\n",
      "4567 [D loss: 0.613(R 0.649, F0.578)] [D acc: 0.688(R 0.625, F 0.750)] [G loss: 0.613] [G acc: 0.156]\n",
      "4568 [D loss: 0.535(R 0.611, F0.459)] [D acc: 0.711(R 0.594, F 0.828)] [G loss: 0.535] [G acc: 0.031]\n",
      "4569 [D loss: 0.561(R 0.585, F0.537)] [D acc: 0.711(R 0.656, F 0.766)] [G loss: 0.561] [G acc: 0.094]\n",
      "4570 [D loss: 0.585(R 0.611, F0.558)] [D acc: 0.695(R 0.594, F 0.797)] [G loss: 0.585] [G acc: 0.031]\n",
      "4571 [D loss: 0.564(R 0.627, F0.502)] [D acc: 0.727(R 0.672, F 0.781)] [G loss: 0.564] [G acc: 0.188]\n",
      "4572 [D loss: 0.497(R 0.487, F0.507)] [D acc: 0.703(R 0.609, F 0.797)] [G loss: 0.497] [G acc: 0.078]\n",
      "4573 [D loss: 0.509(R 0.522, F0.497)] [D acc: 0.758(R 0.672, F 0.844)] [G loss: 0.509] [G acc: 0.109]\n",
      "4574 [D loss: 0.586(R 0.519, F0.653)] [D acc: 0.703(R 0.656, F 0.750)] [G loss: 0.586] [G acc: 0.047]\n",
      "4575 [D loss: 0.526(R 0.479, F0.573)] [D acc: 0.703(R 0.688, F 0.719)] [G loss: 0.526] [G acc: 0.047]\n",
      "4576 [D loss: 0.454(R 0.468, F0.440)] [D acc: 0.766(R 0.688, F 0.844)] [G loss: 0.454] [G acc: 0.109]\n",
      "4577 [D loss: 0.654(R 0.684, F0.624)] [D acc: 0.656(R 0.562, F 0.750)] [G loss: 0.654] [G acc: 0.062]\n",
      "4578 [D loss: 0.469(R 0.529, F0.409)] [D acc: 0.742(R 0.609, F 0.875)] [G loss: 0.469] [G acc: 0.031]\n",
      "4579 [D loss: 0.506(R 0.479, F0.534)] [D acc: 0.727(R 0.703, F 0.750)] [G loss: 0.506] [G acc: 0.094]\n",
      "4580 [D loss: 0.527(R 0.592, F0.462)] [D acc: 0.734(R 0.609, F 0.859)] [G loss: 0.527] [G acc: 0.109]\n",
      "4581 [D loss: 0.502(R 0.506, F0.498)] [D acc: 0.805(R 0.734, F 0.875)] [G loss: 0.502] [G acc: 0.031]\n",
      "4582 [D loss: 0.540(R 0.583, F0.496)] [D acc: 0.695(R 0.625, F 0.766)] [G loss: 0.540] [G acc: 0.062]\n",
      "4583 [D loss: 0.483(R 0.523, F0.443)] [D acc: 0.766(R 0.688, F 0.844)] [G loss: 0.483] [G acc: 0.125]\n",
      "4584 [D loss: 0.361(R 0.346, F0.376)] [D acc: 0.867(R 0.828, F 0.906)] [G loss: 0.361] [G acc: 0.109]\n",
      "4585 [D loss: 0.488(R 0.386, F0.590)] [D acc: 0.766(R 0.797, F 0.734)] [G loss: 0.488] [G acc: 0.078]\n",
      "4586 [D loss: 0.519(R 0.576, F0.462)] [D acc: 0.734(R 0.641, F 0.828)] [G loss: 0.519] [G acc: 0.062]\n",
      "4587 [D loss: 0.592(R 0.667, F0.516)] [D acc: 0.766(R 0.672, F 0.859)] [G loss: 0.592] [G acc: 0.062]\n",
      "4588 [D loss: 0.573(R 0.658, F0.488)] [D acc: 0.711(R 0.594, F 0.828)] [G loss: 0.573] [G acc: 0.078]\n",
      "4589 [D loss: 0.502(R 0.571, F0.433)] [D acc: 0.758(R 0.625, F 0.891)] [G loss: 0.502] [G acc: 0.047]\n",
      "4590 [D loss: 0.520(R 0.605, F0.435)] [D acc: 0.750(R 0.641, F 0.859)] [G loss: 0.520] [G acc: 0.078]\n",
      "4591 [D loss: 0.445(R 0.349, F0.542)] [D acc: 0.781(R 0.812, F 0.750)] [G loss: 0.445] [G acc: 0.062]\n",
      "4592 [D loss: 0.529(R 0.505, F0.554)] [D acc: 0.703(R 0.625, F 0.781)] [G loss: 0.529] [G acc: 0.062]\n",
      "4593 [D loss: 0.501(R 0.577, F0.425)] [D acc: 0.742(R 0.641, F 0.844)] [G loss: 0.501] [G acc: 0.062]\n",
      "4594 [D loss: 0.508(R 0.526, F0.489)] [D acc: 0.758(R 0.688, F 0.828)] [G loss: 0.508] [G acc: 0.141]\n",
      "4595 [D loss: 0.442(R 0.474, F0.410)] [D acc: 0.758(R 0.688, F 0.828)] [G loss: 0.442] [G acc: 0.156]\n",
      "4596 [D loss: 0.511(R 0.470, F0.551)] [D acc: 0.766(R 0.734, F 0.797)] [G loss: 0.511] [G acc: 0.078]\n",
      "4597 [D loss: 0.551(R 0.637, F0.465)] [D acc: 0.695(R 0.609, F 0.781)] [G loss: 0.551] [G acc: 0.031]\n",
      "4598 [D loss: 0.505(R 0.543, F0.468)] [D acc: 0.711(R 0.625, F 0.797)] [G loss: 0.505] [G acc: 0.078]\n",
      "4599 [D loss: 0.448(R 0.482, F0.414)] [D acc: 0.773(R 0.703, F 0.844)] [G loss: 0.448] [G acc: 0.062]\n",
      "INFO:tensorflow:Assets written to: ram://85efc12d-7c94-4d19-a88b-3c78df9c8ee4/assets\n",
      "INFO:tensorflow:Assets written to: ram://310b79e2-670a-4f89-9cdb-0e62844f2fb3/assets\n",
      "INFO:tensorflow:Assets written to: ram://2525dc82-6bfd-4187-b211-0c6e9d93011a/assets\n",
      "4600 [D loss: 0.496(R 0.531, F0.461)] [D acc: 0.758(R 0.688, F 0.828)] [G loss: 0.496] [G acc: 0.078]\n",
      "4601 [D loss: 0.445(R 0.520, F0.370)] [D acc: 0.820(R 0.719, F 0.922)] [G loss: 0.445] [G acc: 0.141]\n",
      "4602 [D loss: 0.625(R 0.473, F0.778)] [D acc: 0.695(R 0.734, F 0.656)] [G loss: 0.625] [G acc: 0.047]\n",
      "4603 [D loss: 0.529(R 0.573, F0.485)] [D acc: 0.727(R 0.641, F 0.812)] [G loss: 0.529] [G acc: 0.062]\n",
      "4604 [D loss: 0.559(R 0.548, F0.570)] [D acc: 0.664(R 0.625, F 0.703)] [G loss: 0.559] [G acc: 0.094]\n",
      "4605 [D loss: 0.508(R 0.573, F0.443)] [D acc: 0.766(R 0.672, F 0.859)] [G loss: 0.508] [G acc: 0.109]\n",
      "4606 [D loss: 0.576(R 0.589, F0.563)] [D acc: 0.703(R 0.625, F 0.781)] [G loss: 0.576] [G acc: 0.141]\n",
      "4607 [D loss: 0.522(R 0.529, F0.515)] [D acc: 0.734(R 0.656, F 0.812)] [G loss: 0.522] [G acc: 0.078]\n",
      "4608 [D loss: 0.527(R 0.458, F0.596)] [D acc: 0.734(R 0.719, F 0.750)] [G loss: 0.527] [G acc: 0.125]\n",
      "4609 [D loss: 0.551(R 0.574, F0.529)] [D acc: 0.766(R 0.703, F 0.828)] [G loss: 0.551] [G acc: 0.062]\n",
      "4610 [D loss: 0.578(R 0.530, F0.625)] [D acc: 0.781(R 0.719, F 0.844)] [G loss: 0.578] [G acc: 0.172]\n",
      "4611 [D loss: 0.515(R 0.597, F0.433)] [D acc: 0.758(R 0.672, F 0.844)] [G loss: 0.515] [G acc: 0.125]\n",
      "4612 [D loss: 0.515(R 0.594, F0.437)] [D acc: 0.773(R 0.672, F 0.875)] [G loss: 0.515] [G acc: 0.094]\n",
      "4613 [D loss: 0.561(R 0.514, F0.608)] [D acc: 0.695(R 0.703, F 0.688)] [G loss: 0.561] [G acc: 0.094]\n",
      "4614 [D loss: 0.455(R 0.521, F0.389)] [D acc: 0.781(R 0.703, F 0.859)] [G loss: 0.455] [G acc: 0.156]\n",
      "4615 [D loss: 0.549(R 0.460, F0.638)] [D acc: 0.734(R 0.719, F 0.750)] [G loss: 0.549] [G acc: 0.125]\n",
      "4616 [D loss: 0.470(R 0.574, F0.366)] [D acc: 0.758(R 0.625, F 0.891)] [G loss: 0.470] [G acc: 0.062]\n",
      "4617 [D loss: 0.520(R 0.471, F0.569)] [D acc: 0.719(R 0.688, F 0.750)] [G loss: 0.520] [G acc: 0.078]\n",
      "4618 [D loss: 0.550(R 0.539, F0.562)] [D acc: 0.727(R 0.656, F 0.797)] [G loss: 0.550] [G acc: 0.109]\n",
      "4619 [D loss: 0.582(R 0.596, F0.569)] [D acc: 0.711(R 0.625, F 0.797)] [G loss: 0.582] [G acc: 0.141]\n",
      "4620 [D loss: 0.478(R 0.537, F0.419)] [D acc: 0.742(R 0.656, F 0.828)] [G loss: 0.478] [G acc: 0.062]\n",
      "4621 [D loss: 0.436(R 0.448, F0.424)] [D acc: 0.773(R 0.703, F 0.844)] [G loss: 0.436] [G acc: 0.078]\n",
      "4622 [D loss: 0.588(R 0.686, F0.489)] [D acc: 0.719(R 0.562, F 0.875)] [G loss: 0.588] [G acc: 0.047]\n",
      "4623 [D loss: 0.529(R 0.460, F0.599)] [D acc: 0.727(R 0.750, F 0.703)] [G loss: 0.529] [G acc: 0.109]\n",
      "4624 [D loss: 0.454(R 0.384, F0.525)] [D acc: 0.781(R 0.750, F 0.812)] [G loss: 0.454] [G acc: 0.047]\n",
      "4625 [D loss: 0.474(R 0.516, F0.432)] [D acc: 0.750(R 0.672, F 0.828)] [G loss: 0.474] [G acc: 0.078]\n",
      "4626 [D loss: 0.442(R 0.420, F0.464)] [D acc: 0.758(R 0.766, F 0.750)] [G loss: 0.442] [G acc: 0.078]\n",
      "4627 [D loss: 0.485(R 0.503, F0.467)] [D acc: 0.750(R 0.703, F 0.797)] [G loss: 0.485] [G acc: 0.078]\n",
      "4628 [D loss: 0.609(R 0.687, F0.532)] [D acc: 0.680(R 0.547, F 0.812)] [G loss: 0.609] [G acc: 0.094]\n",
      "4629 [D loss: 0.453(R 0.493, F0.412)] [D acc: 0.812(R 0.750, F 0.875)] [G loss: 0.453] [G acc: 0.062]\n",
      "4630 [D loss: 0.513(R 0.583, F0.442)] [D acc: 0.727(R 0.641, F 0.812)] [G loss: 0.513] [G acc: 0.094]\n",
      "4631 [D loss: 0.544(R 0.462, F0.625)] [D acc: 0.703(R 0.703, F 0.703)] [G loss: 0.544] [G acc: 0.094]\n",
      "4632 [D loss: 0.484(R 0.514, F0.453)] [D acc: 0.734(R 0.672, F 0.797)] [G loss: 0.484] [G acc: 0.094]\n",
      "4633 [D loss: 0.467(R 0.436, F0.498)] [D acc: 0.781(R 0.766, F 0.797)] [G loss: 0.467] [G acc: 0.016]\n",
      "4634 [D loss: 0.516(R 0.548, F0.485)] [D acc: 0.727(R 0.656, F 0.797)] [G loss: 0.516] [G acc: 0.203]\n",
      "4635 [D loss: 0.567(R 0.543, F0.590)] [D acc: 0.727(R 0.656, F 0.797)] [G loss: 0.567] [G acc: 0.094]\n",
      "4636 [D loss: 0.599(R 0.620, F0.578)] [D acc: 0.695(R 0.656, F 0.734)] [G loss: 0.599] [G acc: 0.125]\n",
      "4637 [D loss: 0.446(R 0.523, F0.369)] [D acc: 0.781(R 0.688, F 0.875)] [G loss: 0.446] [G acc: 0.094]\n",
      "4638 [D loss: 0.612(R 0.530, F0.695)] [D acc: 0.695(R 0.641, F 0.750)] [G loss: 0.612] [G acc: 0.062]\n",
      "4639 [D loss: 0.482(R 0.584, F0.379)] [D acc: 0.742(R 0.641, F 0.844)] [G loss: 0.482] [G acc: 0.078]\n",
      "4640 [D loss: 0.555(R 0.554, F0.555)] [D acc: 0.703(R 0.625, F 0.781)] [G loss: 0.555] [G acc: 0.031]\n",
      "4641 [D loss: 0.614(R 0.587, F0.640)] [D acc: 0.641(R 0.609, F 0.672)] [G loss: 0.614] [G acc: 0.078]\n",
      "4642 [D loss: 0.453(R 0.487, F0.420)] [D acc: 0.789(R 0.703, F 0.875)] [G loss: 0.453] [G acc: 0.078]\n",
      "4643 [D loss: 0.524(R 0.643, F0.406)] [D acc: 0.742(R 0.594, F 0.891)] [G loss: 0.524] [G acc: 0.078]\n",
      "4644 [D loss: 0.485(R 0.577, F0.394)] [D acc: 0.758(R 0.609, F 0.906)] [G loss: 0.485] [G acc: 0.141]\n",
      "4645 [D loss: 0.526(R 0.422, F0.630)] [D acc: 0.766(R 0.750, F 0.781)] [G loss: 0.526] [G acc: 0.062]\n",
      "4646 [D loss: 0.499(R 0.548, F0.450)] [D acc: 0.766(R 0.703, F 0.828)] [G loss: 0.499] [G acc: 0.078]\n",
      "4647 [D loss: 0.509(R 0.543, F0.476)] [D acc: 0.766(R 0.672, F 0.859)] [G loss: 0.509] [G acc: 0.062]\n",
      "4648 [D loss: 0.488(R 0.581, F0.395)] [D acc: 0.766(R 0.641, F 0.891)] [G loss: 0.488] [G acc: 0.062]\n",
      "4649 [D loss: 0.435(R 0.401, F0.470)] [D acc: 0.781(R 0.766, F 0.797)] [G loss: 0.435] [G acc: 0.125]\n",
      "4650 [D loss: 0.512(R 0.592, F0.432)] [D acc: 0.758(R 0.641, F 0.875)] [G loss: 0.512] [G acc: 0.078]\n",
      "4651 [D loss: 0.610(R 0.614, F0.607)] [D acc: 0.641(R 0.578, F 0.703)] [G loss: 0.610] [G acc: 0.094]\n",
      "4652 [D loss: 0.542(R 0.525, F0.558)] [D acc: 0.734(R 0.625, F 0.844)] [G loss: 0.542] [G acc: 0.125]\n",
      "4653 [D loss: 0.527(R 0.609, F0.445)] [D acc: 0.719(R 0.578, F 0.859)] [G loss: 0.527] [G acc: 0.062]\n",
      "4654 [D loss: 0.435(R 0.465, F0.406)] [D acc: 0.773(R 0.688, F 0.859)] [G loss: 0.435] [G acc: 0.109]\n",
      "4655 [D loss: 0.561(R 0.584, F0.538)] [D acc: 0.711(R 0.641, F 0.781)] [G loss: 0.561] [G acc: 0.062]\n",
      "4656 [D loss: 0.507(R 0.526, F0.487)] [D acc: 0.758(R 0.672, F 0.844)] [G loss: 0.507] [G acc: 0.109]\n",
      "4657 [D loss: 0.486(R 0.517, F0.454)] [D acc: 0.758(R 0.688, F 0.828)] [G loss: 0.486] [G acc: 0.109]\n",
      "4658 [D loss: 0.547(R 0.602, F0.491)] [D acc: 0.750(R 0.656, F 0.844)] [G loss: 0.547] [G acc: 0.141]\n",
      "4659 [D loss: 0.494(R 0.448, F0.539)] [D acc: 0.750(R 0.703, F 0.797)] [G loss: 0.494] [G acc: 0.031]\n",
      "4660 [D loss: 0.644(R 0.615, F0.673)] [D acc: 0.648(R 0.562, F 0.734)] [G loss: 0.644] [G acc: 0.094]\n",
      "4661 [D loss: 0.603(R 0.708, F0.498)] [D acc: 0.703(R 0.625, F 0.781)] [G loss: 0.603] [G acc: 0.125]\n",
      "4662 [D loss: 0.537(R 0.519, F0.556)] [D acc: 0.805(R 0.750, F 0.859)] [G loss: 0.537] [G acc: 0.141]\n",
      "4663 [D loss: 0.453(R 0.446, F0.461)] [D acc: 0.820(R 0.766, F 0.875)] [G loss: 0.453] [G acc: 0.125]\n",
      "4664 [D loss: 0.450(R 0.432, F0.467)] [D acc: 0.758(R 0.703, F 0.812)] [G loss: 0.450] [G acc: 0.109]\n",
      "4665 [D loss: 0.524(R 0.472, F0.576)] [D acc: 0.734(R 0.719, F 0.750)] [G loss: 0.524] [G acc: 0.047]\n",
      "4666 [D loss: 0.586(R 0.656, F0.517)] [D acc: 0.680(R 0.594, F 0.766)] [G loss: 0.586] [G acc: 0.094]\n",
      "4667 [D loss: 0.527(R 0.635, F0.419)] [D acc: 0.734(R 0.562, F 0.906)] [G loss: 0.527] [G acc: 0.141]\n",
      "4668 [D loss: 0.444(R 0.376, F0.513)] [D acc: 0.789(R 0.812, F 0.766)] [G loss: 0.444] [G acc: 0.094]\n",
      "4669 [D loss: 0.418(R 0.436, F0.400)] [D acc: 0.828(R 0.781, F 0.875)] [G loss: 0.418] [G acc: 0.016]\n",
      "4670 [D loss: 0.622(R 0.510, F0.733)] [D acc: 0.703(R 0.734, F 0.672)] [G loss: 0.622] [G acc: 0.109]\n",
      "4671 [D loss: 0.483(R 0.517, F0.450)] [D acc: 0.773(R 0.688, F 0.859)] [G loss: 0.483] [G acc: 0.109]\n",
      "4672 [D loss: 0.474(R 0.549, F0.399)] [D acc: 0.750(R 0.641, F 0.859)] [G loss: 0.474] [G acc: 0.094]\n",
      "4673 [D loss: 0.431(R 0.444, F0.418)] [D acc: 0.781(R 0.656, F 0.906)] [G loss: 0.431] [G acc: 0.094]\n",
      "4674 [D loss: 0.509(R 0.582, F0.436)] [D acc: 0.719(R 0.562, F 0.875)] [G loss: 0.509] [G acc: 0.078]\n",
      "4675 [D loss: 0.479(R 0.514, F0.445)] [D acc: 0.758(R 0.703, F 0.812)] [G loss: 0.479] [G acc: 0.109]\n",
      "4676 [D loss: 0.415(R 0.434, F0.395)] [D acc: 0.852(R 0.766, F 0.938)] [G loss: 0.415] [G acc: 0.109]\n",
      "4677 [D loss: 0.505(R 0.490, F0.520)] [D acc: 0.781(R 0.750, F 0.812)] [G loss: 0.505] [G acc: 0.094]\n",
      "4678 [D loss: 0.417(R 0.439, F0.395)] [D acc: 0.805(R 0.766, F 0.844)] [G loss: 0.417] [G acc: 0.031]\n",
      "4679 [D loss: 0.608(R 0.556, F0.660)] [D acc: 0.703(R 0.625, F 0.781)] [G loss: 0.608] [G acc: 0.141]\n",
      "4680 [D loss: 0.446(R 0.528, F0.364)] [D acc: 0.797(R 0.688, F 0.906)] [G loss: 0.446] [G acc: 0.125]\n",
      "4681 [D loss: 0.460(R 0.424, F0.495)] [D acc: 0.820(R 0.750, F 0.891)] [G loss: 0.460] [G acc: 0.141]\n",
      "4682 [D loss: 0.594(R 0.462, F0.726)] [D acc: 0.703(R 0.703, F 0.703)] [G loss: 0.594] [G acc: 0.047]\n",
      "4683 [D loss: 0.651(R 0.658, F0.645)] [D acc: 0.680(R 0.609, F 0.750)] [G loss: 0.651] [G acc: 0.078]\n",
      "4684 [D loss: 0.478(R 0.550, F0.405)] [D acc: 0.805(R 0.719, F 0.891)] [G loss: 0.478] [G acc: 0.094]\n",
      "4685 [D loss: 0.568(R 0.647, F0.489)] [D acc: 0.742(R 0.625, F 0.859)] [G loss: 0.568] [G acc: 0.125]\n",
      "4686 [D loss: 0.440(R 0.457, F0.423)] [D acc: 0.836(R 0.797, F 0.875)] [G loss: 0.440] [G acc: 0.031]\n",
      "4687 [D loss: 0.490(R 0.456, F0.524)] [D acc: 0.766(R 0.750, F 0.781)] [G loss: 0.490] [G acc: 0.047]\n",
      "4688 [D loss: 0.493(R 0.476, F0.510)] [D acc: 0.773(R 0.750, F 0.797)] [G loss: 0.493] [G acc: 0.188]\n",
      "4689 [D loss: 0.428(R 0.485, F0.372)] [D acc: 0.820(R 0.766, F 0.875)] [G loss: 0.428] [G acc: 0.062]\n",
      "4690 [D loss: 0.514(R 0.577, F0.451)] [D acc: 0.766(R 0.641, F 0.891)] [G loss: 0.514] [G acc: 0.109]\n",
      "4691 [D loss: 0.579(R 0.698, F0.459)] [D acc: 0.672(R 0.531, F 0.812)] [G loss: 0.579] [G acc: 0.016]\n",
      "4692 [D loss: 0.516(R 0.477, F0.555)] [D acc: 0.727(R 0.734, F 0.719)] [G loss: 0.516] [G acc: 0.078]\n",
      "4693 [D loss: 0.509(R 0.468, F0.549)] [D acc: 0.750(R 0.750, F 0.750)] [G loss: 0.509] [G acc: 0.109]\n",
      "4694 [D loss: 0.469(R 0.488, F0.449)] [D acc: 0.797(R 0.750, F 0.844)] [G loss: 0.469] [G acc: 0.078]\n",
      "4695 [D loss: 0.530(R 0.453, F0.607)] [D acc: 0.750(R 0.688, F 0.812)] [G loss: 0.530] [G acc: 0.047]\n",
      "4696 [D loss: 0.408(R 0.473, F0.344)] [D acc: 0.836(R 0.734, F 0.938)] [G loss: 0.408] [G acc: 0.047]\n",
      "4697 [D loss: 0.478(R 0.331, F0.625)] [D acc: 0.773(R 0.828, F 0.719)] [G loss: 0.478] [G acc: 0.062]\n",
      "4698 [D loss: 0.508(R 0.703, F0.314)] [D acc: 0.766(R 0.562, F 0.969)] [G loss: 0.508] [G acc: 0.047]\n",
      "4699 [D loss: 0.500(R 0.572, F0.429)] [D acc: 0.789(R 0.734, F 0.844)] [G loss: 0.500] [G acc: 0.094]\n",
      "INFO:tensorflow:Assets written to: ram://769315dd-0aa8-4836-a7e8-0463915df28e/assets\n",
      "INFO:tensorflow:Assets written to: ram://9aa0eafc-e941-4f38-bb35-f1ae63899028/assets\n",
      "INFO:tensorflow:Assets written to: ram://1c1a60ca-c7b6-4919-a04e-da7dd7b79670/assets\n",
      "4700 [D loss: 0.520(R 0.465, F0.574)] [D acc: 0.797(R 0.766, F 0.828)] [G loss: 0.520] [G acc: 0.109]\n",
      "4701 [D loss: 0.489(R 0.482, F0.496)] [D acc: 0.766(R 0.766, F 0.766)] [G loss: 0.489] [G acc: 0.125]\n",
      "4702 [D loss: 0.437(R 0.440, F0.435)] [D acc: 0.797(R 0.766, F 0.828)] [G loss: 0.437] [G acc: 0.125]\n",
      "4703 [D loss: 0.441(R 0.344, F0.537)] [D acc: 0.789(R 0.797, F 0.781)] [G loss: 0.441] [G acc: 0.109]\n",
      "4704 [D loss: 0.511(R 0.558, F0.465)] [D acc: 0.750(R 0.641, F 0.859)] [G loss: 0.511] [G acc: 0.141]\n",
      "4705 [D loss: 0.452(R 0.403, F0.502)] [D acc: 0.789(R 0.750, F 0.828)] [G loss: 0.452] [G acc: 0.141]\n",
      "4706 [D loss: 0.417(R 0.507, F0.326)] [D acc: 0.805(R 0.734, F 0.875)] [G loss: 0.417] [G acc: 0.062]\n",
      "4707 [D loss: 0.497(R 0.474, F0.519)] [D acc: 0.766(R 0.703, F 0.828)] [G loss: 0.497] [G acc: 0.094]\n",
      "4708 [D loss: 0.461(R 0.490, F0.431)] [D acc: 0.836(R 0.734, F 0.938)] [G loss: 0.461] [G acc: 0.047]\n",
      "4709 [D loss: 0.476(R 0.409, F0.544)] [D acc: 0.758(R 0.734, F 0.781)] [G loss: 0.476] [G acc: 0.109]\n",
      "4710 [D loss: 0.455(R 0.470, F0.440)] [D acc: 0.789(R 0.766, F 0.812)] [G loss: 0.455] [G acc: 0.109]\n",
      "4711 [D loss: 0.505(R 0.472, F0.538)] [D acc: 0.789(R 0.734, F 0.844)] [G loss: 0.505] [G acc: 0.094]\n",
      "4712 [D loss: 0.542(R 0.618, F0.467)] [D acc: 0.727(R 0.625, F 0.828)] [G loss: 0.542] [G acc: 0.062]\n",
      "4713 [D loss: 0.515(R 0.609, F0.422)] [D acc: 0.766(R 0.641, F 0.891)] [G loss: 0.515] [G acc: 0.062]\n",
      "4714 [D loss: 0.549(R 0.344, F0.755)] [D acc: 0.750(R 0.797, F 0.703)] [G loss: 0.549] [G acc: 0.016]\n",
      "4715 [D loss: 0.499(R 0.640, F0.357)] [D acc: 0.789(R 0.625, F 0.953)] [G loss: 0.499] [G acc: 0.031]\n",
      "4716 [D loss: 0.454(R 0.542, F0.367)] [D acc: 0.797(R 0.719, F 0.875)] [G loss: 0.454] [G acc: 0.156]\n",
      "4717 [D loss: 0.432(R 0.488, F0.376)] [D acc: 0.789(R 0.688, F 0.891)] [G loss: 0.432] [G acc: 0.078]\n",
      "4718 [D loss: 0.472(R 0.378, F0.566)] [D acc: 0.789(R 0.781, F 0.797)] [G loss: 0.472] [G acc: 0.094]\n",
      "4719 [D loss: 0.531(R 0.632, F0.430)] [D acc: 0.711(R 0.578, F 0.844)] [G loss: 0.531] [G acc: 0.141]\n",
      "4720 [D loss: 0.477(R 0.388, F0.566)] [D acc: 0.789(R 0.781, F 0.797)] [G loss: 0.477] [G acc: 0.109]\n",
      "4721 [D loss: 0.425(R 0.384, F0.465)] [D acc: 0.766(R 0.766, F 0.766)] [G loss: 0.425] [G acc: 0.094]\n",
      "4722 [D loss: 0.462(R 0.503, F0.421)] [D acc: 0.766(R 0.688, F 0.844)] [G loss: 0.462] [G acc: 0.141]\n",
      "4723 [D loss: 0.510(R 0.423, F0.597)] [D acc: 0.750(R 0.734, F 0.766)] [G loss: 0.510] [G acc: 0.031]\n",
      "4724 [D loss: 0.481(R 0.527, F0.435)] [D acc: 0.781(R 0.734, F 0.828)] [G loss: 0.481] [G acc: 0.016]\n",
      "4725 [D loss: 0.587(R 0.726, F0.448)] [D acc: 0.680(R 0.562, F 0.797)] [G loss: 0.587] [G acc: 0.047]\n",
      "4726 [D loss: 0.463(R 0.504, F0.422)] [D acc: 0.805(R 0.734, F 0.875)] [G loss: 0.463] [G acc: 0.094]\n",
      "4727 [D loss: 0.515(R 0.515, F0.515)] [D acc: 0.766(R 0.719, F 0.812)] [G loss: 0.515] [G acc: 0.047]\n",
      "4728 [D loss: 0.579(R 0.608, F0.549)] [D acc: 0.750(R 0.672, F 0.828)] [G loss: 0.579] [G acc: 0.062]\n",
      "4729 [D loss: 0.469(R 0.511, F0.427)] [D acc: 0.781(R 0.703, F 0.859)] [G loss: 0.469] [G acc: 0.031]\n",
      "4730 [D loss: 0.488(R 0.558, F0.418)] [D acc: 0.727(R 0.641, F 0.812)] [G loss: 0.488] [G acc: 0.062]\n",
      "4731 [D loss: 0.456(R 0.441, F0.471)] [D acc: 0.758(R 0.703, F 0.812)] [G loss: 0.456] [G acc: 0.094]\n",
      "4732 [D loss: 0.507(R 0.463, F0.550)] [D acc: 0.773(R 0.766, F 0.781)] [G loss: 0.507] [G acc: 0.031]\n",
      "4733 [D loss: 0.478(R 0.539, F0.418)] [D acc: 0.781(R 0.672, F 0.891)] [G loss: 0.478] [G acc: 0.109]\n",
      "4734 [D loss: 0.550(R 0.494, F0.606)] [D acc: 0.719(R 0.688, F 0.750)] [G loss: 0.550] [G acc: 0.094]\n",
      "4735 [D loss: 0.540(R 0.704, F0.375)] [D acc: 0.781(R 0.609, F 0.953)] [G loss: 0.540] [G acc: 0.172]\n",
      "4736 [D loss: 0.540(R 0.426, F0.653)] [D acc: 0.742(R 0.781, F 0.703)] [G loss: 0.540] [G acc: 0.078]\n",
      "4737 [D loss: 0.584(R 0.655, F0.512)] [D acc: 0.695(R 0.594, F 0.797)] [G loss: 0.584] [G acc: 0.141]\n",
      "4738 [D loss: 0.456(R 0.515, F0.397)] [D acc: 0.797(R 0.703, F 0.891)] [G loss: 0.456] [G acc: 0.125]\n",
      "4739 [D loss: 0.434(R 0.463, F0.404)] [D acc: 0.797(R 0.672, F 0.922)] [G loss: 0.434] [G acc: 0.062]\n",
      "4740 [D loss: 0.537(R 0.509, F0.566)] [D acc: 0.727(R 0.672, F 0.781)] [G loss: 0.537] [G acc: 0.094]\n",
      "4741 [D loss: 0.563(R 0.538, F0.587)] [D acc: 0.672(R 0.672, F 0.672)] [G loss: 0.563] [G acc: 0.062]\n",
      "4742 [D loss: 0.455(R 0.490, F0.420)] [D acc: 0.750(R 0.656, F 0.844)] [G loss: 0.455] [G acc: 0.094]\n",
      "4743 [D loss: 0.565(R 0.437, F0.694)] [D acc: 0.773(R 0.750, F 0.797)] [G loss: 0.565] [G acc: 0.109]\n",
      "4744 [D loss: 0.602(R 0.720, F0.485)] [D acc: 0.711(R 0.594, F 0.828)] [G loss: 0.602] [G acc: 0.094]\n",
      "4745 [D loss: 0.488(R 0.497, F0.479)] [D acc: 0.805(R 0.781, F 0.828)] [G loss: 0.488] [G acc: 0.047]\n",
      "4746 [D loss: 0.644(R 0.558, F0.729)] [D acc: 0.672(R 0.625, F 0.719)] [G loss: 0.644] [G acc: 0.047]\n",
      "4747 [D loss: 0.580(R 0.722, F0.439)] [D acc: 0.719(R 0.562, F 0.875)] [G loss: 0.580] [G acc: 0.078]\n",
      "4748 [D loss: 0.510(R 0.617, F0.402)] [D acc: 0.781(R 0.641, F 0.922)] [G loss: 0.510] [G acc: 0.156]\n",
      "4749 [D loss: 0.461(R 0.489, F0.432)] [D acc: 0.766(R 0.688, F 0.844)] [G loss: 0.461] [G acc: 0.078]\n",
      "4750 [D loss: 0.427(R 0.450, F0.403)] [D acc: 0.797(R 0.703, F 0.891)] [G loss: 0.427] [G acc: 0.125]\n",
      "4751 [D loss: 0.509(R 0.577, F0.440)] [D acc: 0.812(R 0.703, F 0.922)] [G loss: 0.509] [G acc: 0.094]\n",
      "4752 [D loss: 0.524(R 0.418, F0.630)] [D acc: 0.750(R 0.734, F 0.766)] [G loss: 0.524] [G acc: 0.031]\n",
      "4753 [D loss: 0.452(R 0.515, F0.388)] [D acc: 0.773(R 0.688, F 0.859)] [G loss: 0.452] [G acc: 0.047]\n",
      "4754 [D loss: 0.475(R 0.406, F0.544)] [D acc: 0.773(R 0.750, F 0.797)] [G loss: 0.475] [G acc: 0.047]\n",
      "4755 [D loss: 0.441(R 0.512, F0.370)] [D acc: 0.797(R 0.719, F 0.875)] [G loss: 0.441] [G acc: 0.047]\n",
      "4756 [D loss: 0.512(R 0.541, F0.483)] [D acc: 0.727(R 0.625, F 0.828)] [G loss: 0.512] [G acc: 0.094]\n",
      "4757 [D loss: 0.506(R 0.403, F0.610)] [D acc: 0.766(R 0.750, F 0.781)] [G loss: 0.506] [G acc: 0.078]\n",
      "4758 [D loss: 0.486(R 0.603, F0.369)] [D acc: 0.742(R 0.609, F 0.875)] [G loss: 0.486] [G acc: 0.141]\n",
      "4759 [D loss: 0.385(R 0.415, F0.354)] [D acc: 0.828(R 0.750, F 0.906)] [G loss: 0.385] [G acc: 0.109]\n",
      "4760 [D loss: 0.419(R 0.376, F0.463)] [D acc: 0.820(R 0.797, F 0.844)] [G loss: 0.419] [G acc: 0.016]\n",
      "4761 [D loss: 0.583(R 0.613, F0.554)] [D acc: 0.727(R 0.672, F 0.781)] [G loss: 0.583] [G acc: 0.094]\n",
      "4762 [D loss: 0.460(R 0.502, F0.418)] [D acc: 0.766(R 0.719, F 0.812)] [G loss: 0.460] [G acc: 0.016]\n",
      "4763 [D loss: 0.488(R 0.532, F0.445)] [D acc: 0.766(R 0.703, F 0.828)] [G loss: 0.488] [G acc: 0.078]\n",
      "4764 [D loss: 0.608(R 0.688, F0.527)] [D acc: 0.688(R 0.594, F 0.781)] [G loss: 0.608] [G acc: 0.094]\n",
      "4765 [D loss: 0.462(R 0.490, F0.435)] [D acc: 0.812(R 0.719, F 0.906)] [G loss: 0.462] [G acc: 0.078]\n",
      "4766 [D loss: 0.444(R 0.415, F0.473)] [D acc: 0.734(R 0.703, F 0.766)] [G loss: 0.444] [G acc: 0.062]\n",
      "4767 [D loss: 0.505(R 0.523, F0.487)] [D acc: 0.773(R 0.688, F 0.859)] [G loss: 0.505] [G acc: 0.062]\n",
      "4768 [D loss: 0.470(R 0.567, F0.373)] [D acc: 0.766(R 0.641, F 0.891)] [G loss: 0.470] [G acc: 0.109]\n",
      "4769 [D loss: 0.433(R 0.493, F0.373)] [D acc: 0.828(R 0.766, F 0.891)] [G loss: 0.433] [G acc: 0.172]\n",
      "4770 [D loss: 0.434(R 0.418, F0.450)] [D acc: 0.805(R 0.797, F 0.812)] [G loss: 0.434] [G acc: 0.125]\n",
      "4771 [D loss: 0.543(R 0.420, F0.665)] [D acc: 0.734(R 0.766, F 0.703)] [G loss: 0.543] [G acc: 0.109]\n",
      "4772 [D loss: 0.635(R 0.646, F0.624)] [D acc: 0.695(R 0.625, F 0.766)] [G loss: 0.635] [G acc: 0.125]\n",
      "4773 [D loss: 0.441(R 0.469, F0.414)] [D acc: 0.789(R 0.734, F 0.844)] [G loss: 0.441] [G acc: 0.125]\n",
      "4774 [D loss: 0.438(R 0.508, F0.367)] [D acc: 0.781(R 0.688, F 0.875)] [G loss: 0.438] [G acc: 0.094]\n",
      "4775 [D loss: 0.541(R 0.387, F0.695)] [D acc: 0.820(R 0.797, F 0.844)] [G loss: 0.541] [G acc: 0.125]\n",
      "4776 [D loss: 0.515(R 0.555, F0.475)] [D acc: 0.719(R 0.609, F 0.828)] [G loss: 0.515] [G acc: 0.062]\n",
      "4777 [D loss: 0.408(R 0.490, F0.326)] [D acc: 0.797(R 0.688, F 0.906)] [G loss: 0.408] [G acc: 0.047]\n",
      "4778 [D loss: 0.564(R 0.569, F0.560)] [D acc: 0.758(R 0.719, F 0.797)] [G loss: 0.564] [G acc: 0.094]\n",
      "4779 [D loss: 0.481(R 0.497, F0.465)] [D acc: 0.766(R 0.719, F 0.812)] [G loss: 0.481] [G acc: 0.078]\n",
      "4780 [D loss: 0.477(R 0.483, F0.472)] [D acc: 0.758(R 0.703, F 0.812)] [G loss: 0.477] [G acc: 0.078]\n",
      "4781 [D loss: 0.481(R 0.476, F0.486)] [D acc: 0.781(R 0.750, F 0.812)] [G loss: 0.481] [G acc: 0.094]\n",
      "4782 [D loss: 0.539(R 0.550, F0.528)] [D acc: 0.703(R 0.641, F 0.766)] [G loss: 0.539] [G acc: 0.078]\n",
      "4783 [D loss: 0.485(R 0.550, F0.421)] [D acc: 0.758(R 0.656, F 0.859)] [G loss: 0.485] [G acc: 0.078]\n",
      "4784 [D loss: 0.531(R 0.489, F0.573)] [D acc: 0.734(R 0.703, F 0.766)] [G loss: 0.531] [G acc: 0.078]\n",
      "4785 [D loss: 0.490(R 0.609, F0.372)] [D acc: 0.781(R 0.641, F 0.922)] [G loss: 0.490] [G acc: 0.047]\n",
      "4786 [D loss: 0.535(R 0.533, F0.536)] [D acc: 0.695(R 0.625, F 0.766)] [G loss: 0.535] [G acc: 0.031]\n",
      "4787 [D loss: 0.466(R 0.509, F0.422)] [D acc: 0.805(R 0.750, F 0.859)] [G loss: 0.466] [G acc: 0.109]\n",
      "4788 [D loss: 0.525(R 0.448, F0.602)] [D acc: 0.812(R 0.766, F 0.859)] [G loss: 0.525] [G acc: 0.062]\n",
      "4789 [D loss: 0.491(R 0.486, F0.496)] [D acc: 0.766(R 0.688, F 0.844)] [G loss: 0.491] [G acc: 0.047]\n",
      "4790 [D loss: 0.480(R 0.531, F0.430)] [D acc: 0.773(R 0.703, F 0.844)] [G loss: 0.480] [G acc: 0.078]\n",
      "4791 [D loss: 0.534(R 0.445, F0.622)] [D acc: 0.758(R 0.766, F 0.750)] [G loss: 0.534] [G acc: 0.031]\n",
      "4792 [D loss: 0.427(R 0.552, F0.303)] [D acc: 0.812(R 0.672, F 0.953)] [G loss: 0.427] [G acc: 0.047]\n",
      "4793 [D loss: 0.496(R 0.536, F0.457)] [D acc: 0.758(R 0.703, F 0.812)] [G loss: 0.496] [G acc: 0.016]\n",
      "4794 [D loss: 0.654(R 0.594, F0.713)] [D acc: 0.656(R 0.688, F 0.625)] [G loss: 0.654] [G acc: 0.016]\n",
      "4795 [D loss: 0.549(R 0.680, F0.419)] [D acc: 0.719(R 0.547, F 0.891)] [G loss: 0.549] [G acc: 0.156]\n",
      "4796 [D loss: 0.509(R 0.472, F0.545)] [D acc: 0.750(R 0.734, F 0.766)] [G loss: 0.509] [G acc: 0.047]\n",
      "4797 [D loss: 0.512(R 0.536, F0.488)] [D acc: 0.789(R 0.688, F 0.891)] [G loss: 0.512] [G acc: 0.125]\n",
      "4798 [D loss: 0.498(R 0.483, F0.514)] [D acc: 0.719(R 0.688, F 0.750)] [G loss: 0.498] [G acc: 0.125]\n",
      "4799 [D loss: 0.534(R 0.507, F0.562)] [D acc: 0.766(R 0.703, F 0.828)] [G loss: 0.534] [G acc: 0.062]\n",
      "INFO:tensorflow:Assets written to: ram://e60f6b58-550a-41ff-b4e1-038139cd9209/assets\n",
      "INFO:tensorflow:Assets written to: ram://ce51ff3c-6963-46b1-83c1-e0f3011449d0/assets\n",
      "INFO:tensorflow:Assets written to: ram://822f365a-b244-4c0b-88a8-60b8c5c11d2b/assets\n",
      "4800 [D loss: 0.516(R 0.645, F0.387)] [D acc: 0.773(R 0.656, F 0.891)] [G loss: 0.516] [G acc: 0.078]\n",
      "4801 [D loss: 0.504(R 0.517, F0.491)] [D acc: 0.781(R 0.750, F 0.812)] [G loss: 0.504] [G acc: 0.047]\n",
      "4802 [D loss: 0.565(R 0.710, F0.419)] [D acc: 0.711(R 0.516, F 0.906)] [G loss: 0.565] [G acc: 0.125]\n",
      "4803 [D loss: 0.532(R 0.524, F0.539)] [D acc: 0.734(R 0.703, F 0.766)] [G loss: 0.532] [G acc: 0.109]\n",
      "4804 [D loss: 0.560(R 0.584, F0.536)] [D acc: 0.695(R 0.625, F 0.766)] [G loss: 0.560] [G acc: 0.109]\n",
      "4805 [D loss: 0.464(R 0.513, F0.415)] [D acc: 0.773(R 0.688, F 0.859)] [G loss: 0.464] [G acc: 0.125]\n",
      "4806 [D loss: 0.581(R 0.666, F0.497)] [D acc: 0.727(R 0.625, F 0.828)] [G loss: 0.581] [G acc: 0.031]\n",
      "4807 [D loss: 0.410(R 0.391, F0.428)] [D acc: 0.828(R 0.797, F 0.859)] [G loss: 0.410] [G acc: 0.062]\n",
      "4808 [D loss: 0.531(R 0.612, F0.450)] [D acc: 0.750(R 0.641, F 0.859)] [G loss: 0.531] [G acc: 0.000]\n",
      "4809 [D loss: 0.453(R 0.427, F0.480)] [D acc: 0.797(R 0.781, F 0.812)] [G loss: 0.453] [G acc: 0.062]\n",
      "4810 [D loss: 0.509(R 0.454, F0.564)] [D acc: 0.797(R 0.812, F 0.781)] [G loss: 0.509] [G acc: 0.031]\n",
      "4811 [D loss: 0.521(R 0.653, F0.389)] [D acc: 0.727(R 0.594, F 0.859)] [G loss: 0.521] [G acc: 0.078]\n",
      "4812 [D loss: 0.456(R 0.503, F0.408)] [D acc: 0.758(R 0.656, F 0.859)] [G loss: 0.456] [G acc: 0.078]\n",
      "4813 [D loss: 0.554(R 0.707, F0.400)] [D acc: 0.695(R 0.547, F 0.844)] [G loss: 0.554] [G acc: 0.062]\n",
      "4814 [D loss: 0.593(R 0.414, F0.771)] [D acc: 0.719(R 0.781, F 0.656)] [G loss: 0.593] [G acc: 0.047]\n",
      "4815 [D loss: 0.600(R 0.575, F0.625)] [D acc: 0.695(R 0.672, F 0.719)] [G loss: 0.600] [G acc: 0.094]\n",
      "4816 [D loss: 0.511(R 0.668, F0.354)] [D acc: 0.805(R 0.656, F 0.953)] [G loss: 0.511] [G acc: 0.078]\n",
      "4817 [D loss: 0.512(R 0.505, F0.520)] [D acc: 0.734(R 0.703, F 0.766)] [G loss: 0.512] [G acc: 0.109]\n",
      "4818 [D loss: 0.643(R 0.531, F0.755)] [D acc: 0.664(R 0.703, F 0.625)] [G loss: 0.643] [G acc: 0.047]\n",
      "4819 [D loss: 0.467(R 0.537, F0.397)] [D acc: 0.812(R 0.719, F 0.906)] [G loss: 0.467] [G acc: 0.078]\n",
      "4820 [D loss: 0.466(R 0.524, F0.408)] [D acc: 0.766(R 0.688, F 0.844)] [G loss: 0.466] [G acc: 0.047]\n",
      "4821 [D loss: 0.505(R 0.543, F0.467)] [D acc: 0.742(R 0.688, F 0.797)] [G loss: 0.505] [G acc: 0.031]\n",
      "4822 [D loss: 0.428(R 0.452, F0.403)] [D acc: 0.836(R 0.750, F 0.922)] [G loss: 0.428] [G acc: 0.047]\n",
      "4823 [D loss: 0.501(R 0.558, F0.444)] [D acc: 0.719(R 0.609, F 0.828)] [G loss: 0.501] [G acc: 0.094]\n",
      "4824 [D loss: 0.408(R 0.482, F0.333)] [D acc: 0.773(R 0.656, F 0.891)] [G loss: 0.408] [G acc: 0.125]\n",
      "4825 [D loss: 0.482(R 0.449, F0.515)] [D acc: 0.742(R 0.703, F 0.781)] [G loss: 0.482] [G acc: 0.062]\n",
      "4826 [D loss: 0.597(R 0.661, F0.532)] [D acc: 0.695(R 0.578, F 0.812)] [G loss: 0.597] [G acc: 0.062]\n",
      "4827 [D loss: 0.556(R 0.670, F0.441)] [D acc: 0.711(R 0.609, F 0.812)] [G loss: 0.556] [G acc: 0.078]\n",
      "4828 [D loss: 0.559(R 0.591, F0.526)] [D acc: 0.703(R 0.656, F 0.750)] [G loss: 0.559] [G acc: 0.047]\n",
      "4829 [D loss: 0.565(R 0.579, F0.552)] [D acc: 0.758(R 0.672, F 0.844)] [G loss: 0.565] [G acc: 0.109]\n",
      "4830 [D loss: 0.445(R 0.478, F0.411)] [D acc: 0.805(R 0.750, F 0.859)] [G loss: 0.445] [G acc: 0.109]\n",
      "4831 [D loss: 0.606(R 0.641, F0.571)] [D acc: 0.664(R 0.578, F 0.750)] [G loss: 0.606] [G acc: 0.125]\n",
      "4832 [D loss: 0.526(R 0.464, F0.588)] [D acc: 0.742(R 0.719, F 0.766)] [G loss: 0.526] [G acc: 0.031]\n",
      "4833 [D loss: 0.467(R 0.472, F0.463)] [D acc: 0.828(R 0.766, F 0.891)] [G loss: 0.467] [G acc: 0.094]\n",
      "4834 [D loss: 0.438(R 0.401, F0.476)] [D acc: 0.812(R 0.812, F 0.812)] [G loss: 0.438] [G acc: 0.062]\n",
      "4835 [D loss: 0.619(R 0.689, F0.550)] [D acc: 0.695(R 0.641, F 0.750)] [G loss: 0.619] [G acc: 0.094]\n",
      "4836 [D loss: 0.446(R 0.516, F0.375)] [D acc: 0.773(R 0.625, F 0.922)] [G loss: 0.446] [G acc: 0.031]\n",
      "4837 [D loss: 0.522(R 0.601, F0.444)] [D acc: 0.695(R 0.578, F 0.812)] [G loss: 0.522] [G acc: 0.172]\n",
      "4838 [D loss: 0.541(R 0.416, F0.666)] [D acc: 0.805(R 0.828, F 0.781)] [G loss: 0.541] [G acc: 0.094]\n",
      "4839 [D loss: 0.413(R 0.445, F0.381)] [D acc: 0.820(R 0.766, F 0.875)] [G loss: 0.413] [G acc: 0.141]\n",
      "4840 [D loss: 0.529(R 0.469, F0.590)] [D acc: 0.695(R 0.703, F 0.688)] [G loss: 0.529] [G acc: 0.094]\n",
      "4841 [D loss: 0.441(R 0.483, F0.398)] [D acc: 0.805(R 0.750, F 0.859)] [G loss: 0.441] [G acc: 0.141]\n",
      "4842 [D loss: 0.565(R 0.587, F0.544)] [D acc: 0.727(R 0.656, F 0.797)] [G loss: 0.565] [G acc: 0.109]\n",
      "4843 [D loss: 0.389(R 0.414, F0.365)] [D acc: 0.836(R 0.750, F 0.922)] [G loss: 0.389] [G acc: 0.125]\n",
      "4844 [D loss: 0.501(R 0.579, F0.422)] [D acc: 0.766(R 0.625, F 0.906)] [G loss: 0.501] [G acc: 0.047]\n",
      "4845 [D loss: 0.557(R 0.504, F0.610)] [D acc: 0.719(R 0.656, F 0.781)] [G loss: 0.557] [G acc: 0.094]\n",
      "4846 [D loss: 0.588(R 0.681, F0.495)] [D acc: 0.719(R 0.625, F 0.812)] [G loss: 0.588] [G acc: 0.078]\n",
      "4847 [D loss: 0.493(R 0.485, F0.500)] [D acc: 0.742(R 0.672, F 0.812)] [G loss: 0.493] [G acc: 0.125]\n",
      "4848 [D loss: 0.461(R 0.456, F0.467)] [D acc: 0.758(R 0.719, F 0.797)] [G loss: 0.461] [G acc: 0.078]\n",
      "4849 [D loss: 0.444(R 0.436, F0.452)] [D acc: 0.797(R 0.719, F 0.875)] [G loss: 0.444] [G acc: 0.094]\n",
      "4850 [D loss: 0.474(R 0.519, F0.428)] [D acc: 0.750(R 0.641, F 0.859)] [G loss: 0.474] [G acc: 0.094]\n",
      "4851 [D loss: 0.570(R 0.525, F0.616)] [D acc: 0.750(R 0.672, F 0.828)] [G loss: 0.570] [G acc: 0.125]\n",
      "4852 [D loss: 0.527(R 0.630, F0.423)] [D acc: 0.727(R 0.594, F 0.859)] [G loss: 0.527] [G acc: 0.047]\n",
      "4853 [D loss: 0.493(R 0.508, F0.478)] [D acc: 0.742(R 0.656, F 0.828)] [G loss: 0.493] [G acc: 0.141]\n",
      "4854 [D loss: 0.510(R 0.522, F0.497)] [D acc: 0.742(R 0.672, F 0.812)] [G loss: 0.510] [G acc: 0.062]\n",
      "4855 [D loss: 0.548(R 0.538, F0.557)] [D acc: 0.727(R 0.719, F 0.734)] [G loss: 0.548] [G acc: 0.125]\n",
      "4856 [D loss: 0.482(R 0.471, F0.493)] [D acc: 0.766(R 0.719, F 0.812)] [G loss: 0.482] [G acc: 0.062]\n",
      "4857 [D loss: 0.497(R 0.542, F0.452)] [D acc: 0.758(R 0.672, F 0.844)] [G loss: 0.497] [G acc: 0.125]\n",
      "4858 [D loss: 0.508(R 0.465, F0.551)] [D acc: 0.750(R 0.719, F 0.781)] [G loss: 0.508] [G acc: 0.141]\n",
      "4859 [D loss: 0.451(R 0.490, F0.412)] [D acc: 0.789(R 0.688, F 0.891)] [G loss: 0.451] [G acc: 0.156]\n",
      "4860 [D loss: 0.565(R 0.564, F0.566)] [D acc: 0.703(R 0.641, F 0.766)] [G loss: 0.565] [G acc: 0.047]\n",
      "4861 [D loss: 0.595(R 0.497, F0.693)] [D acc: 0.711(R 0.719, F 0.703)] [G loss: 0.595] [G acc: 0.031]\n",
      "4862 [D loss: 0.454(R 0.477, F0.431)] [D acc: 0.789(R 0.750, F 0.828)] [G loss: 0.454] [G acc: 0.109]\n",
      "4863 [D loss: 0.474(R 0.423, F0.525)] [D acc: 0.773(R 0.781, F 0.766)] [G loss: 0.474] [G acc: 0.047]\n",
      "4864 [D loss: 0.595(R 0.548, F0.642)] [D acc: 0.711(R 0.641, F 0.781)] [G loss: 0.595] [G acc: 0.047]\n",
      "4865 [D loss: 0.466(R 0.523, F0.408)] [D acc: 0.766(R 0.688, F 0.844)] [G loss: 0.466] [G acc: 0.141]\n",
      "4866 [D loss: 0.565(R 0.443, F0.687)] [D acc: 0.727(R 0.750, F 0.703)] [G loss: 0.565] [G acc: 0.078]\n",
      "4867 [D loss: 0.531(R 0.521, F0.540)] [D acc: 0.719(R 0.656, F 0.781)] [G loss: 0.531] [G acc: 0.094]\n",
      "4868 [D loss: 0.497(R 0.619, F0.375)] [D acc: 0.758(R 0.641, F 0.875)] [G loss: 0.497] [G acc: 0.078]\n",
      "4869 [D loss: 0.477(R 0.488, F0.466)] [D acc: 0.758(R 0.672, F 0.844)] [G loss: 0.477] [G acc: 0.047]\n",
      "4870 [D loss: 0.536(R 0.653, F0.420)] [D acc: 0.734(R 0.609, F 0.859)] [G loss: 0.536] [G acc: 0.062]\n",
      "4871 [D loss: 0.506(R 0.386, F0.626)] [D acc: 0.812(R 0.828, F 0.797)] [G loss: 0.506] [G acc: 0.062]\n",
      "4872 [D loss: 0.553(R 0.638, F0.467)] [D acc: 0.703(R 0.578, F 0.828)] [G loss: 0.553] [G acc: 0.172]\n",
      "4873 [D loss: 0.534(R 0.588, F0.479)] [D acc: 0.727(R 0.641, F 0.812)] [G loss: 0.534] [G acc: 0.141]\n",
      "4874 [D loss: 0.538(R 0.561, F0.515)] [D acc: 0.727(R 0.719, F 0.734)] [G loss: 0.538] [G acc: 0.062]\n",
      "4875 [D loss: 0.492(R 0.394, F0.590)] [D acc: 0.781(R 0.781, F 0.781)] [G loss: 0.492] [G acc: 0.094]\n",
      "4876 [D loss: 0.547(R 0.542, F0.552)] [D acc: 0.742(R 0.656, F 0.828)] [G loss: 0.547] [G acc: 0.062]\n",
      "4877 [D loss: 0.558(R 0.626, F0.489)] [D acc: 0.695(R 0.578, F 0.812)] [G loss: 0.558] [G acc: 0.047]\n",
      "4878 [D loss: 0.498(R 0.535, F0.460)] [D acc: 0.758(R 0.688, F 0.828)] [G loss: 0.498] [G acc: 0.125]\n",
      "4879 [D loss: 0.444(R 0.483, F0.406)] [D acc: 0.773(R 0.719, F 0.828)] [G loss: 0.444] [G acc: 0.062]\n",
      "4880 [D loss: 0.541(R 0.495, F0.588)] [D acc: 0.711(R 0.688, F 0.734)] [G loss: 0.541] [G acc: 0.109]\n",
      "4881 [D loss: 0.524(R 0.538, F0.510)] [D acc: 0.773(R 0.719, F 0.828)] [G loss: 0.524] [G acc: 0.016]\n",
      "4882 [D loss: 0.442(R 0.442, F0.441)] [D acc: 0.828(R 0.781, F 0.875)] [G loss: 0.442] [G acc: 0.125]\n",
      "4883 [D loss: 0.474(R 0.483, F0.465)] [D acc: 0.797(R 0.750, F 0.844)] [G loss: 0.474] [G acc: 0.141]\n",
      "4884 [D loss: 0.574(R 0.561, F0.588)] [D acc: 0.711(R 0.641, F 0.781)] [G loss: 0.574] [G acc: 0.062]\n",
      "4885 [D loss: 0.539(R 0.539, F0.539)] [D acc: 0.750(R 0.656, F 0.844)] [G loss: 0.539] [G acc: 0.094]\n",
      "4886 [D loss: 0.525(R 0.539, F0.511)] [D acc: 0.750(R 0.688, F 0.812)] [G loss: 0.525] [G acc: 0.062]\n",
      "4887 [D loss: 0.514(R 0.506, F0.523)] [D acc: 0.758(R 0.703, F 0.812)] [G loss: 0.514] [G acc: 0.094]\n",
      "4888 [D loss: 0.571(R 0.581, F0.560)] [D acc: 0.727(R 0.672, F 0.781)] [G loss: 0.571] [G acc: 0.062]\n",
      "4889 [D loss: 0.474(R 0.541, F0.406)] [D acc: 0.766(R 0.672, F 0.859)] [G loss: 0.474] [G acc: 0.078]\n",
      "4890 [D loss: 0.435(R 0.434, F0.436)] [D acc: 0.797(R 0.734, F 0.859)] [G loss: 0.435] [G acc: 0.047]\n",
      "4891 [D loss: 0.568(R 0.473, F0.662)] [D acc: 0.742(R 0.656, F 0.828)] [G loss: 0.568] [G acc: 0.062]\n",
      "4892 [D loss: 0.533(R 0.585, F0.481)] [D acc: 0.711(R 0.609, F 0.812)] [G loss: 0.533] [G acc: 0.141]\n",
      "4893 [D loss: 0.447(R 0.430, F0.464)] [D acc: 0.805(R 0.781, F 0.828)] [G loss: 0.447] [G acc: 0.047]\n",
      "4894 [D loss: 0.490(R 0.495, F0.484)] [D acc: 0.742(R 0.734, F 0.750)] [G loss: 0.490] [G acc: 0.078]\n",
      "4895 [D loss: 0.541(R 0.630, F0.453)] [D acc: 0.766(R 0.672, F 0.859)] [G loss: 0.541] [G acc: 0.016]\n",
      "4896 [D loss: 0.586(R 0.581, F0.590)] [D acc: 0.688(R 0.672, F 0.703)] [G loss: 0.586] [G acc: 0.219]\n",
      "4897 [D loss: 0.478(R 0.531, F0.424)] [D acc: 0.766(R 0.672, F 0.859)] [G loss: 0.478] [G acc: 0.078]\n",
      "4898 [D loss: 0.488(R 0.400, F0.576)] [D acc: 0.766(R 0.719, F 0.812)] [G loss: 0.488] [G acc: 0.047]\n",
      "4899 [D loss: 0.494(R 0.478, F0.511)] [D acc: 0.781(R 0.719, F 0.844)] [G loss: 0.494] [G acc: 0.047]\n",
      "INFO:tensorflow:Assets written to: ram://6dbafc65-bf00-4c6e-8e76-37e35015fa87/assets\n",
      "INFO:tensorflow:Assets written to: ram://50902ebc-66bc-407a-b7cc-e13dd5e5b766/assets\n",
      "INFO:tensorflow:Assets written to: ram://7dd8be41-6bac-48b7-885b-dbbbadaeafc6/assets\n",
      "4900 [D loss: 0.468(R 0.507, F0.428)] [D acc: 0.789(R 0.688, F 0.891)] [G loss: 0.468] [G acc: 0.109]\n",
      "4901 [D loss: 0.486(R 0.497, F0.474)] [D acc: 0.742(R 0.688, F 0.797)] [G loss: 0.486] [G acc: 0.047]\n",
      "4902 [D loss: 0.507(R 0.443, F0.572)] [D acc: 0.773(R 0.719, F 0.828)] [G loss: 0.507] [G acc: 0.109]\n",
      "4903 [D loss: 0.458(R 0.380, F0.537)] [D acc: 0.828(R 0.781, F 0.875)] [G loss: 0.458] [G acc: 0.094]\n",
      "4904 [D loss: 0.496(R 0.667, F0.325)] [D acc: 0.773(R 0.656, F 0.891)] [G loss: 0.496] [G acc: 0.094]\n",
      "4905 [D loss: 0.586(R 0.601, F0.571)] [D acc: 0.750(R 0.672, F 0.828)] [G loss: 0.586] [G acc: 0.094]\n",
      "4906 [D loss: 0.509(R 0.559, F0.458)] [D acc: 0.719(R 0.609, F 0.828)] [G loss: 0.509] [G acc: 0.172]\n",
      "4907 [D loss: 0.464(R 0.451, F0.476)] [D acc: 0.797(R 0.734, F 0.859)] [G loss: 0.464] [G acc: 0.094]\n",
      "4908 [D loss: 0.452(R 0.457, F0.448)] [D acc: 0.773(R 0.750, F 0.797)] [G loss: 0.452] [G acc: 0.094]\n",
      "4909 [D loss: 0.531(R 0.556, F0.505)] [D acc: 0.734(R 0.719, F 0.750)] [G loss: 0.531] [G acc: 0.062]\n",
      "4910 [D loss: 0.524(R 0.548, F0.500)] [D acc: 0.719(R 0.656, F 0.781)] [G loss: 0.524] [G acc: 0.094]\n",
      "4911 [D loss: 0.645(R 0.588, F0.702)] [D acc: 0.711(R 0.688, F 0.734)] [G loss: 0.645] [G acc: 0.047]\n",
      "4912 [D loss: 0.574(R 0.591, F0.558)] [D acc: 0.758(R 0.688, F 0.828)] [G loss: 0.574] [G acc: 0.062]\n",
      "4913 [D loss: 0.480(R 0.526, F0.435)] [D acc: 0.773(R 0.719, F 0.828)] [G loss: 0.480] [G acc: 0.094]\n",
      "4914 [D loss: 0.492(R 0.437, F0.547)] [D acc: 0.711(R 0.719, F 0.703)] [G loss: 0.492] [G acc: 0.062]\n",
      "4915 [D loss: 0.458(R 0.493, F0.423)] [D acc: 0.766(R 0.734, F 0.797)] [G loss: 0.458] [G acc: 0.141]\n",
      "4916 [D loss: 0.568(R 0.497, F0.640)] [D acc: 0.797(R 0.781, F 0.812)] [G loss: 0.568] [G acc: 0.078]\n",
      "4917 [D loss: 0.483(R 0.652, F0.314)] [D acc: 0.719(R 0.531, F 0.906)] [G loss: 0.483] [G acc: 0.047]\n",
      "4918 [D loss: 0.538(R 0.580, F0.497)] [D acc: 0.734(R 0.672, F 0.797)] [G loss: 0.538] [G acc: 0.125]\n",
      "4919 [D loss: 0.426(R 0.316, F0.537)] [D acc: 0.820(R 0.875, F 0.766)] [G loss: 0.426] [G acc: 0.062]\n",
      "4920 [D loss: 0.583(R 0.615, F0.550)] [D acc: 0.648(R 0.594, F 0.703)] [G loss: 0.583] [G acc: 0.062]\n",
      "4921 [D loss: 0.538(R 0.579, F0.498)] [D acc: 0.742(R 0.672, F 0.812)] [G loss: 0.538] [G acc: 0.109]\n",
      "4922 [D loss: 0.549(R 0.652, F0.446)] [D acc: 0.773(R 0.656, F 0.891)] [G loss: 0.549] [G acc: 0.109]\n",
      "4923 [D loss: 0.528(R 0.502, F0.553)] [D acc: 0.695(R 0.656, F 0.734)] [G loss: 0.528] [G acc: 0.125]\n",
      "4924 [D loss: 0.586(R 0.678, F0.494)] [D acc: 0.664(R 0.516, F 0.812)] [G loss: 0.586] [G acc: 0.062]\n",
      "4925 [D loss: 0.508(R 0.574, F0.441)] [D acc: 0.734(R 0.625, F 0.844)] [G loss: 0.508] [G acc: 0.062]\n",
      "4926 [D loss: 0.544(R 0.554, F0.534)] [D acc: 0.734(R 0.688, F 0.781)] [G loss: 0.544] [G acc: 0.109]\n",
      "4927 [D loss: 0.579(R 0.661, F0.496)] [D acc: 0.680(R 0.578, F 0.781)] [G loss: 0.579] [G acc: 0.078]\n",
      "4928 [D loss: 0.465(R 0.540, F0.391)] [D acc: 0.750(R 0.641, F 0.859)] [G loss: 0.465] [G acc: 0.094]\n",
      "4929 [D loss: 0.491(R 0.488, F0.493)] [D acc: 0.734(R 0.656, F 0.812)] [G loss: 0.491] [G acc: 0.094]\n",
      "4930 [D loss: 0.560(R 0.522, F0.597)] [D acc: 0.711(R 0.719, F 0.703)] [G loss: 0.560] [G acc: 0.047]\n",
      "4931 [D loss: 0.548(R 0.621, F0.475)] [D acc: 0.711(R 0.609, F 0.812)] [G loss: 0.548] [G acc: 0.062]\n",
      "4932 [D loss: 0.458(R 0.473, F0.444)] [D acc: 0.773(R 0.688, F 0.859)] [G loss: 0.458] [G acc: 0.078]\n",
      "4933 [D loss: 0.639(R 0.717, F0.561)] [D acc: 0.727(R 0.609, F 0.844)] [G loss: 0.639] [G acc: 0.062]\n",
      "4934 [D loss: 0.498(R 0.604, F0.391)] [D acc: 0.773(R 0.641, F 0.906)] [G loss: 0.498] [G acc: 0.156]\n",
      "4935 [D loss: 0.469(R 0.387, F0.550)] [D acc: 0.773(R 0.781, F 0.766)] [G loss: 0.469] [G acc: 0.094]\n",
      "4936 [D loss: 0.455(R 0.456, F0.454)] [D acc: 0.828(R 0.781, F 0.875)] [G loss: 0.455] [G acc: 0.047]\n",
      "4937 [D loss: 0.516(R 0.513, F0.519)] [D acc: 0.680(R 0.656, F 0.703)] [G loss: 0.516] [G acc: 0.109]\n",
      "4938 [D loss: 0.539(R 0.547, F0.531)] [D acc: 0.750(R 0.688, F 0.812)] [G loss: 0.539] [G acc: 0.094]\n",
      "4939 [D loss: 0.550(R 0.633, F0.466)] [D acc: 0.711(R 0.609, F 0.812)] [G loss: 0.550] [G acc: 0.031]\n",
      "4940 [D loss: 0.466(R 0.483, F0.449)] [D acc: 0.789(R 0.688, F 0.891)] [G loss: 0.466] [G acc: 0.078]\n",
      "4941 [D loss: 0.578(R 0.602, F0.554)] [D acc: 0.703(R 0.656, F 0.750)] [G loss: 0.578] [G acc: 0.125]\n",
      "4942 [D loss: 0.561(R 0.623, F0.499)] [D acc: 0.750(R 0.625, F 0.875)] [G loss: 0.561] [G acc: 0.078]\n",
      "4943 [D loss: 0.429(R 0.462, F0.396)] [D acc: 0.805(R 0.781, F 0.828)] [G loss: 0.429] [G acc: 0.047]\n",
      "4944 [D loss: 0.453(R 0.434, F0.473)] [D acc: 0.797(R 0.766, F 0.828)] [G loss: 0.453] [G acc: 0.031]\n",
      "4945 [D loss: 0.441(R 0.437, F0.445)] [D acc: 0.797(R 0.797, F 0.797)] [G loss: 0.441] [G acc: 0.047]\n",
      "4946 [D loss: 0.566(R 0.636, F0.496)] [D acc: 0.688(R 0.578, F 0.797)] [G loss: 0.566] [G acc: 0.062]\n",
      "4947 [D loss: 0.526(R 0.570, F0.482)] [D acc: 0.734(R 0.594, F 0.875)] [G loss: 0.526] [G acc: 0.109]\n",
      "4948 [D loss: 0.457(R 0.511, F0.402)] [D acc: 0.742(R 0.656, F 0.828)] [G loss: 0.457] [G acc: 0.031]\n",
      "4949 [D loss: 0.510(R 0.563, F0.456)] [D acc: 0.750(R 0.719, F 0.781)] [G loss: 0.510] [G acc: 0.125]\n",
      "4950 [D loss: 0.550(R 0.605, F0.496)] [D acc: 0.711(R 0.625, F 0.797)] [G loss: 0.550] [G acc: 0.078]\n",
      "4951 [D loss: 0.519(R 0.493, F0.545)] [D acc: 0.727(R 0.734, F 0.719)] [G loss: 0.519] [G acc: 0.094]\n",
      "4952 [D loss: 0.507(R 0.635, F0.378)] [D acc: 0.742(R 0.578, F 0.906)] [G loss: 0.507] [G acc: 0.141]\n",
      "4953 [D loss: 0.533(R 0.482, F0.583)] [D acc: 0.719(R 0.734, F 0.703)] [G loss: 0.533] [G acc: 0.062]\n",
      "4954 [D loss: 0.497(R 0.519, F0.475)] [D acc: 0.750(R 0.672, F 0.828)] [G loss: 0.497] [G acc: 0.031]\n",
      "4955 [D loss: 0.457(R 0.500, F0.415)] [D acc: 0.766(R 0.656, F 0.875)] [G loss: 0.457] [G acc: 0.125]\n",
      "4956 [D loss: 0.483(R 0.471, F0.495)] [D acc: 0.789(R 0.734, F 0.844)] [G loss: 0.483] [G acc: 0.141]\n",
      "4957 [D loss: 0.569(R 0.477, F0.660)] [D acc: 0.719(R 0.719, F 0.719)] [G loss: 0.569] [G acc: 0.094]\n",
      "4958 [D loss: 0.629(R 0.638, F0.621)] [D acc: 0.664(R 0.594, F 0.734)] [G loss: 0.629] [G acc: 0.094]\n",
      "4959 [D loss: 0.614(R 0.671, F0.558)] [D acc: 0.688(R 0.609, F 0.766)] [G loss: 0.614] [G acc: 0.094]\n",
      "4960 [D loss: 0.461(R 0.464, F0.458)] [D acc: 0.805(R 0.766, F 0.844)] [G loss: 0.461] [G acc: 0.047]\n",
      "4961 [D loss: 0.575(R 0.519, F0.632)] [D acc: 0.695(R 0.656, F 0.734)] [G loss: 0.575] [G acc: 0.094]\n",
      "4962 [D loss: 0.508(R 0.602, F0.415)] [D acc: 0.711(R 0.594, F 0.828)] [G loss: 0.508] [G acc: 0.109]\n",
      "4963 [D loss: 0.519(R 0.488, F0.550)] [D acc: 0.750(R 0.688, F 0.812)] [G loss: 0.519] [G acc: 0.125]\n",
      "4964 [D loss: 0.550(R 0.678, F0.421)] [D acc: 0.766(R 0.656, F 0.875)] [G loss: 0.550] [G acc: 0.062]\n",
      "4965 [D loss: 0.484(R 0.469, F0.498)] [D acc: 0.805(R 0.750, F 0.859)] [G loss: 0.484] [G acc: 0.109]\n",
      "4966 [D loss: 0.493(R 0.396, F0.591)] [D acc: 0.766(R 0.797, F 0.734)] [G loss: 0.493] [G acc: 0.062]\n",
      "4967 [D loss: 0.515(R 0.660, F0.370)] [D acc: 0.758(R 0.578, F 0.938)] [G loss: 0.515] [G acc: 0.125]\n",
      "4968 [D loss: 0.545(R 0.597, F0.493)] [D acc: 0.734(R 0.656, F 0.812)] [G loss: 0.545] [G acc: 0.094]\n",
      "4969 [D loss: 0.506(R 0.533, F0.478)] [D acc: 0.766(R 0.703, F 0.828)] [G loss: 0.506] [G acc: 0.062]\n",
      "4970 [D loss: 0.535(R 0.534, F0.536)] [D acc: 0.719(R 0.641, F 0.797)] [G loss: 0.535] [G acc: 0.094]\n",
      "4971 [D loss: 0.513(R 0.580, F0.446)] [D acc: 0.750(R 0.656, F 0.844)] [G loss: 0.513] [G acc: 0.062]\n",
      "4972 [D loss: 0.502(R 0.464, F0.539)] [D acc: 0.766(R 0.734, F 0.797)] [G loss: 0.502] [G acc: 0.078]\n",
      "4973 [D loss: 0.476(R 0.515, F0.436)] [D acc: 0.789(R 0.703, F 0.875)] [G loss: 0.476] [G acc: 0.094]\n",
      "4974 [D loss: 0.551(R 0.553, F0.548)] [D acc: 0.719(R 0.656, F 0.781)] [G loss: 0.551] [G acc: 0.141]\n",
      "4975 [D loss: 0.503(R 0.473, F0.532)] [D acc: 0.766(R 0.750, F 0.781)] [G loss: 0.503] [G acc: 0.031]\n",
      "4976 [D loss: 0.630(R 0.765, F0.496)] [D acc: 0.672(R 0.500, F 0.844)] [G loss: 0.630] [G acc: 0.062]\n",
      "4977 [D loss: 0.507(R 0.596, F0.419)] [D acc: 0.727(R 0.641, F 0.812)] [G loss: 0.507] [G acc: 0.031]\n",
      "4978 [D loss: 0.504(R 0.564, F0.445)] [D acc: 0.758(R 0.672, F 0.844)] [G loss: 0.504] [G acc: 0.062]\n",
      "4979 [D loss: 0.613(R 0.593, F0.634)] [D acc: 0.672(R 0.594, F 0.750)] [G loss: 0.613] [G acc: 0.047]\n",
      "4980 [D loss: 0.552(R 0.569, F0.534)] [D acc: 0.742(R 0.641, F 0.844)] [G loss: 0.552] [G acc: 0.156]\n",
      "4981 [D loss: 0.680(R 0.557, F0.803)] [D acc: 0.656(R 0.656, F 0.656)] [G loss: 0.680] [G acc: 0.047]\n",
      "4982 [D loss: 0.615(R 0.784, F0.445)] [D acc: 0.625(R 0.438, F 0.812)] [G loss: 0.615] [G acc: 0.078]\n",
      "4983 [D loss: 0.455(R 0.475, F0.434)] [D acc: 0.750(R 0.641, F 0.859)] [G loss: 0.455] [G acc: 0.094]\n",
      "4984 [D loss: 0.472(R 0.446, F0.498)] [D acc: 0.828(R 0.812, F 0.844)] [G loss: 0.472] [G acc: 0.078]\n",
      "4985 [D loss: 0.469(R 0.444, F0.495)] [D acc: 0.742(R 0.688, F 0.797)] [G loss: 0.469] [G acc: 0.109]\n",
      "4986 [D loss: 0.470(R 0.570, F0.370)] [D acc: 0.773(R 0.641, F 0.906)] [G loss: 0.470] [G acc: 0.109]\n",
      "4987 [D loss: 0.516(R 0.466, F0.566)] [D acc: 0.742(R 0.719, F 0.766)] [G loss: 0.516] [G acc: 0.078]\n",
      "4988 [D loss: 0.543(R 0.624, F0.462)] [D acc: 0.727(R 0.547, F 0.906)] [G loss: 0.543] [G acc: 0.109]\n",
      "4989 [D loss: 0.534(R 0.496, F0.572)] [D acc: 0.695(R 0.656, F 0.734)] [G loss: 0.534] [G acc: 0.047]\n",
      "4990 [D loss: 0.510(R 0.628, F0.392)] [D acc: 0.766(R 0.625, F 0.906)] [G loss: 0.510] [G acc: 0.078]\n",
      "4991 [D loss: 0.565(R 0.580, F0.550)] [D acc: 0.719(R 0.641, F 0.797)] [G loss: 0.565] [G acc: 0.094]\n",
      "4992 [D loss: 0.528(R 0.515, F0.540)] [D acc: 0.711(R 0.672, F 0.750)] [G loss: 0.528] [G acc: 0.125]\n",
      "4993 [D loss: 0.586(R 0.542, F0.631)] [D acc: 0.734(R 0.672, F 0.797)] [G loss: 0.586] [G acc: 0.141]\n",
      "4994 [D loss: 0.431(R 0.438, F0.424)] [D acc: 0.797(R 0.734, F 0.859)] [G loss: 0.431] [G acc: 0.125]\n",
      "4995 [D loss: 0.526(R 0.474, F0.579)] [D acc: 0.703(R 0.688, F 0.719)] [G loss: 0.526] [G acc: 0.047]\n",
      "4996 [D loss: 0.559(R 0.641, F0.478)] [D acc: 0.719(R 0.578, F 0.859)] [G loss: 0.559] [G acc: 0.094]\n",
      "4997 [D loss: 0.561(R 0.497, F0.625)] [D acc: 0.781(R 0.734, F 0.828)] [G loss: 0.561] [G acc: 0.094]\n",
      "4998 [D loss: 0.573(R 0.574, F0.573)] [D acc: 0.711(R 0.641, F 0.781)] [G loss: 0.573] [G acc: 0.016]\n",
      "4999 [D loss: 0.588(R 0.741, F0.435)] [D acc: 0.734(R 0.609, F 0.859)] [G loss: 0.588] [G acc: 0.016]\n",
      "INFO:tensorflow:Assets written to: ram://5b53520d-222a-4b84-aba1-7214b67430ba/assets\n",
      "INFO:tensorflow:Assets written to: ram://16ef36b1-e0cb-40b3-9cc4-fbc6dc773260/assets\n",
      "INFO:tensorflow:Assets written to: ram://fd40e1ea-3741-4779-bb04-8db50560d178/assets\n",
      "5000 [D loss: 0.529(R 0.548, F0.510)] [D acc: 0.742(R 0.656, F 0.828)] [G loss: 0.529] [G acc: 0.078]\n",
      "5001 [D loss: 0.408(R 0.373, F0.443)] [D acc: 0.812(R 0.797, F 0.828)] [G loss: 0.408] [G acc: 0.125]\n",
      "5002 [D loss: 0.422(R 0.409, F0.435)] [D acc: 0.789(R 0.734, F 0.844)] [G loss: 0.422] [G acc: 0.078]\n",
      "5003 [D loss: 0.525(R 0.377, F0.672)] [D acc: 0.750(R 0.750, F 0.750)] [G loss: 0.525] [G acc: 0.094]\n",
      "5004 [D loss: 0.476(R 0.490, F0.463)] [D acc: 0.758(R 0.703, F 0.812)] [G loss: 0.476] [G acc: 0.172]\n",
      "5005 [D loss: 0.420(R 0.474, F0.366)] [D acc: 0.781(R 0.656, F 0.906)] [G loss: 0.420] [G acc: 0.078]\n",
      "5006 [D loss: 0.473(R 0.469, F0.478)] [D acc: 0.727(R 0.688, F 0.766)] [G loss: 0.473] [G acc: 0.078]\n",
      "5007 [D loss: 0.492(R 0.547, F0.438)] [D acc: 0.758(R 0.688, F 0.828)] [G loss: 0.492] [G acc: 0.062]\n",
      "5008 [D loss: 0.451(R 0.530, F0.373)] [D acc: 0.812(R 0.688, F 0.938)] [G loss: 0.451] [G acc: 0.094]\n",
      "5009 [D loss: 0.525(R 0.480, F0.570)] [D acc: 0.750(R 0.734, F 0.766)] [G loss: 0.525] [G acc: 0.062]\n",
      "5010 [D loss: 0.574(R 0.548, F0.599)] [D acc: 0.711(R 0.641, F 0.781)] [G loss: 0.574] [G acc: 0.062]\n",
      "5011 [D loss: 0.473(R 0.573, F0.373)] [D acc: 0.789(R 0.703, F 0.875)] [G loss: 0.473] [G acc: 0.078]\n",
      "5012 [D loss: 0.390(R 0.347, F0.432)] [D acc: 0.852(R 0.812, F 0.891)] [G loss: 0.390] [G acc: 0.125]\n",
      "5013 [D loss: 0.517(R 0.554, F0.480)] [D acc: 0.758(R 0.703, F 0.812)] [G loss: 0.517] [G acc: 0.141]\n",
      "5014 [D loss: 0.388(R 0.342, F0.434)] [D acc: 0.812(R 0.781, F 0.844)] [G loss: 0.388] [G acc: 0.109]\n",
      "5015 [D loss: 0.571(R 0.595, F0.546)] [D acc: 0.727(R 0.641, F 0.812)] [G loss: 0.571] [G acc: 0.078]\n",
      "5016 [D loss: 0.485(R 0.617, F0.354)] [D acc: 0.758(R 0.641, F 0.875)] [G loss: 0.485] [G acc: 0.125]\n",
      "5017 [D loss: 0.502(R 0.524, F0.480)] [D acc: 0.711(R 0.672, F 0.750)] [G loss: 0.502] [G acc: 0.109]\n",
      "5018 [D loss: 0.469(R 0.365, F0.572)] [D acc: 0.836(R 0.812, F 0.859)] [G loss: 0.469] [G acc: 0.109]\n",
      "5019 [D loss: 0.445(R 0.376, F0.515)] [D acc: 0.781(R 0.797, F 0.766)] [G loss: 0.445] [G acc: 0.094]\n",
      "5020 [D loss: 0.673(R 0.760, F0.587)] [D acc: 0.664(R 0.594, F 0.734)] [G loss: 0.673] [G acc: 0.125]\n",
      "5021 [D loss: 0.582(R 0.671, F0.493)] [D acc: 0.688(R 0.594, F 0.781)] [G loss: 0.582] [G acc: 0.125]\n",
      "5022 [D loss: 0.518(R 0.488, F0.549)] [D acc: 0.719(R 0.656, F 0.781)] [G loss: 0.518] [G acc: 0.062]\n",
      "5023 [D loss: 0.495(R 0.548, F0.441)] [D acc: 0.719(R 0.641, F 0.797)] [G loss: 0.495] [G acc: 0.031]\n",
      "5024 [D loss: 0.471(R 0.511, F0.431)] [D acc: 0.758(R 0.672, F 0.844)] [G loss: 0.471] [G acc: 0.141]\n",
      "5025 [D loss: 0.435(R 0.391, F0.479)] [D acc: 0.781(R 0.766, F 0.797)] [G loss: 0.435] [G acc: 0.125]\n",
      "5026 [D loss: 0.493(R 0.493, F0.494)] [D acc: 0.750(R 0.734, F 0.766)] [G loss: 0.493] [G acc: 0.094]\n",
      "5027 [D loss: 0.538(R 0.555, F0.521)] [D acc: 0.742(R 0.719, F 0.766)] [G loss: 0.538] [G acc: 0.062]\n",
      "5028 [D loss: 0.533(R 0.556, F0.510)] [D acc: 0.688(R 0.625, F 0.750)] [G loss: 0.533] [G acc: 0.062]\n",
      "5029 [D loss: 0.541(R 0.460, F0.622)] [D acc: 0.734(R 0.703, F 0.766)] [G loss: 0.541] [G acc: 0.141]\n",
      "5030 [D loss: 0.524(R 0.571, F0.477)] [D acc: 0.742(R 0.688, F 0.797)] [G loss: 0.524] [G acc: 0.078]\n",
      "5031 [D loss: 0.486(R 0.550, F0.422)] [D acc: 0.766(R 0.656, F 0.875)] [G loss: 0.486] [G acc: 0.078]\n",
      "5032 [D loss: 0.506(R 0.487, F0.525)] [D acc: 0.773(R 0.703, F 0.844)] [G loss: 0.506] [G acc: 0.062]\n",
      "5033 [D loss: 0.514(R 0.553, F0.476)] [D acc: 0.727(R 0.656, F 0.797)] [G loss: 0.514] [G acc: 0.062]\n",
      "5034 [D loss: 0.470(R 0.484, F0.456)] [D acc: 0.773(R 0.672, F 0.875)] [G loss: 0.470] [G acc: 0.016]\n",
      "5035 [D loss: 0.511(R 0.573, F0.449)] [D acc: 0.711(R 0.578, F 0.844)] [G loss: 0.511] [G acc: 0.078]\n",
      "5036 [D loss: 0.500(R 0.428, F0.572)] [D acc: 0.750(R 0.781, F 0.719)] [G loss: 0.500] [G acc: 0.031]\n",
      "5037 [D loss: 0.444(R 0.483, F0.405)] [D acc: 0.805(R 0.734, F 0.875)] [G loss: 0.444] [G acc: 0.047]\n",
      "5038 [D loss: 0.479(R 0.502, F0.456)] [D acc: 0.789(R 0.703, F 0.875)] [G loss: 0.479] [G acc: 0.031]\n",
      "5039 [D loss: 0.465(R 0.430, F0.500)] [D acc: 0.758(R 0.719, F 0.797)] [G loss: 0.465] [G acc: 0.031]\n",
      "5040 [D loss: 0.524(R 0.533, F0.514)] [D acc: 0.781(R 0.688, F 0.875)] [G loss: 0.524] [G acc: 0.031]\n",
      "5041 [D loss: 0.444(R 0.516, F0.372)] [D acc: 0.805(R 0.734, F 0.875)] [G loss: 0.444] [G acc: 0.047]\n",
      "5042 [D loss: 0.531(R 0.490, F0.573)] [D acc: 0.734(R 0.688, F 0.781)] [G loss: 0.531] [G acc: 0.141]\n",
      "5043 [D loss: 0.530(R 0.452, F0.608)] [D acc: 0.734(R 0.719, F 0.750)] [G loss: 0.530] [G acc: 0.172]\n",
      "5044 [D loss: 0.504(R 0.485, F0.522)] [D acc: 0.742(R 0.734, F 0.750)] [G loss: 0.504] [G acc: 0.078]\n",
      "5045 [D loss: 0.448(R 0.438, F0.457)] [D acc: 0.766(R 0.750, F 0.781)] [G loss: 0.448] [G acc: 0.094]\n",
      "5046 [D loss: 0.432(R 0.509, F0.356)] [D acc: 0.797(R 0.734, F 0.859)] [G loss: 0.432] [G acc: 0.078]\n",
      "5047 [D loss: 0.434(R 0.482, F0.387)] [D acc: 0.812(R 0.719, F 0.906)] [G loss: 0.434] [G acc: 0.000]\n",
      "5048 [D loss: 0.472(R 0.413, F0.530)] [D acc: 0.727(R 0.719, F 0.734)] [G loss: 0.472] [G acc: 0.078]\n",
      "5049 [D loss: 0.632(R 0.740, F0.525)] [D acc: 0.688(R 0.562, F 0.812)] [G loss: 0.632] [G acc: 0.062]\n",
      "5050 [D loss: 0.446(R 0.459, F0.432)] [D acc: 0.797(R 0.750, F 0.844)] [G loss: 0.446] [G acc: 0.062]\n",
      "5051 [D loss: 0.469(R 0.498, F0.440)] [D acc: 0.789(R 0.719, F 0.859)] [G loss: 0.469] [G acc: 0.047]\n",
      "5052 [D loss: 0.424(R 0.536, F0.313)] [D acc: 0.773(R 0.609, F 0.938)] [G loss: 0.424] [G acc: 0.047]\n",
      "5053 [D loss: 0.391(R 0.386, F0.396)] [D acc: 0.844(R 0.812, F 0.875)] [G loss: 0.391] [G acc: 0.062]\n",
      "5054 [D loss: 0.484(R 0.436, F0.532)] [D acc: 0.734(R 0.734, F 0.734)] [G loss: 0.484] [G acc: 0.047]\n",
      "5055 [D loss: 0.546(R 0.646, F0.446)] [D acc: 0.734(R 0.578, F 0.891)] [G loss: 0.546] [G acc: 0.016]\n",
      "5056 [D loss: 0.500(R 0.603, F0.397)] [D acc: 0.789(R 0.703, F 0.875)] [G loss: 0.500] [G acc: 0.094]\n",
      "5057 [D loss: 0.452(R 0.402, F0.502)] [D acc: 0.781(R 0.797, F 0.766)] [G loss: 0.452] [G acc: 0.109]\n",
      "5058 [D loss: 0.544(R 0.581, F0.507)] [D acc: 0.703(R 0.625, F 0.781)] [G loss: 0.544] [G acc: 0.031]\n",
      "5059 [D loss: 0.474(R 0.565, F0.383)] [D acc: 0.750(R 0.656, F 0.844)] [G loss: 0.474] [G acc: 0.047]\n",
      "5060 [D loss: 0.490(R 0.516, F0.463)] [D acc: 0.734(R 0.672, F 0.797)] [G loss: 0.490] [G acc: 0.062]\n",
      "5061 [D loss: 0.553(R 0.534, F0.572)] [D acc: 0.742(R 0.672, F 0.812)] [G loss: 0.553] [G acc: 0.062]\n",
      "5062 [D loss: 0.457(R 0.384, F0.530)] [D acc: 0.758(R 0.734, F 0.781)] [G loss: 0.457] [G acc: 0.062]\n",
      "5063 [D loss: 0.527(R 0.615, F0.439)] [D acc: 0.758(R 0.625, F 0.891)] [G loss: 0.527] [G acc: 0.062]\n",
      "5064 [D loss: 0.433(R 0.457, F0.410)] [D acc: 0.781(R 0.703, F 0.859)] [G loss: 0.433] [G acc: 0.000]\n",
      "5065 [D loss: 0.564(R 0.520, F0.608)] [D acc: 0.719(R 0.703, F 0.734)] [G loss: 0.564] [G acc: 0.062]\n",
      "5066 [D loss: 0.533(R 0.607, F0.459)] [D acc: 0.758(R 0.609, F 0.906)] [G loss: 0.533] [G acc: 0.047]\n",
      "5067 [D loss: 0.504(R 0.550, F0.459)] [D acc: 0.773(R 0.688, F 0.859)] [G loss: 0.504] [G acc: 0.078]\n",
      "5068 [D loss: 0.624(R 0.685, F0.563)] [D acc: 0.711(R 0.625, F 0.797)] [G loss: 0.624] [G acc: 0.094]\n",
      "5069 [D loss: 0.525(R 0.520, F0.529)] [D acc: 0.734(R 0.719, F 0.750)] [G loss: 0.525] [G acc: 0.094]\n",
      "5070 [D loss: 0.435(R 0.471, F0.400)] [D acc: 0.789(R 0.672, F 0.906)] [G loss: 0.435] [G acc: 0.094]\n",
      "5071 [D loss: 0.543(R 0.503, F0.583)] [D acc: 0.719(R 0.688, F 0.750)] [G loss: 0.543] [G acc: 0.047]\n",
      "5072 [D loss: 0.544(R 0.680, F0.407)] [D acc: 0.750(R 0.609, F 0.891)] [G loss: 0.544] [G acc: 0.031]\n",
      "5073 [D loss: 0.472(R 0.491, F0.454)] [D acc: 0.766(R 0.656, F 0.875)] [G loss: 0.472] [G acc: 0.094]\n",
      "5074 [D loss: 0.534(R 0.505, F0.563)] [D acc: 0.727(R 0.688, F 0.766)] [G loss: 0.534] [G acc: 0.031]\n",
      "5075 [D loss: 0.467(R 0.451, F0.484)] [D acc: 0.766(R 0.656, F 0.875)] [G loss: 0.467] [G acc: 0.047]\n",
      "5076 [D loss: 0.542(R 0.590, F0.494)] [D acc: 0.758(R 0.688, F 0.828)] [G loss: 0.542] [G acc: 0.094]\n",
      "5077 [D loss: 0.413(R 0.519, F0.308)] [D acc: 0.820(R 0.672, F 0.969)] [G loss: 0.413] [G acc: 0.094]\n",
      "5078 [D loss: 0.601(R 0.417, F0.784)] [D acc: 0.703(R 0.734, F 0.672)] [G loss: 0.601] [G acc: 0.062]\n",
      "5079 [D loss: 0.563(R 0.652, F0.473)] [D acc: 0.727(R 0.578, F 0.875)] [G loss: 0.563] [G acc: 0.047]\n",
      "5080 [D loss: 0.537(R 0.602, F0.472)] [D acc: 0.719(R 0.609, F 0.828)] [G loss: 0.537] [G acc: 0.078]\n",
      "5081 [D loss: 0.491(R 0.501, F0.480)] [D acc: 0.773(R 0.688, F 0.859)] [G loss: 0.491] [G acc: 0.094]\n",
      "5082 [D loss: 0.497(R 0.585, F0.408)] [D acc: 0.750(R 0.641, F 0.859)] [G loss: 0.497] [G acc: 0.109]\n",
      "5083 [D loss: 0.431(R 0.447, F0.416)] [D acc: 0.789(R 0.719, F 0.859)] [G loss: 0.431] [G acc: 0.125]\n",
      "5084 [D loss: 0.446(R 0.344, F0.548)] [D acc: 0.758(R 0.781, F 0.734)] [G loss: 0.446] [G acc: 0.016]\n",
      "5085 [D loss: 0.527(R 0.580, F0.474)] [D acc: 0.766(R 0.656, F 0.875)] [G loss: 0.527] [G acc: 0.094]\n",
      "5086 [D loss: 0.536(R 0.454, F0.619)] [D acc: 0.711(R 0.672, F 0.750)] [G loss: 0.536] [G acc: 0.078]\n",
      "5087 [D loss: 0.591(R 0.753, F0.430)] [D acc: 0.695(R 0.531, F 0.859)] [G loss: 0.591] [G acc: 0.031]\n",
      "5088 [D loss: 0.509(R 0.559, F0.459)] [D acc: 0.719(R 0.641, F 0.797)] [G loss: 0.509] [G acc: 0.109]\n",
      "5089 [D loss: 0.509(R 0.512, F0.506)] [D acc: 0.703(R 0.672, F 0.734)] [G loss: 0.509] [G acc: 0.062]\n",
      "5090 [D loss: 0.490(R 0.469, F0.511)] [D acc: 0.766(R 0.703, F 0.828)] [G loss: 0.490] [G acc: 0.062]\n",
      "5091 [D loss: 0.432(R 0.494, F0.370)] [D acc: 0.789(R 0.719, F 0.859)] [G loss: 0.432] [G acc: 0.172]\n",
      "5092 [D loss: 0.532(R 0.335, F0.729)] [D acc: 0.789(R 0.844, F 0.734)] [G loss: 0.532] [G acc: 0.047]\n",
      "5093 [D loss: 0.516(R 0.607, F0.426)] [D acc: 0.734(R 0.625, F 0.844)] [G loss: 0.516] [G acc: 0.125]\n",
      "5094 [D loss: 0.348(R 0.358, F0.337)] [D acc: 0.836(R 0.781, F 0.891)] [G loss: 0.348] [G acc: 0.062]\n",
      "5095 [D loss: 0.496(R 0.490, F0.502)] [D acc: 0.758(R 0.750, F 0.766)] [G loss: 0.496] [G acc: 0.047]\n",
      "5096 [D loss: 0.706(R 0.771, F0.641)] [D acc: 0.688(R 0.578, F 0.797)] [G loss: 0.706] [G acc: 0.047]\n",
      "5097 [D loss: 0.578(R 0.689, F0.468)] [D acc: 0.703(R 0.578, F 0.828)] [G loss: 0.578] [G acc: 0.125]\n",
      "5098 [D loss: 0.539(R 0.516, F0.561)] [D acc: 0.797(R 0.750, F 0.844)] [G loss: 0.539] [G acc: 0.078]\n",
      "5099 [D loss: 0.528(R 0.550, F0.506)] [D acc: 0.766(R 0.703, F 0.828)] [G loss: 0.528] [G acc: 0.141]\n",
      "INFO:tensorflow:Assets written to: ram://8e0d1e5b-314b-40b4-8d08-f3bd5a72da40/assets\n",
      "INFO:tensorflow:Assets written to: ram://46ceb5d6-cc79-4cff-b4cc-3b61b6185750/assets\n",
      "INFO:tensorflow:Assets written to: ram://8461c196-66f0-461f-b93c-19389ff5b451/assets\n",
      "5100 [D loss: 0.497(R 0.536, F0.458)] [D acc: 0.750(R 0.672, F 0.828)] [G loss: 0.497] [G acc: 0.109]\n",
      "5101 [D loss: 0.524(R 0.549, F0.499)] [D acc: 0.742(R 0.688, F 0.797)] [G loss: 0.524] [G acc: 0.156]\n",
      "5102 [D loss: 0.547(R 0.665, F0.429)] [D acc: 0.656(R 0.516, F 0.797)] [G loss: 0.547] [G acc: 0.062]\n",
      "5103 [D loss: 0.507(R 0.441, F0.573)] [D acc: 0.750(R 0.734, F 0.766)] [G loss: 0.507] [G acc: 0.047]\n",
      "5104 [D loss: 0.464(R 0.451, F0.477)] [D acc: 0.812(R 0.750, F 0.875)] [G loss: 0.464] [G acc: 0.156]\n",
      "5105 [D loss: 0.495(R 0.455, F0.536)] [D acc: 0.734(R 0.688, F 0.781)] [G loss: 0.495] [G acc: 0.078]\n",
      "5106 [D loss: 0.508(R 0.506, F0.510)] [D acc: 0.734(R 0.672, F 0.797)] [G loss: 0.508] [G acc: 0.031]\n",
      "5107 [D loss: 0.547(R 0.646, F0.448)] [D acc: 0.703(R 0.594, F 0.812)] [G loss: 0.547] [G acc: 0.078]\n",
      "5108 [D loss: 0.528(R 0.536, F0.520)] [D acc: 0.711(R 0.641, F 0.781)] [G loss: 0.528] [G acc: 0.156]\n",
      "5109 [D loss: 0.461(R 0.436, F0.486)] [D acc: 0.773(R 0.734, F 0.812)] [G loss: 0.461] [G acc: 0.156]\n",
      "5110 [D loss: 0.449(R 0.431, F0.467)] [D acc: 0.742(R 0.734, F 0.750)] [G loss: 0.449] [G acc: 0.156]\n",
      "5111 [D loss: 0.585(R 0.592, F0.577)] [D acc: 0.727(R 0.625, F 0.828)] [G loss: 0.585] [G acc: 0.062]\n",
      "5112 [D loss: 0.472(R 0.452, F0.492)] [D acc: 0.797(R 0.766, F 0.828)] [G loss: 0.472] [G acc: 0.094]\n",
      "5113 [D loss: 0.417(R 0.443, F0.392)] [D acc: 0.812(R 0.734, F 0.891)] [G loss: 0.417] [G acc: 0.078]\n",
      "5114 [D loss: 0.557(R 0.506, F0.607)] [D acc: 0.719(R 0.688, F 0.750)] [G loss: 0.557] [G acc: 0.125]\n",
      "5115 [D loss: 0.495(R 0.546, F0.444)] [D acc: 0.781(R 0.703, F 0.859)] [G loss: 0.495] [G acc: 0.172]\n",
      "5116 [D loss: 0.623(R 0.640, F0.606)] [D acc: 0.727(R 0.688, F 0.766)] [G loss: 0.623] [G acc: 0.047]\n",
      "5117 [D loss: 0.419(R 0.417, F0.421)] [D acc: 0.859(R 0.828, F 0.891)] [G loss: 0.419] [G acc: 0.062]\n",
      "5118 [D loss: 0.525(R 0.596, F0.453)] [D acc: 0.711(R 0.625, F 0.797)] [G loss: 0.525] [G acc: 0.109]\n",
      "5119 [D loss: 0.499(R 0.479, F0.518)] [D acc: 0.727(R 0.703, F 0.750)] [G loss: 0.499] [G acc: 0.078]\n",
      "5120 [D loss: 0.492(R 0.549, F0.434)] [D acc: 0.766(R 0.688, F 0.844)] [G loss: 0.492] [G acc: 0.094]\n",
      "5121 [D loss: 0.527(R 0.594, F0.459)] [D acc: 0.781(R 0.656, F 0.906)] [G loss: 0.527] [G acc: 0.094]\n",
      "5122 [D loss: 0.431(R 0.312, F0.551)] [D acc: 0.797(R 0.828, F 0.766)] [G loss: 0.431] [G acc: 0.094]\n",
      "5123 [D loss: 0.458(R 0.507, F0.409)] [D acc: 0.773(R 0.719, F 0.828)] [G loss: 0.458] [G acc: 0.109]\n",
      "5124 [D loss: 0.459(R 0.453, F0.465)] [D acc: 0.805(R 0.750, F 0.859)] [G loss: 0.459] [G acc: 0.062]\n",
      "5125 [D loss: 0.528(R 0.620, F0.436)] [D acc: 0.766(R 0.641, F 0.891)] [G loss: 0.528] [G acc: 0.016]\n",
      "5126 [D loss: 0.596(R 0.650, F0.543)] [D acc: 0.680(R 0.562, F 0.797)] [G loss: 0.596] [G acc: 0.078]\n",
      "5127 [D loss: 0.513(R 0.437, F0.588)] [D acc: 0.750(R 0.734, F 0.766)] [G loss: 0.513] [G acc: 0.094]\n",
      "5128 [D loss: 0.457(R 0.476, F0.438)] [D acc: 0.758(R 0.688, F 0.828)] [G loss: 0.457] [G acc: 0.062]\n",
      "5129 [D loss: 0.410(R 0.421, F0.399)] [D acc: 0.828(R 0.797, F 0.859)] [G loss: 0.410] [G acc: 0.062]\n",
      "5130 [D loss: 0.511(R 0.491, F0.530)] [D acc: 0.758(R 0.719, F 0.797)] [G loss: 0.511] [G acc: 0.062]\n",
      "5131 [D loss: 0.560(R 0.527, F0.593)] [D acc: 0.727(R 0.688, F 0.766)] [G loss: 0.560] [G acc: 0.062]\n",
      "5132 [D loss: 0.531(R 0.598, F0.464)] [D acc: 0.734(R 0.625, F 0.844)] [G loss: 0.531] [G acc: 0.078]\n",
      "5133 [D loss: 0.352(R 0.299, F0.406)] [D acc: 0.844(R 0.859, F 0.828)] [G loss: 0.352] [G acc: 0.094]\n",
      "5134 [D loss: 0.521(R 0.537, F0.506)] [D acc: 0.766(R 0.703, F 0.828)] [G loss: 0.521] [G acc: 0.094]\n",
      "5135 [D loss: 0.649(R 0.695, F0.603)] [D acc: 0.680(R 0.562, F 0.797)] [G loss: 0.649] [G acc: 0.094]\n",
      "5136 [D loss: 0.562(R 0.402, F0.721)] [D acc: 0.695(R 0.719, F 0.672)] [G loss: 0.562] [G acc: 0.047]\n",
      "5137 [D loss: 0.538(R 0.638, F0.438)] [D acc: 0.719(R 0.609, F 0.828)] [G loss: 0.538] [G acc: 0.047]\n",
      "5138 [D loss: 0.414(R 0.400, F0.429)] [D acc: 0.828(R 0.797, F 0.859)] [G loss: 0.414] [G acc: 0.078]\n",
      "5139 [D loss: 0.537(R 0.621, F0.454)] [D acc: 0.719(R 0.641, F 0.797)] [G loss: 0.537] [G acc: 0.094]\n",
      "5140 [D loss: 0.429(R 0.470, F0.387)] [D acc: 0.797(R 0.734, F 0.859)] [G loss: 0.429] [G acc: 0.172]\n",
      "5141 [D loss: 0.443(R 0.417, F0.469)] [D acc: 0.805(R 0.734, F 0.875)] [G loss: 0.443] [G acc: 0.094]\n",
      "5142 [D loss: 0.486(R 0.414, F0.557)] [D acc: 0.773(R 0.781, F 0.766)] [G loss: 0.486] [G acc: 0.188]\n",
      "5143 [D loss: 0.378(R 0.263, F0.492)] [D acc: 0.852(R 0.906, F 0.797)] [G loss: 0.378] [G acc: 0.031]\n",
      "5144 [D loss: 0.521(R 0.569, F0.473)] [D acc: 0.773(R 0.672, F 0.875)] [G loss: 0.521] [G acc: 0.047]\n",
      "5145 [D loss: 0.480(R 0.457, F0.502)] [D acc: 0.820(R 0.766, F 0.875)] [G loss: 0.480] [G acc: 0.125]\n",
      "5146 [D loss: 0.422(R 0.448, F0.397)] [D acc: 0.820(R 0.766, F 0.875)] [G loss: 0.422] [G acc: 0.125]\n",
      "5147 [D loss: 0.523(R 0.490, F0.556)] [D acc: 0.758(R 0.719, F 0.797)] [G loss: 0.523] [G acc: 0.047]\n",
      "5148 [D loss: 0.599(R 0.674, F0.523)] [D acc: 0.719(R 0.625, F 0.812)] [G loss: 0.599] [G acc: 0.047]\n",
      "5149 [D loss: 0.480(R 0.505, F0.455)] [D acc: 0.758(R 0.656, F 0.859)] [G loss: 0.480] [G acc: 0.141]\n",
      "5150 [D loss: 0.467(R 0.459, F0.475)] [D acc: 0.781(R 0.703, F 0.859)] [G loss: 0.467] [G acc: 0.125]\n",
      "5151 [D loss: 0.435(R 0.384, F0.487)] [D acc: 0.852(R 0.844, F 0.859)] [G loss: 0.435] [G acc: 0.062]\n",
      "5152 [D loss: 0.406(R 0.437, F0.375)] [D acc: 0.836(R 0.781, F 0.891)] [G loss: 0.406] [G acc: 0.031]\n",
      "5153 [D loss: 0.541(R 0.557, F0.525)] [D acc: 0.766(R 0.688, F 0.844)] [G loss: 0.541] [G acc: 0.047]\n",
      "5154 [D loss: 0.585(R 0.574, F0.596)] [D acc: 0.695(R 0.594, F 0.797)] [G loss: 0.585] [G acc: 0.000]\n",
      "5155 [D loss: 0.488(R 0.573, F0.403)] [D acc: 0.742(R 0.688, F 0.797)] [G loss: 0.488] [G acc: 0.078]\n",
      "5156 [D loss: 0.503(R 0.569, F0.438)] [D acc: 0.727(R 0.609, F 0.844)] [G loss: 0.503] [G acc: 0.047]\n",
      "5157 [D loss: 0.418(R 0.401, F0.435)] [D acc: 0.820(R 0.781, F 0.859)] [G loss: 0.418] [G acc: 0.078]\n",
      "5158 [D loss: 0.530(R 0.584, F0.476)] [D acc: 0.766(R 0.672, F 0.859)] [G loss: 0.530] [G acc: 0.109]\n",
      "5159 [D loss: 0.499(R 0.509, F0.489)] [D acc: 0.789(R 0.703, F 0.875)] [G loss: 0.499] [G acc: 0.047]\n",
      "5160 [D loss: 0.505(R 0.519, F0.492)] [D acc: 0.719(R 0.688, F 0.750)] [G loss: 0.505] [G acc: 0.125]\n",
      "5161 [D loss: 0.503(R 0.570, F0.436)] [D acc: 0.719(R 0.625, F 0.812)] [G loss: 0.503] [G acc: 0.125]\n",
      "5162 [D loss: 0.445(R 0.388, F0.501)] [D acc: 0.828(R 0.812, F 0.844)] [G loss: 0.445] [G acc: 0.062]\n",
      "5163 [D loss: 0.494(R 0.576, F0.412)] [D acc: 0.773(R 0.688, F 0.859)] [G loss: 0.494] [G acc: 0.141]\n",
      "5164 [D loss: 0.428(R 0.419, F0.436)] [D acc: 0.797(R 0.750, F 0.844)] [G loss: 0.428] [G acc: 0.031]\n",
      "5165 [D loss: 0.472(R 0.466, F0.479)] [D acc: 0.805(R 0.734, F 0.875)] [G loss: 0.472] [G acc: 0.047]\n",
      "5166 [D loss: 0.491(R 0.510, F0.472)] [D acc: 0.773(R 0.750, F 0.797)] [G loss: 0.491] [G acc: 0.000]\n",
      "5167 [D loss: 0.533(R 0.626, F0.439)] [D acc: 0.742(R 0.625, F 0.859)] [G loss: 0.533] [G acc: 0.094]\n",
      "5168 [D loss: 0.480(R 0.497, F0.463)] [D acc: 0.734(R 0.688, F 0.781)] [G loss: 0.480] [G acc: 0.047]\n",
      "5169 [D loss: 0.466(R 0.459, F0.474)] [D acc: 0.766(R 0.719, F 0.812)] [G loss: 0.466] [G acc: 0.109]\n",
      "5170 [D loss: 0.508(R 0.542, F0.474)] [D acc: 0.766(R 0.703, F 0.828)] [G loss: 0.508] [G acc: 0.094]\n",
      "5171 [D loss: 0.721(R 0.581, F0.862)] [D acc: 0.672(R 0.656, F 0.688)] [G loss: 0.721] [G acc: 0.016]\n",
      "5172 [D loss: 0.548(R 0.678, F0.418)] [D acc: 0.742(R 0.594, F 0.891)] [G loss: 0.548] [G acc: 0.062]\n",
      "5173 [D loss: 0.418(R 0.434, F0.402)] [D acc: 0.820(R 0.734, F 0.906)] [G loss: 0.418] [G acc: 0.109]\n",
      "5174 [D loss: 0.454(R 0.499, F0.409)] [D acc: 0.781(R 0.688, F 0.875)] [G loss: 0.454] [G acc: 0.094]\n",
      "5175 [D loss: 0.681(R 0.591, F0.771)] [D acc: 0.688(R 0.656, F 0.719)] [G loss: 0.681] [G acc: 0.047]\n",
      "5176 [D loss: 0.473(R 0.552, F0.394)] [D acc: 0.797(R 0.719, F 0.875)] [G loss: 0.473] [G acc: 0.094]\n",
      "5177 [D loss: 0.462(R 0.498, F0.426)] [D acc: 0.750(R 0.672, F 0.828)] [G loss: 0.462] [G acc: 0.094]\n",
      "5178 [D loss: 0.439(R 0.402, F0.476)] [D acc: 0.766(R 0.750, F 0.781)] [G loss: 0.439] [G acc: 0.078]\n",
      "5179 [D loss: 0.441(R 0.365, F0.518)] [D acc: 0.781(R 0.812, F 0.750)] [G loss: 0.441] [G acc: 0.047]\n",
      "5180 [D loss: 0.502(R 0.510, F0.494)] [D acc: 0.750(R 0.703, F 0.797)] [G loss: 0.502] [G acc: 0.031]\n",
      "5181 [D loss: 0.498(R 0.488, F0.507)] [D acc: 0.766(R 0.750, F 0.781)] [G loss: 0.498] [G acc: 0.031]\n",
      "5182 [D loss: 0.453(R 0.523, F0.382)] [D acc: 0.805(R 0.703, F 0.906)] [G loss: 0.453] [G acc: 0.047]\n",
      "5183 [D loss: 0.535(R 0.523, F0.547)] [D acc: 0.742(R 0.719, F 0.766)] [G loss: 0.535] [G acc: 0.062]\n",
      "5184 [D loss: 0.389(R 0.461, F0.318)] [D acc: 0.844(R 0.734, F 0.953)] [G loss: 0.389] [G acc: 0.062]\n",
      "5185 [D loss: 0.536(R 0.686, F0.386)] [D acc: 0.727(R 0.594, F 0.859)] [G loss: 0.536] [G acc: 0.078]\n",
      "5186 [D loss: 0.381(R 0.380, F0.383)] [D acc: 0.844(R 0.797, F 0.891)] [G loss: 0.381] [G acc: 0.094]\n",
      "5187 [D loss: 0.485(R 0.542, F0.428)] [D acc: 0.789(R 0.688, F 0.891)] [G loss: 0.485] [G acc: 0.109]\n",
      "5188 [D loss: 0.621(R 0.663, F0.580)] [D acc: 0.625(R 0.578, F 0.672)] [G loss: 0.621] [G acc: 0.047]\n",
      "5189 [D loss: 0.393(R 0.382, F0.404)] [D acc: 0.820(R 0.766, F 0.875)] [G loss: 0.393] [G acc: 0.078]\n",
      "5190 [D loss: 0.540(R 0.568, F0.511)] [D acc: 0.758(R 0.688, F 0.828)] [G loss: 0.540] [G acc: 0.094]\n",
      "5191 [D loss: 0.602(R 0.707, F0.497)] [D acc: 0.656(R 0.547, F 0.766)] [G loss: 0.602] [G acc: 0.141]\n",
      "5192 [D loss: 0.470(R 0.524, F0.415)] [D acc: 0.797(R 0.734, F 0.859)] [G loss: 0.470] [G acc: 0.062]\n",
      "5193 [D loss: 0.433(R 0.428, F0.439)] [D acc: 0.820(R 0.812, F 0.828)] [G loss: 0.433] [G acc: 0.047]\n",
      "5194 [D loss: 0.451(R 0.465, F0.436)] [D acc: 0.766(R 0.719, F 0.812)] [G loss: 0.451] [G acc: 0.047]\n",
      "5195 [D loss: 0.564(R 0.498, F0.630)] [D acc: 0.680(R 0.688, F 0.672)] [G loss: 0.564] [G acc: 0.031]\n",
      "5196 [D loss: 0.581(R 0.737, F0.425)] [D acc: 0.734(R 0.578, F 0.891)] [G loss: 0.581] [G acc: 0.078]\n",
      "5197 [D loss: 0.470(R 0.465, F0.475)] [D acc: 0.781(R 0.734, F 0.828)] [G loss: 0.470] [G acc: 0.156]\n",
      "5198 [D loss: 0.440(R 0.425, F0.455)] [D acc: 0.781(R 0.766, F 0.797)] [G loss: 0.440] [G acc: 0.078]\n",
      "5199 [D loss: 0.483(R 0.522, F0.445)] [D acc: 0.750(R 0.672, F 0.828)] [G loss: 0.483] [G acc: 0.156]\n",
      "INFO:tensorflow:Assets written to: ram://34ba71df-380c-4f44-b6e7-554cacf9de7a/assets\n",
      "INFO:tensorflow:Assets written to: ram://07307c07-bfea-47f3-a760-28b87b64fc64/assets\n",
      "INFO:tensorflow:Assets written to: ram://6ced19c7-067c-463a-92d9-3fa9f363d5bc/assets\n",
      "5200 [D loss: 0.571(R 0.541, F0.601)] [D acc: 0.719(R 0.641, F 0.797)] [G loss: 0.571] [G acc: 0.016]\n",
      "5201 [D loss: 0.615(R 0.541, F0.689)] [D acc: 0.648(R 0.641, F 0.656)] [G loss: 0.615] [G acc: 0.047]\n",
      "5202 [D loss: 0.544(R 0.673, F0.414)] [D acc: 0.703(R 0.578, F 0.828)] [G loss: 0.544] [G acc: 0.047]\n",
      "5203 [D loss: 0.434(R 0.552, F0.317)] [D acc: 0.828(R 0.688, F 0.969)] [G loss: 0.434] [G acc: 0.078]\n",
      "5204 [D loss: 0.419(R 0.334, F0.504)] [D acc: 0.805(R 0.797, F 0.812)] [G loss: 0.419] [G acc: 0.016]\n",
      "5205 [D loss: 0.529(R 0.553, F0.506)] [D acc: 0.727(R 0.656, F 0.797)] [G loss: 0.529] [G acc: 0.125]\n",
      "5206 [D loss: 0.521(R 0.611, F0.431)] [D acc: 0.703(R 0.578, F 0.828)] [G loss: 0.521] [G acc: 0.094]\n",
      "5207 [D loss: 0.462(R 0.500, F0.424)] [D acc: 0.758(R 0.703, F 0.812)] [G loss: 0.462] [G acc: 0.062]\n",
      "5208 [D loss: 0.564(R 0.639, F0.489)] [D acc: 0.688(R 0.578, F 0.797)] [G loss: 0.564] [G acc: 0.109]\n",
      "5209 [D loss: 0.450(R 0.417, F0.483)] [D acc: 0.812(R 0.781, F 0.844)] [G loss: 0.450] [G acc: 0.062]\n",
      "5210 [D loss: 0.570(R 0.595, F0.546)] [D acc: 0.688(R 0.578, F 0.797)] [G loss: 0.570] [G acc: 0.062]\n",
      "5211 [D loss: 0.533(R 0.531, F0.535)] [D acc: 0.664(R 0.562, F 0.766)] [G loss: 0.533] [G acc: 0.047]\n",
      "5212 [D loss: 0.503(R 0.577, F0.429)] [D acc: 0.742(R 0.641, F 0.844)] [G loss: 0.503] [G acc: 0.047]\n",
      "5213 [D loss: 0.602(R 0.575, F0.630)] [D acc: 0.688(R 0.688, F 0.688)] [G loss: 0.602] [G acc: 0.031]\n",
      "5214 [D loss: 0.495(R 0.585, F0.404)] [D acc: 0.742(R 0.594, F 0.891)] [G loss: 0.495] [G acc: 0.062]\n",
      "5215 [D loss: 0.551(R 0.635, F0.467)] [D acc: 0.734(R 0.641, F 0.828)] [G loss: 0.551] [G acc: 0.047]\n",
      "5216 [D loss: 0.481(R 0.482, F0.480)] [D acc: 0.750(R 0.672, F 0.828)] [G loss: 0.481] [G acc: 0.078]\n",
      "5217 [D loss: 0.459(R 0.413, F0.506)] [D acc: 0.789(R 0.781, F 0.797)] [G loss: 0.459] [G acc: 0.109]\n",
      "5218 [D loss: 0.544(R 0.549, F0.539)] [D acc: 0.719(R 0.672, F 0.766)] [G loss: 0.544] [G acc: 0.000]\n",
      "5219 [D loss: 0.467(R 0.455, F0.480)] [D acc: 0.797(R 0.766, F 0.828)] [G loss: 0.467] [G acc: 0.094]\n",
      "5220 [D loss: 0.530(R 0.622, F0.437)] [D acc: 0.742(R 0.609, F 0.875)] [G loss: 0.530] [G acc: 0.078]\n",
      "5221 [D loss: 0.546(R 0.565, F0.527)] [D acc: 0.734(R 0.672, F 0.797)] [G loss: 0.546] [G acc: 0.078]\n",
      "5222 [D loss: 0.476(R 0.401, F0.552)] [D acc: 0.742(R 0.781, F 0.703)] [G loss: 0.476] [G acc: 0.109]\n",
      "5223 [D loss: 0.445(R 0.483, F0.407)] [D acc: 0.828(R 0.734, F 0.922)] [G loss: 0.445] [G acc: 0.016]\n",
      "5224 [D loss: 0.423(R 0.443, F0.403)] [D acc: 0.789(R 0.719, F 0.859)] [G loss: 0.423] [G acc: 0.031]\n",
      "5225 [D loss: 0.509(R 0.440, F0.577)] [D acc: 0.789(R 0.734, F 0.844)] [G loss: 0.509] [G acc: 0.094]\n",
      "5226 [D loss: 0.398(R 0.443, F0.354)] [D acc: 0.820(R 0.734, F 0.906)] [G loss: 0.398] [G acc: 0.047]\n",
      "5227 [D loss: 0.410(R 0.393, F0.427)] [D acc: 0.836(R 0.781, F 0.891)] [G loss: 0.410] [G acc: 0.047]\n",
      "5228 [D loss: 0.436(R 0.327, F0.545)] [D acc: 0.797(R 0.828, F 0.766)] [G loss: 0.436] [G acc: 0.062]\n",
      "5229 [D loss: 0.440(R 0.474, F0.407)] [D acc: 0.789(R 0.719, F 0.859)] [G loss: 0.440] [G acc: 0.109]\n",
      "5230 [D loss: 0.388(R 0.503, F0.273)] [D acc: 0.844(R 0.750, F 0.938)] [G loss: 0.388] [G acc: 0.031]\n",
      "5231 [D loss: 0.628(R 0.557, F0.699)] [D acc: 0.688(R 0.656, F 0.719)] [G loss: 0.628] [G acc: 0.047]\n",
      "5232 [D loss: 0.620(R 0.734, F0.505)] [D acc: 0.688(R 0.562, F 0.812)] [G loss: 0.620] [G acc: 0.094]\n",
      "5233 [D loss: 0.487(R 0.500, F0.475)] [D acc: 0.727(R 0.703, F 0.750)] [G loss: 0.487] [G acc: 0.062]\n",
      "5234 [D loss: 0.476(R 0.569, F0.383)] [D acc: 0.773(R 0.641, F 0.906)] [G loss: 0.476] [G acc: 0.031]\n",
      "5235 [D loss: 0.454(R 0.495, F0.413)] [D acc: 0.805(R 0.688, F 0.922)] [G loss: 0.454] [G acc: 0.109]\n",
      "5236 [D loss: 0.514(R 0.498, F0.531)] [D acc: 0.773(R 0.734, F 0.812)] [G loss: 0.514] [G acc: 0.047]\n",
      "5237 [D loss: 0.466(R 0.472, F0.460)] [D acc: 0.773(R 0.688, F 0.859)] [G loss: 0.466] [G acc: 0.031]\n",
      "5238 [D loss: 0.512(R 0.501, F0.524)] [D acc: 0.727(R 0.719, F 0.734)] [G loss: 0.512] [G acc: 0.141]\n",
      "5239 [D loss: 0.509(R 0.534, F0.484)] [D acc: 0.680(R 0.641, F 0.719)] [G loss: 0.509] [G acc: 0.141]\n",
      "5240 [D loss: 0.489(R 0.499, F0.480)] [D acc: 0.750(R 0.672, F 0.828)] [G loss: 0.489] [G acc: 0.047]\n",
      "5241 [D loss: 0.477(R 0.464, F0.490)] [D acc: 0.758(R 0.734, F 0.781)] [G loss: 0.477] [G acc: 0.078]\n",
      "5242 [D loss: 0.602(R 0.690, F0.514)] [D acc: 0.695(R 0.578, F 0.812)] [G loss: 0.602] [G acc: 0.016]\n",
      "5243 [D loss: 0.595(R 0.698, F0.493)] [D acc: 0.703(R 0.609, F 0.797)] [G loss: 0.595] [G acc: 0.016]\n",
      "5244 [D loss: 0.561(R 0.613, F0.508)] [D acc: 0.688(R 0.609, F 0.766)] [G loss: 0.561] [G acc: 0.078]\n",
      "5245 [D loss: 0.565(R 0.552, F0.578)] [D acc: 0.680(R 0.641, F 0.719)] [G loss: 0.565] [G acc: 0.047]\n",
      "5246 [D loss: 0.462(R 0.471, F0.453)] [D acc: 0.773(R 0.688, F 0.859)] [G loss: 0.462] [G acc: 0.172]\n",
      "5247 [D loss: 0.591(R 0.600, F0.582)] [D acc: 0.695(R 0.594, F 0.797)] [G loss: 0.591] [G acc: 0.031]\n",
      "5248 [D loss: 0.655(R 0.903, F0.407)] [D acc: 0.703(R 0.516, F 0.891)] [G loss: 0.655] [G acc: 0.078]\n",
      "5249 [D loss: 0.585(R 0.527, F0.644)] [D acc: 0.711(R 0.672, F 0.750)] [G loss: 0.585] [G acc: 0.047]\n",
      "5250 [D loss: 0.494(R 0.579, F0.409)] [D acc: 0.781(R 0.656, F 0.906)] [G loss: 0.494] [G acc: 0.094]\n",
      "5251 [D loss: 0.539(R 0.502, F0.576)] [D acc: 0.719(R 0.719, F 0.719)] [G loss: 0.539] [G acc: 0.047]\n",
      "5252 [D loss: 0.548(R 0.589, F0.507)] [D acc: 0.734(R 0.656, F 0.812)] [G loss: 0.548] [G acc: 0.062]\n",
      "5253 [D loss: 0.464(R 0.523, F0.405)] [D acc: 0.758(R 0.672, F 0.844)] [G loss: 0.464] [G acc: 0.125]\n",
      "5254 [D loss: 0.449(R 0.421, F0.476)] [D acc: 0.781(R 0.766, F 0.797)] [G loss: 0.449] [G acc: 0.031]\n",
      "5255 [D loss: 0.584(R 0.547, F0.622)] [D acc: 0.734(R 0.672, F 0.797)] [G loss: 0.584] [G acc: 0.000]\n",
      "5256 [D loss: 0.601(R 0.771, F0.430)] [D acc: 0.672(R 0.453, F 0.891)] [G loss: 0.601] [G acc: 0.078]\n",
      "5257 [D loss: 0.505(R 0.492, F0.518)] [D acc: 0.742(R 0.703, F 0.781)] [G loss: 0.505] [G acc: 0.094]\n",
      "5258 [D loss: 0.472(R 0.550, F0.393)] [D acc: 0.758(R 0.641, F 0.875)] [G loss: 0.472] [G acc: 0.188]\n",
      "5259 [D loss: 0.447(R 0.443, F0.451)] [D acc: 0.773(R 0.719, F 0.828)] [G loss: 0.447] [G acc: 0.062]\n",
      "5260 [D loss: 0.584(R 0.593, F0.575)] [D acc: 0.758(R 0.688, F 0.828)] [G loss: 0.584] [G acc: 0.109]\n",
      "5261 [D loss: 0.557(R 0.599, F0.515)] [D acc: 0.703(R 0.625, F 0.781)] [G loss: 0.557] [G acc: 0.188]\n",
      "5262 [D loss: 0.535(R 0.619, F0.451)] [D acc: 0.727(R 0.578, F 0.875)] [G loss: 0.535] [G acc: 0.094]\n",
      "5263 [D loss: 0.480(R 0.407, F0.553)] [D acc: 0.758(R 0.750, F 0.766)] [G loss: 0.480] [G acc: 0.094]\n",
      "5264 [D loss: 0.568(R 0.494, F0.642)] [D acc: 0.695(R 0.734, F 0.656)] [G loss: 0.568] [G acc: 0.047]\n",
      "5265 [D loss: 0.475(R 0.611, F0.340)] [D acc: 0.789(R 0.656, F 0.922)] [G loss: 0.475] [G acc: 0.031]\n",
      "5266 [D loss: 0.480(R 0.472, F0.489)] [D acc: 0.781(R 0.750, F 0.812)] [G loss: 0.480] [G acc: 0.062]\n",
      "5267 [D loss: 0.606(R 0.521, F0.691)] [D acc: 0.695(R 0.672, F 0.719)] [G loss: 0.606] [G acc: 0.094]\n",
      "5268 [D loss: 0.467(R 0.489, F0.445)] [D acc: 0.773(R 0.688, F 0.859)] [G loss: 0.467] [G acc: 0.078]\n",
      "5269 [D loss: 0.454(R 0.464, F0.444)] [D acc: 0.766(R 0.734, F 0.797)] [G loss: 0.454] [G acc: 0.109]\n",
      "5270 [D loss: 0.513(R 0.537, F0.489)] [D acc: 0.758(R 0.672, F 0.844)] [G loss: 0.513] [G acc: 0.062]\n",
      "5271 [D loss: 0.523(R 0.538, F0.508)] [D acc: 0.711(R 0.641, F 0.781)] [G loss: 0.523] [G acc: 0.156]\n",
      "5272 [D loss: 0.552(R 0.505, F0.600)] [D acc: 0.742(R 0.688, F 0.797)] [G loss: 0.552] [G acc: 0.094]\n",
      "5273 [D loss: 0.526(R 0.600, F0.452)] [D acc: 0.719(R 0.609, F 0.828)] [G loss: 0.526] [G acc: 0.031]\n",
      "5274 [D loss: 0.496(R 0.567, F0.425)] [D acc: 0.773(R 0.688, F 0.859)] [G loss: 0.496] [G acc: 0.047]\n",
      "5275 [D loss: 0.476(R 0.460, F0.492)] [D acc: 0.750(R 0.703, F 0.797)] [G loss: 0.476] [G acc: 0.188]\n",
      "5276 [D loss: 0.639(R 0.599, F0.679)] [D acc: 0.695(R 0.688, F 0.703)] [G loss: 0.639] [G acc: 0.031]\n",
      "5277 [D loss: 0.523(R 0.566, F0.479)] [D acc: 0.711(R 0.656, F 0.766)] [G loss: 0.523] [G acc: 0.047]\n",
      "5278 [D loss: 0.544(R 0.577, F0.512)] [D acc: 0.734(R 0.672, F 0.797)] [G loss: 0.544] [G acc: 0.047]\n",
      "5279 [D loss: 0.541(R 0.513, F0.568)] [D acc: 0.711(R 0.672, F 0.750)] [G loss: 0.541] [G acc: 0.109]\n",
      "5280 [D loss: 0.481(R 0.571, F0.391)] [D acc: 0.766(R 0.641, F 0.891)] [G loss: 0.481] [G acc: 0.016]\n",
      "5281 [D loss: 0.463(R 0.462, F0.464)] [D acc: 0.781(R 0.688, F 0.875)] [G loss: 0.463] [G acc: 0.109]\n",
      "5282 [D loss: 0.576(R 0.557, F0.594)] [D acc: 0.711(R 0.656, F 0.766)] [G loss: 0.576] [G acc: 0.078]\n",
      "5283 [D loss: 0.532(R 0.541, F0.523)] [D acc: 0.750(R 0.688, F 0.812)] [G loss: 0.532] [G acc: 0.094]\n",
      "5284 [D loss: 0.499(R 0.491, F0.507)] [D acc: 0.797(R 0.703, F 0.891)] [G loss: 0.499] [G acc: 0.031]\n",
      "5285 [D loss: 0.562(R 0.539, F0.584)] [D acc: 0.758(R 0.656, F 0.859)] [G loss: 0.562] [G acc: 0.031]\n",
      "5286 [D loss: 0.497(R 0.555, F0.440)] [D acc: 0.727(R 0.625, F 0.828)] [G loss: 0.497] [G acc: 0.141]\n",
      "5287 [D loss: 0.485(R 0.566, F0.405)] [D acc: 0.781(R 0.703, F 0.859)] [G loss: 0.485] [G acc: 0.000]\n",
      "5288 [D loss: 0.554(R 0.507, F0.600)] [D acc: 0.695(R 0.641, F 0.750)] [G loss: 0.554] [G acc: 0.062]\n",
      "5289 [D loss: 0.590(R 0.608, F0.571)] [D acc: 0.664(R 0.594, F 0.734)] [G loss: 0.590] [G acc: 0.141]\n",
      "5290 [D loss: 0.424(R 0.446, F0.402)] [D acc: 0.781(R 0.719, F 0.844)] [G loss: 0.424] [G acc: 0.062]\n",
      "5291 [D loss: 0.432(R 0.514, F0.350)] [D acc: 0.781(R 0.688, F 0.875)] [G loss: 0.432] [G acc: 0.203]\n",
      "5292 [D loss: 0.525(R 0.494, F0.557)] [D acc: 0.758(R 0.703, F 0.812)] [G loss: 0.525] [G acc: 0.078]\n",
      "5293 [D loss: 0.449(R 0.443, F0.454)] [D acc: 0.820(R 0.812, F 0.828)] [G loss: 0.449] [G acc: 0.047]\n",
      "5294 [D loss: 0.532(R 0.478, F0.585)] [D acc: 0.766(R 0.734, F 0.797)] [G loss: 0.532] [G acc: 0.094]\n",
      "5295 [D loss: 0.474(R 0.506, F0.443)] [D acc: 0.773(R 0.750, F 0.797)] [G loss: 0.474] [G acc: 0.125]\n",
      "5296 [D loss: 0.452(R 0.463, F0.442)] [D acc: 0.758(R 0.688, F 0.828)] [G loss: 0.452] [G acc: 0.062]\n",
      "5297 [D loss: 0.483(R 0.421, F0.546)] [D acc: 0.805(R 0.766, F 0.844)] [G loss: 0.483] [G acc: 0.109]\n",
      "5298 [D loss: 0.498(R 0.454, F0.542)] [D acc: 0.742(R 0.750, F 0.734)] [G loss: 0.498] [G acc: 0.016]\n",
      "5299 [D loss: 0.421(R 0.499, F0.342)] [D acc: 0.812(R 0.734, F 0.891)] [G loss: 0.421] [G acc: 0.109]\n",
      "INFO:tensorflow:Assets written to: ram://279ec0b5-602f-4653-957c-bae3fbdc1040/assets\n",
      "INFO:tensorflow:Assets written to: ram://51a53705-d80a-4487-8de6-a4774d8c04a5/assets\n",
      "INFO:tensorflow:Assets written to: ram://737aa2c8-d9eb-43fd-b066-464b018408e9/assets\n",
      "5300 [D loss: 0.609(R 0.631, F0.587)] [D acc: 0.711(R 0.656, F 0.766)] [G loss: 0.609] [G acc: 0.094]\n",
      "5301 [D loss: 0.445(R 0.603, F0.287)] [D acc: 0.789(R 0.625, F 0.953)] [G loss: 0.445] [G acc: 0.031]\n",
      "5302 [D loss: 0.404(R 0.396, F0.413)] [D acc: 0.812(R 0.750, F 0.875)] [G loss: 0.404] [G acc: 0.109]\n",
      "5303 [D loss: 0.713(R 0.454, F0.971)] [D acc: 0.633(R 0.703, F 0.562)] [G loss: 0.713] [G acc: 0.062]\n",
      "5304 [D loss: 0.552(R 0.672, F0.432)] [D acc: 0.680(R 0.547, F 0.812)] [G loss: 0.552] [G acc: 0.172]\n",
      "5305 [D loss: 0.465(R 0.416, F0.513)] [D acc: 0.781(R 0.766, F 0.797)] [G loss: 0.465] [G acc: 0.062]\n",
      "5306 [D loss: 0.492(R 0.538, F0.446)] [D acc: 0.750(R 0.641, F 0.859)] [G loss: 0.492] [G acc: 0.078]\n",
      "5307 [D loss: 0.409(R 0.368, F0.450)] [D acc: 0.836(R 0.859, F 0.812)] [G loss: 0.409] [G acc: 0.062]\n",
      "5308 [D loss: 0.431(R 0.469, F0.394)] [D acc: 0.773(R 0.766, F 0.781)] [G loss: 0.431] [G acc: 0.078]\n",
      "5309 [D loss: 0.464(R 0.465, F0.463)] [D acc: 0.781(R 0.719, F 0.844)] [G loss: 0.464] [G acc: 0.078]\n",
      "5310 [D loss: 0.616(R 0.547, F0.686)] [D acc: 0.727(R 0.766, F 0.688)] [G loss: 0.616] [G acc: 0.125]\n",
      "5311 [D loss: 0.456(R 0.506, F0.406)] [D acc: 0.789(R 0.688, F 0.891)] [G loss: 0.456] [G acc: 0.016]\n",
      "5312 [D loss: 0.489(R 0.538, F0.441)] [D acc: 0.719(R 0.641, F 0.797)] [G loss: 0.489] [G acc: 0.156]\n",
      "5313 [D loss: 0.528(R 0.602, F0.454)] [D acc: 0.773(R 0.703, F 0.844)] [G loss: 0.528] [G acc: 0.031]\n",
      "5314 [D loss: 0.506(R 0.542, F0.470)] [D acc: 0.758(R 0.719, F 0.797)] [G loss: 0.506] [G acc: 0.156]\n",
      "5315 [D loss: 0.518(R 0.544, F0.493)] [D acc: 0.711(R 0.625, F 0.797)] [G loss: 0.518] [G acc: 0.125]\n",
      "5316 [D loss: 0.436(R 0.405, F0.467)] [D acc: 0.836(R 0.797, F 0.875)] [G loss: 0.436] [G acc: 0.094]\n",
      "5317 [D loss: 0.586(R 0.636, F0.536)] [D acc: 0.680(R 0.625, F 0.734)] [G loss: 0.586] [G acc: 0.078]\n",
      "5318 [D loss: 0.473(R 0.534, F0.413)] [D acc: 0.805(R 0.734, F 0.875)] [G loss: 0.473] [G acc: 0.141]\n",
      "5319 [D loss: 0.508(R 0.455, F0.561)] [D acc: 0.773(R 0.750, F 0.797)] [G loss: 0.508] [G acc: 0.062]\n",
      "5320 [D loss: 0.510(R 0.532, F0.488)] [D acc: 0.750(R 0.703, F 0.797)] [G loss: 0.510] [G acc: 0.078]\n",
      "5321 [D loss: 0.450(R 0.446, F0.453)] [D acc: 0.766(R 0.703, F 0.828)] [G loss: 0.450] [G acc: 0.172]\n",
      "5322 [D loss: 0.513(R 0.607, F0.420)] [D acc: 0.742(R 0.625, F 0.859)] [G loss: 0.513] [G acc: 0.109]\n",
      "5323 [D loss: 0.523(R 0.624, F0.421)] [D acc: 0.742(R 0.609, F 0.875)] [G loss: 0.523] [G acc: 0.031]\n",
      "5324 [D loss: 0.482(R 0.484, F0.480)] [D acc: 0.758(R 0.719, F 0.797)] [G loss: 0.482] [G acc: 0.031]\n",
      "5325 [D loss: 0.557(R 0.525, F0.588)] [D acc: 0.680(R 0.625, F 0.734)] [G loss: 0.557] [G acc: 0.109]\n",
      "5326 [D loss: 0.493(R 0.457, F0.529)] [D acc: 0.766(R 0.734, F 0.797)] [G loss: 0.493] [G acc: 0.062]\n",
      "5327 [D loss: 0.535(R 0.578, F0.491)] [D acc: 0.719(R 0.641, F 0.797)] [G loss: 0.535] [G acc: 0.078]\n",
      "5328 [D loss: 0.511(R 0.522, F0.499)] [D acc: 0.781(R 0.703, F 0.859)] [G loss: 0.511] [G acc: 0.094]\n",
      "5329 [D loss: 0.555(R 0.691, F0.418)] [D acc: 0.703(R 0.562, F 0.844)] [G loss: 0.555] [G acc: 0.078]\n",
      "5330 [D loss: 0.529(R 0.402, F0.656)] [D acc: 0.750(R 0.750, F 0.750)] [G loss: 0.529] [G acc: 0.062]\n",
      "5331 [D loss: 0.507(R 0.611, F0.404)] [D acc: 0.773(R 0.641, F 0.906)] [G loss: 0.507] [G acc: 0.062]\n",
      "5332 [D loss: 0.569(R 0.507, F0.630)] [D acc: 0.742(R 0.703, F 0.781)] [G loss: 0.569] [G acc: 0.031]\n",
      "5333 [D loss: 0.556(R 0.729, F0.384)] [D acc: 0.773(R 0.641, F 0.906)] [G loss: 0.556] [G acc: 0.078]\n",
      "5334 [D loss: 0.604(R 0.673, F0.535)] [D acc: 0.680(R 0.562, F 0.797)] [G loss: 0.604] [G acc: 0.016]\n",
      "5335 [D loss: 0.404(R 0.491, F0.316)] [D acc: 0.852(R 0.766, F 0.938)] [G loss: 0.404] [G acc: 0.078]\n",
      "5336 [D loss: 0.479(R 0.503, F0.454)] [D acc: 0.758(R 0.734, F 0.781)] [G loss: 0.479] [G acc: 0.156]\n",
      "5337 [D loss: 0.591(R 0.510, F0.672)] [D acc: 0.680(R 0.688, F 0.672)] [G loss: 0.591] [G acc: 0.062]\n",
      "5338 [D loss: 0.498(R 0.533, F0.463)] [D acc: 0.789(R 0.750, F 0.828)] [G loss: 0.498] [G acc: 0.031]\n",
      "5339 [D loss: 0.409(R 0.399, F0.419)] [D acc: 0.781(R 0.734, F 0.828)] [G loss: 0.409] [G acc: 0.047]\n",
      "5340 [D loss: 0.546(R 0.497, F0.595)] [D acc: 0.703(R 0.734, F 0.672)] [G loss: 0.546] [G acc: 0.031]\n",
      "5341 [D loss: 0.395(R 0.469, F0.321)] [D acc: 0.859(R 0.766, F 0.953)] [G loss: 0.395] [G acc: 0.109]\n",
      "5342 [D loss: 0.558(R 0.565, F0.552)] [D acc: 0.688(R 0.641, F 0.734)] [G loss: 0.558] [G acc: 0.000]\n",
      "5343 [D loss: 0.486(R 0.541, F0.431)] [D acc: 0.781(R 0.688, F 0.875)] [G loss: 0.486] [G acc: 0.062]\n",
      "5344 [D loss: 0.580(R 0.702, F0.457)] [D acc: 0.695(R 0.562, F 0.828)] [G loss: 0.580] [G acc: 0.109]\n",
      "5345 [D loss: 0.487(R 0.463, F0.511)] [D acc: 0.734(R 0.672, F 0.797)] [G loss: 0.487] [G acc: 0.094]\n",
      "5346 [D loss: 0.453(R 0.540, F0.366)] [D acc: 0.781(R 0.641, F 0.922)] [G loss: 0.453] [G acc: 0.094]\n",
      "5347 [D loss: 0.532(R 0.452, F0.612)] [D acc: 0.742(R 0.812, F 0.672)] [G loss: 0.532] [G acc: 0.109]\n",
      "5348 [D loss: 0.451(R 0.461, F0.442)] [D acc: 0.781(R 0.766, F 0.797)] [G loss: 0.451] [G acc: 0.062]\n",
      "5349 [D loss: 0.407(R 0.339, F0.475)] [D acc: 0.789(R 0.781, F 0.797)] [G loss: 0.407] [G acc: 0.062]\n",
      "5350 [D loss: 0.439(R 0.449, F0.428)] [D acc: 0.805(R 0.719, F 0.891)] [G loss: 0.439] [G acc: 0.078]\n",
      "5351 [D loss: 0.551(R 0.536, F0.566)] [D acc: 0.758(R 0.734, F 0.781)] [G loss: 0.551] [G acc: 0.016]\n",
      "5352 [D loss: 0.398(R 0.453, F0.342)] [D acc: 0.852(R 0.734, F 0.969)] [G loss: 0.398] [G acc: 0.016]\n",
      "5353 [D loss: 0.502(R 0.549, F0.454)] [D acc: 0.742(R 0.641, F 0.844)] [G loss: 0.502] [G acc: 0.062]\n",
      "5354 [D loss: 0.579(R 0.503, F0.656)] [D acc: 0.727(R 0.672, F 0.781)] [G loss: 0.579] [G acc: 0.062]\n",
      "5355 [D loss: 0.598(R 0.778, F0.417)] [D acc: 0.680(R 0.500, F 0.859)] [G loss: 0.598] [G acc: 0.094]\n",
      "5356 [D loss: 0.416(R 0.411, F0.422)] [D acc: 0.758(R 0.688, F 0.828)] [G loss: 0.416] [G acc: 0.094]\n",
      "5357 [D loss: 0.484(R 0.503, F0.464)] [D acc: 0.812(R 0.750, F 0.875)] [G loss: 0.484] [G acc: 0.062]\n",
      "5358 [D loss: 0.505(R 0.520, F0.490)] [D acc: 0.758(R 0.688, F 0.828)] [G loss: 0.505] [G acc: 0.125]\n",
      "5359 [D loss: 0.500(R 0.533, F0.466)] [D acc: 0.758(R 0.688, F 0.828)] [G loss: 0.500] [G acc: 0.031]\n",
      "5360 [D loss: 0.377(R 0.342, F0.413)] [D acc: 0.828(R 0.797, F 0.859)] [G loss: 0.377] [G acc: 0.062]\n",
      "5361 [D loss: 0.473(R 0.454, F0.492)] [D acc: 0.758(R 0.703, F 0.812)] [G loss: 0.473] [G acc: 0.125]\n",
      "5362 [D loss: 0.382(R 0.388, F0.376)] [D acc: 0.828(R 0.766, F 0.891)] [G loss: 0.382] [G acc: 0.078]\n",
      "5363 [D loss: 0.390(R 0.385, F0.395)] [D acc: 0.836(R 0.750, F 0.922)] [G loss: 0.390] [G acc: 0.156]\n",
      "5364 [D loss: 0.543(R 0.526, F0.559)] [D acc: 0.742(R 0.688, F 0.797)] [G loss: 0.543] [G acc: 0.156]\n",
      "5365 [D loss: 0.404(R 0.329, F0.478)] [D acc: 0.812(R 0.828, F 0.797)] [G loss: 0.404] [G acc: 0.078]\n",
      "5366 [D loss: 0.563(R 0.481, F0.645)] [D acc: 0.734(R 0.703, F 0.766)] [G loss: 0.563] [G acc: 0.031]\n",
      "5367 [D loss: 0.440(R 0.492, F0.389)] [D acc: 0.805(R 0.750, F 0.859)] [G loss: 0.440] [G acc: 0.047]\n",
      "5368 [D loss: 0.481(R 0.555, F0.407)] [D acc: 0.742(R 0.656, F 0.828)] [G loss: 0.481] [G acc: 0.078]\n",
      "5369 [D loss: 0.535(R 0.501, F0.568)] [D acc: 0.766(R 0.750, F 0.781)] [G loss: 0.535] [G acc: 0.094]\n",
      "5370 [D loss: 0.559(R 0.625, F0.493)] [D acc: 0.719(R 0.625, F 0.812)] [G loss: 0.559] [G acc: 0.047]\n",
      "5371 [D loss: 0.461(R 0.592, F0.330)] [D acc: 0.812(R 0.672, F 0.953)] [G loss: 0.461] [G acc: 0.031]\n",
      "5372 [D loss: 0.532(R 0.427, F0.636)] [D acc: 0.789(R 0.719, F 0.859)] [G loss: 0.532] [G acc: 0.047]\n",
      "5373 [D loss: 0.520(R 0.497, F0.544)] [D acc: 0.750(R 0.719, F 0.781)] [G loss: 0.520] [G acc: 0.078]\n",
      "5374 [D loss: 0.544(R 0.497, F0.591)] [D acc: 0.719(R 0.688, F 0.750)] [G loss: 0.544] [G acc: 0.094]\n",
      "5375 [D loss: 0.565(R 0.525, F0.605)] [D acc: 0.711(R 0.750, F 0.672)] [G loss: 0.565] [G acc: 0.094]\n",
      "5376 [D loss: 0.459(R 0.470, F0.448)] [D acc: 0.797(R 0.766, F 0.828)] [G loss: 0.459] [G acc: 0.125]\n",
      "5377 [D loss: 0.433(R 0.498, F0.369)] [D acc: 0.805(R 0.719, F 0.891)] [G loss: 0.433] [G acc: 0.078]\n",
      "5378 [D loss: 0.428(R 0.453, F0.403)] [D acc: 0.766(R 0.750, F 0.781)] [G loss: 0.428] [G acc: 0.125]\n",
      "5379 [D loss: 0.518(R 0.465, F0.571)] [D acc: 0.750(R 0.750, F 0.750)] [G loss: 0.518] [G acc: 0.047]\n",
      "5380 [D loss: 0.581(R 0.691, F0.470)] [D acc: 0.695(R 0.547, F 0.844)] [G loss: 0.581] [G acc: 0.047]\n",
      "5381 [D loss: 0.444(R 0.445, F0.443)] [D acc: 0.789(R 0.734, F 0.844)] [G loss: 0.444] [G acc: 0.016]\n",
      "5382 [D loss: 0.576(R 0.441, F0.710)] [D acc: 0.773(R 0.750, F 0.797)] [G loss: 0.576] [G acc: 0.000]\n",
      "5383 [D loss: 0.484(R 0.616, F0.351)] [D acc: 0.773(R 0.641, F 0.906)] [G loss: 0.484] [G acc: 0.109]\n",
      "5384 [D loss: 0.518(R 0.590, F0.445)] [D acc: 0.727(R 0.625, F 0.828)] [G loss: 0.518] [G acc: 0.062]\n",
      "5385 [D loss: 0.501(R 0.557, F0.446)] [D acc: 0.742(R 0.656, F 0.828)] [G loss: 0.501] [G acc: 0.047]\n",
      "5386 [D loss: 0.450(R 0.416, F0.485)] [D acc: 0.797(R 0.766, F 0.828)] [G loss: 0.450] [G acc: 0.078]\n",
      "5387 [D loss: 0.468(R 0.510, F0.426)] [D acc: 0.820(R 0.734, F 0.906)] [G loss: 0.468] [G acc: 0.000]\n",
      "5388 [D loss: 0.523(R 0.562, F0.484)] [D acc: 0.727(R 0.719, F 0.734)] [G loss: 0.523] [G acc: 0.062]\n",
      "5389 [D loss: 0.498(R 0.632, F0.364)] [D acc: 0.781(R 0.656, F 0.906)] [G loss: 0.498] [G acc: 0.000]\n",
      "5390 [D loss: 0.459(R 0.436, F0.482)] [D acc: 0.781(R 0.781, F 0.781)] [G loss: 0.459] [G acc: 0.031]\n",
      "5391 [D loss: 0.499(R 0.467, F0.531)] [D acc: 0.742(R 0.703, F 0.781)] [G loss: 0.499] [G acc: 0.109]\n",
      "5392 [D loss: 0.533(R 0.525, F0.542)] [D acc: 0.734(R 0.703, F 0.766)] [G loss: 0.533] [G acc: 0.047]\n",
      "5393 [D loss: 0.515(R 0.479, F0.551)] [D acc: 0.742(R 0.688, F 0.797)] [G loss: 0.515] [G acc: 0.031]\n",
      "5394 [D loss: 0.440(R 0.500, F0.380)] [D acc: 0.773(R 0.688, F 0.859)] [G loss: 0.440] [G acc: 0.094]\n",
      "5395 [D loss: 0.397(R 0.428, F0.366)] [D acc: 0.844(R 0.797, F 0.891)] [G loss: 0.397] [G acc: 0.062]\n",
      "5396 [D loss: 0.572(R 0.548, F0.596)] [D acc: 0.680(R 0.625, F 0.734)] [G loss: 0.572] [G acc: 0.078]\n",
      "5397 [D loss: 0.540(R 0.548, F0.532)] [D acc: 0.742(R 0.672, F 0.812)] [G loss: 0.540] [G acc: 0.125]\n",
      "5398 [D loss: 0.482(R 0.582, F0.382)] [D acc: 0.797(R 0.672, F 0.922)] [G loss: 0.482] [G acc: 0.062]\n",
      "5399 [D loss: 0.469(R 0.454, F0.485)] [D acc: 0.797(R 0.750, F 0.844)] [G loss: 0.469] [G acc: 0.031]\n",
      "INFO:tensorflow:Assets written to: ram://e8518541-5827-45b0-9428-76063f277451/assets\n",
      "INFO:tensorflow:Assets written to: ram://0e6316c6-ae49-4b98-a2d1-20173d6dc25b/assets\n",
      "INFO:tensorflow:Assets written to: ram://cce7d0ba-d95b-452a-90fa-542ba1770669/assets\n",
      "5400 [D loss: 0.467(R 0.416, F0.517)] [D acc: 0.781(R 0.719, F 0.844)] [G loss: 0.467] [G acc: 0.016]\n",
      "5401 [D loss: 0.516(R 0.580, F0.452)] [D acc: 0.750(R 0.609, F 0.891)] [G loss: 0.516] [G acc: 0.016]\n",
      "5402 [D loss: 0.564(R 0.497, F0.631)] [D acc: 0.680(R 0.609, F 0.750)] [G loss: 0.564] [G acc: 0.062]\n",
      "5403 [D loss: 0.554(R 0.612, F0.497)] [D acc: 0.750(R 0.641, F 0.859)] [G loss: 0.554] [G acc: 0.000]\n",
      "5404 [D loss: 0.484(R 0.572, F0.396)] [D acc: 0.773(R 0.672, F 0.875)] [G loss: 0.484] [G acc: 0.062]\n",
      "5405 [D loss: 0.449(R 0.299, F0.599)] [D acc: 0.797(R 0.844, F 0.750)] [G loss: 0.449] [G acc: 0.078]\n",
      "5406 [D loss: 0.553(R 0.566, F0.539)] [D acc: 0.727(R 0.688, F 0.766)] [G loss: 0.553] [G acc: 0.031]\n",
      "5407 [D loss: 0.493(R 0.466, F0.520)] [D acc: 0.719(R 0.719, F 0.719)] [G loss: 0.493] [G acc: 0.109]\n",
      "5408 [D loss: 0.594(R 0.652, F0.536)] [D acc: 0.695(R 0.547, F 0.844)] [G loss: 0.594] [G acc: 0.031]\n",
      "5409 [D loss: 0.525(R 0.509, F0.540)] [D acc: 0.719(R 0.688, F 0.750)] [G loss: 0.525] [G acc: 0.031]\n",
      "5410 [D loss: 0.430(R 0.539, F0.322)] [D acc: 0.797(R 0.625, F 0.969)] [G loss: 0.430] [G acc: 0.000]\n",
      "5411 [D loss: 0.424(R 0.499, F0.349)] [D acc: 0.781(R 0.656, F 0.906)] [G loss: 0.424] [G acc: 0.031]\n",
      "5412 [D loss: 0.499(R 0.559, F0.439)] [D acc: 0.789(R 0.672, F 0.906)] [G loss: 0.499] [G acc: 0.078]\n",
      "5413 [D loss: 0.491(R 0.436, F0.546)] [D acc: 0.758(R 0.703, F 0.812)] [G loss: 0.491] [G acc: 0.078]\n",
      "5414 [D loss: 0.575(R 0.484, F0.666)] [D acc: 0.742(R 0.734, F 0.750)] [G loss: 0.575] [G acc: 0.078]\n",
      "5415 [D loss: 0.442(R 0.543, F0.340)] [D acc: 0.781(R 0.641, F 0.922)] [G loss: 0.442] [G acc: 0.078]\n",
      "5416 [D loss: 0.538(R 0.613, F0.464)] [D acc: 0.742(R 0.672, F 0.812)] [G loss: 0.538] [G acc: 0.125]\n",
      "5417 [D loss: 0.538(R 0.437, F0.639)] [D acc: 0.766(R 0.734, F 0.797)] [G loss: 0.538] [G acc: 0.047]\n",
      "5418 [D loss: 0.350(R 0.407, F0.292)] [D acc: 0.844(R 0.734, F 0.953)] [G loss: 0.350] [G acc: 0.031]\n",
      "5419 [D loss: 0.384(R 0.390, F0.378)] [D acc: 0.797(R 0.766, F 0.828)] [G loss: 0.384] [G acc: 0.078]\n",
      "5420 [D loss: 0.530(R 0.627, F0.433)] [D acc: 0.727(R 0.641, F 0.812)] [G loss: 0.530] [G acc: 0.109]\n",
      "5421 [D loss: 0.417(R 0.426, F0.408)] [D acc: 0.820(R 0.781, F 0.859)] [G loss: 0.417] [G acc: 0.141]\n",
      "5422 [D loss: 0.401(R 0.426, F0.375)] [D acc: 0.836(R 0.797, F 0.875)] [G loss: 0.401] [G acc: 0.125]\n",
      "5423 [D loss: 0.443(R 0.355, F0.530)] [D acc: 0.758(R 0.797, F 0.719)] [G loss: 0.443] [G acc: 0.094]\n",
      "5424 [D loss: 0.450(R 0.545, F0.356)] [D acc: 0.781(R 0.672, F 0.891)] [G loss: 0.450] [G acc: 0.031]\n",
      "5425 [D loss: 0.467(R 0.432, F0.503)] [D acc: 0.781(R 0.750, F 0.812)] [G loss: 0.467] [G acc: 0.031]\n",
      "5426 [D loss: 0.440(R 0.556, F0.325)] [D acc: 0.781(R 0.656, F 0.906)] [G loss: 0.440] [G acc: 0.109]\n",
      "5427 [D loss: 0.555(R 0.502, F0.608)] [D acc: 0.703(R 0.734, F 0.672)] [G loss: 0.555] [G acc: 0.047]\n",
      "5428 [D loss: 0.404(R 0.531, F0.278)] [D acc: 0.797(R 0.656, F 0.938)] [G loss: 0.404] [G acc: 0.031]\n",
      "5429 [D loss: 0.572(R 0.622, F0.521)] [D acc: 0.703(R 0.625, F 0.781)] [G loss: 0.572] [G acc: 0.000]\n",
      "5430 [D loss: 0.506(R 0.522, F0.491)] [D acc: 0.750(R 0.734, F 0.766)] [G loss: 0.506] [G acc: 0.078]\n",
      "5431 [D loss: 0.377(R 0.372, F0.383)] [D acc: 0.859(R 0.812, F 0.906)] [G loss: 0.377] [G acc: 0.062]\n",
      "5432 [D loss: 0.483(R 0.578, F0.389)] [D acc: 0.773(R 0.688, F 0.859)] [G loss: 0.483] [G acc: 0.047]\n",
      "5433 [D loss: 0.594(R 0.683, F0.504)] [D acc: 0.727(R 0.625, F 0.828)] [G loss: 0.594] [G acc: 0.109]\n",
      "5434 [D loss: 0.492(R 0.434, F0.549)] [D acc: 0.750(R 0.703, F 0.797)] [G loss: 0.492] [G acc: 0.062]\n",
      "5435 [D loss: 0.587(R 0.535, F0.638)] [D acc: 0.711(R 0.703, F 0.719)] [G loss: 0.587] [G acc: 0.062]\n",
      "5436 [D loss: 0.565(R 0.638, F0.492)] [D acc: 0.703(R 0.578, F 0.828)] [G loss: 0.565] [G acc: 0.047]\n",
      "5437 [D loss: 0.608(R 0.670, F0.546)] [D acc: 0.703(R 0.609, F 0.797)] [G loss: 0.608] [G acc: 0.125]\n",
      "5438 [D loss: 0.522(R 0.518, F0.525)] [D acc: 0.742(R 0.688, F 0.797)] [G loss: 0.522] [G acc: 0.094]\n",
      "5439 [D loss: 0.446(R 0.521, F0.371)] [D acc: 0.797(R 0.688, F 0.906)] [G loss: 0.446] [G acc: 0.047]\n",
      "5440 [D loss: 0.425(R 0.344, F0.506)] [D acc: 0.836(R 0.844, F 0.828)] [G loss: 0.425] [G acc: 0.047]\n",
      "5441 [D loss: 0.400(R 0.413, F0.387)] [D acc: 0.852(R 0.781, F 0.922)] [G loss: 0.400] [G acc: 0.094]\n",
      "5442 [D loss: 0.498(R 0.463, F0.533)] [D acc: 0.773(R 0.719, F 0.828)] [G loss: 0.498] [G acc: 0.031]\n",
      "5443 [D loss: 0.492(R 0.513, F0.471)] [D acc: 0.773(R 0.703, F 0.844)] [G loss: 0.492] [G acc: 0.016]\n",
      "5444 [D loss: 0.564(R 0.624, F0.503)] [D acc: 0.727(R 0.641, F 0.812)] [G loss: 0.564] [G acc: 0.016]\n",
      "5445 [D loss: 0.451(R 0.450, F0.451)] [D acc: 0.789(R 0.781, F 0.797)] [G loss: 0.451] [G acc: 0.109]\n",
      "5446 [D loss: 0.446(R 0.531, F0.360)] [D acc: 0.789(R 0.703, F 0.875)] [G loss: 0.446] [G acc: 0.062]\n",
      "5447 [D loss: 0.435(R 0.421, F0.449)] [D acc: 0.805(R 0.734, F 0.875)] [G loss: 0.435] [G acc: 0.062]\n",
      "5448 [D loss: 0.549(R 0.574, F0.523)] [D acc: 0.703(R 0.656, F 0.750)] [G loss: 0.549] [G acc: 0.094]\n",
      "5449 [D loss: 0.506(R 0.579, F0.434)] [D acc: 0.727(R 0.609, F 0.844)] [G loss: 0.506] [G acc: 0.094]\n",
      "5450 [D loss: 0.509(R 0.441, F0.577)] [D acc: 0.797(R 0.797, F 0.797)] [G loss: 0.509] [G acc: 0.062]\n",
      "5451 [D loss: 0.519(R 0.660, F0.378)] [D acc: 0.789(R 0.641, F 0.938)] [G loss: 0.519] [G acc: 0.062]\n",
      "5452 [D loss: 0.392(R 0.357, F0.428)] [D acc: 0.828(R 0.797, F 0.859)] [G loss: 0.392] [G acc: 0.078]\n",
      "5453 [D loss: 0.509(R 0.474, F0.544)] [D acc: 0.703(R 0.703, F 0.703)] [G loss: 0.509] [G acc: 0.125]\n",
      "5454 [D loss: 0.462(R 0.514, F0.411)] [D acc: 0.734(R 0.594, F 0.875)] [G loss: 0.462] [G acc: 0.125]\n",
      "5455 [D loss: 0.446(R 0.342, F0.551)] [D acc: 0.797(R 0.797, F 0.797)] [G loss: 0.446] [G acc: 0.078]\n",
      "5456 [D loss: 0.535(R 0.620, F0.450)] [D acc: 0.727(R 0.609, F 0.844)] [G loss: 0.535] [G acc: 0.094]\n",
      "5457 [D loss: 0.495(R 0.531, F0.460)] [D acc: 0.773(R 0.734, F 0.812)] [G loss: 0.495] [G acc: 0.109]\n",
      "5458 [D loss: 0.545(R 0.478, F0.612)] [D acc: 0.734(R 0.688, F 0.781)] [G loss: 0.545] [G acc: 0.109]\n",
      "5459 [D loss: 0.590(R 0.633, F0.546)] [D acc: 0.703(R 0.656, F 0.750)] [G loss: 0.590] [G acc: 0.078]\n",
      "5460 [D loss: 0.570(R 0.576, F0.565)] [D acc: 0.703(R 0.625, F 0.781)] [G loss: 0.570] [G acc: 0.094]\n",
      "5461 [D loss: 0.466(R 0.467, F0.465)] [D acc: 0.773(R 0.719, F 0.828)] [G loss: 0.466] [G acc: 0.047]\n",
      "5462 [D loss: 0.471(R 0.519, F0.423)] [D acc: 0.773(R 0.672, F 0.875)] [G loss: 0.471] [G acc: 0.062]\n",
      "5463 [D loss: 0.534(R 0.445, F0.623)] [D acc: 0.727(R 0.703, F 0.750)] [G loss: 0.534] [G acc: 0.141]\n",
      "5464 [D loss: 0.563(R 0.592, F0.534)] [D acc: 0.672(R 0.625, F 0.719)] [G loss: 0.563] [G acc: 0.094]\n",
      "5465 [D loss: 0.520(R 0.599, F0.440)] [D acc: 0.750(R 0.656, F 0.844)] [G loss: 0.520] [G acc: 0.109]\n",
      "5466 [D loss: 0.474(R 0.537, F0.412)] [D acc: 0.773(R 0.672, F 0.875)] [G loss: 0.474] [G acc: 0.109]\n",
      "5467 [D loss: 0.606(R 0.372, F0.841)] [D acc: 0.734(R 0.781, F 0.688)] [G loss: 0.606] [G acc: 0.031]\n",
      "5468 [D loss: 0.554(R 0.696, F0.412)] [D acc: 0.711(R 0.562, F 0.859)] [G loss: 0.554] [G acc: 0.094]\n",
      "5469 [D loss: 0.471(R 0.458, F0.485)] [D acc: 0.781(R 0.719, F 0.844)] [G loss: 0.471] [G acc: 0.094]\n",
      "5470 [D loss: 0.427(R 0.497, F0.356)] [D acc: 0.828(R 0.734, F 0.922)] [G loss: 0.427] [G acc: 0.078]\n",
      "5471 [D loss: 0.426(R 0.440, F0.413)] [D acc: 0.844(R 0.766, F 0.922)] [G loss: 0.426] [G acc: 0.078]\n",
      "5472 [D loss: 0.453(R 0.445, F0.461)] [D acc: 0.773(R 0.734, F 0.812)] [G loss: 0.453] [G acc: 0.016]\n",
      "5473 [D loss: 0.487(R 0.452, F0.521)] [D acc: 0.766(R 0.734, F 0.797)] [G loss: 0.487] [G acc: 0.062]\n",
      "5474 [D loss: 0.459(R 0.510, F0.409)] [D acc: 0.789(R 0.688, F 0.891)] [G loss: 0.459] [G acc: 0.047]\n",
      "5475 [D loss: 0.481(R 0.544, F0.418)] [D acc: 0.750(R 0.688, F 0.812)] [G loss: 0.481] [G acc: 0.078]\n",
      "5476 [D loss: 0.524(R 0.516, F0.531)] [D acc: 0.750(R 0.688, F 0.812)] [G loss: 0.524] [G acc: 0.078]\n",
      "5477 [D loss: 0.563(R 0.742, F0.385)] [D acc: 0.711(R 0.547, F 0.875)] [G loss: 0.563] [G acc: 0.141]\n",
      "5478 [D loss: 0.507(R 0.531, F0.483)] [D acc: 0.719(R 0.625, F 0.812)] [G loss: 0.507] [G acc: 0.016]\n",
      "5479 [D loss: 0.448(R 0.412, F0.484)] [D acc: 0.781(R 0.766, F 0.797)] [G loss: 0.448] [G acc: 0.156]\n",
      "5480 [D loss: 0.581(R 0.518, F0.643)] [D acc: 0.719(R 0.703, F 0.734)] [G loss: 0.581] [G acc: 0.078]\n",
      "5481 [D loss: 0.544(R 0.612, F0.477)] [D acc: 0.797(R 0.703, F 0.891)] [G loss: 0.544] [G acc: 0.047]\n",
      "5482 [D loss: 0.580(R 0.587, F0.574)] [D acc: 0.664(R 0.594, F 0.734)] [G loss: 0.580] [G acc: 0.094]\n",
      "5483 [D loss: 0.506(R 0.493, F0.519)] [D acc: 0.742(R 0.719, F 0.766)] [G loss: 0.506] [G acc: 0.078]\n",
      "5484 [D loss: 0.495(R 0.555, F0.435)] [D acc: 0.742(R 0.625, F 0.859)] [G loss: 0.495] [G acc: 0.078]\n",
      "5485 [D loss: 0.551(R 0.512, F0.590)] [D acc: 0.742(R 0.703, F 0.781)] [G loss: 0.551] [G acc: 0.094]\n",
      "5486 [D loss: 0.521(R 0.627, F0.414)] [D acc: 0.734(R 0.625, F 0.844)] [G loss: 0.521] [G acc: 0.047]\n",
      "5487 [D loss: 0.569(R 0.641, F0.498)] [D acc: 0.703(R 0.609, F 0.797)] [G loss: 0.569] [G acc: 0.062]\n",
      "5488 [D loss: 0.531(R 0.503, F0.560)] [D acc: 0.773(R 0.734, F 0.812)] [G loss: 0.531] [G acc: 0.031]\n",
      "5489 [D loss: 0.483(R 0.562, F0.403)] [D acc: 0.773(R 0.703, F 0.844)] [G loss: 0.483] [G acc: 0.109]\n",
      "5490 [D loss: 0.474(R 0.519, F0.428)] [D acc: 0.773(R 0.688, F 0.859)] [G loss: 0.474] [G acc: 0.078]\n",
      "5491 [D loss: 0.565(R 0.573, F0.557)] [D acc: 0.742(R 0.688, F 0.797)] [G loss: 0.565] [G acc: 0.062]\n",
      "5492 [D loss: 0.504(R 0.493, F0.516)] [D acc: 0.742(R 0.703, F 0.781)] [G loss: 0.504] [G acc: 0.047]\n",
      "5493 [D loss: 0.523(R 0.505, F0.542)] [D acc: 0.742(R 0.656, F 0.828)] [G loss: 0.523] [G acc: 0.109]\n",
      "5494 [D loss: 0.500(R 0.556, F0.444)] [D acc: 0.766(R 0.641, F 0.891)] [G loss: 0.500] [G acc: 0.031]\n",
      "5495 [D loss: 0.447(R 0.503, F0.390)] [D acc: 0.836(R 0.750, F 0.922)] [G loss: 0.447] [G acc: 0.047]\n",
      "5496 [D loss: 0.469(R 0.518, F0.420)] [D acc: 0.781(R 0.719, F 0.844)] [G loss: 0.469] [G acc: 0.094]\n",
      "5497 [D loss: 0.481(R 0.418, F0.543)] [D acc: 0.727(R 0.750, F 0.703)] [G loss: 0.481] [G acc: 0.016]\n",
      "5498 [D loss: 0.520(R 0.411, F0.630)] [D acc: 0.750(R 0.766, F 0.734)] [G loss: 0.520] [G acc: 0.062]\n",
      "5499 [D loss: 0.553(R 0.684, F0.422)] [D acc: 0.719(R 0.578, F 0.859)] [G loss: 0.553] [G acc: 0.047]\n",
      "INFO:tensorflow:Assets written to: ram://faa724b2-1df1-478a-b856-4102a8adb772/assets\n",
      "INFO:tensorflow:Assets written to: ram://da4ff574-f934-42cb-bc12-0f6a86b53b0e/assets\n",
      "INFO:tensorflow:Assets written to: ram://7c2a6be5-fab5-4141-ae85-58795d7e5523/assets\n",
      "5500 [D loss: 0.445(R 0.455, F0.435)] [D acc: 0.828(R 0.797, F 0.859)] [G loss: 0.445] [G acc: 0.047]\n",
      "5501 [D loss: 0.506(R 0.541, F0.471)] [D acc: 0.766(R 0.672, F 0.859)] [G loss: 0.506] [G acc: 0.078]\n",
      "5502 [D loss: 0.557(R 0.548, F0.565)] [D acc: 0.719(R 0.609, F 0.828)] [G loss: 0.557] [G acc: 0.047]\n",
      "5503 [D loss: 0.670(R 0.815, F0.525)] [D acc: 0.664(R 0.500, F 0.828)] [G loss: 0.670] [G acc: 0.062]\n",
      "5504 [D loss: 0.506(R 0.580, F0.432)] [D acc: 0.727(R 0.625, F 0.828)] [G loss: 0.506] [G acc: 0.078]\n",
      "5505 [D loss: 0.465(R 0.464, F0.466)] [D acc: 0.781(R 0.734, F 0.828)] [G loss: 0.465] [G acc: 0.109]\n",
      "5506 [D loss: 0.458(R 0.478, F0.437)] [D acc: 0.789(R 0.734, F 0.844)] [G loss: 0.458] [G acc: 0.078]\n",
      "5507 [D loss: 0.512(R 0.527, F0.497)] [D acc: 0.711(R 0.641, F 0.781)] [G loss: 0.512] [G acc: 0.062]\n",
      "5508 [D loss: 0.468(R 0.385, F0.550)] [D acc: 0.750(R 0.781, F 0.719)] [G loss: 0.468] [G acc: 0.062]\n",
      "5509 [D loss: 0.515(R 0.459, F0.570)] [D acc: 0.797(R 0.750, F 0.844)] [G loss: 0.515] [G acc: 0.047]\n",
      "5510 [D loss: 0.516(R 0.555, F0.478)] [D acc: 0.742(R 0.625, F 0.859)] [G loss: 0.516] [G acc: 0.094]\n",
      "5511 [D loss: 0.536(R 0.456, F0.616)] [D acc: 0.766(R 0.766, F 0.766)] [G loss: 0.536] [G acc: 0.047]\n",
      "5512 [D loss: 0.510(R 0.626, F0.393)] [D acc: 0.750(R 0.641, F 0.859)] [G loss: 0.510] [G acc: 0.062]\n",
      "5513 [D loss: 0.409(R 0.479, F0.339)] [D acc: 0.797(R 0.672, F 0.922)] [G loss: 0.409] [G acc: 0.094]\n",
      "5514 [D loss: 0.448(R 0.439, F0.456)] [D acc: 0.789(R 0.734, F 0.844)] [G loss: 0.448] [G acc: 0.094]\n",
      "5515 [D loss: 0.487(R 0.516, F0.458)] [D acc: 0.750(R 0.656, F 0.844)] [G loss: 0.487] [G acc: 0.156]\n",
      "5516 [D loss: 0.438(R 0.497, F0.379)] [D acc: 0.781(R 0.688, F 0.875)] [G loss: 0.438] [G acc: 0.094]\n",
      "5517 [D loss: 0.580(R 0.590, F0.571)] [D acc: 0.680(R 0.641, F 0.719)] [G loss: 0.580] [G acc: 0.172]\n",
      "5518 [D loss: 0.423(R 0.475, F0.372)] [D acc: 0.812(R 0.750, F 0.875)] [G loss: 0.423] [G acc: 0.047]\n",
      "5519 [D loss: 0.466(R 0.463, F0.469)] [D acc: 0.758(R 0.750, F 0.766)] [G loss: 0.466] [G acc: 0.078]\n",
      "5520 [D loss: 0.398(R 0.381, F0.415)] [D acc: 0.805(R 0.781, F 0.828)] [G loss: 0.398] [G acc: 0.062]\n",
      "5521 [D loss: 0.565(R 0.531, F0.600)] [D acc: 0.711(R 0.703, F 0.719)] [G loss: 0.565] [G acc: 0.109]\n",
      "5522 [D loss: 0.462(R 0.480, F0.443)] [D acc: 0.750(R 0.672, F 0.828)] [G loss: 0.462] [G acc: 0.141]\n",
      "5523 [D loss: 0.459(R 0.479, F0.440)] [D acc: 0.766(R 0.734, F 0.797)] [G loss: 0.459] [G acc: 0.109]\n",
      "5524 [D loss: 0.554(R 0.504, F0.605)] [D acc: 0.758(R 0.734, F 0.781)] [G loss: 0.554] [G acc: 0.047]\n",
      "5525 [D loss: 0.403(R 0.478, F0.328)] [D acc: 0.828(R 0.703, F 0.953)] [G loss: 0.403] [G acc: 0.094]\n",
      "5526 [D loss: 0.528(R 0.425, F0.632)] [D acc: 0.742(R 0.734, F 0.750)] [G loss: 0.528] [G acc: 0.031]\n",
      "5527 [D loss: 0.584(R 0.587, F0.581)] [D acc: 0.711(R 0.672, F 0.750)] [G loss: 0.584] [G acc: 0.094]\n",
      "5528 [D loss: 0.468(R 0.695, F0.240)] [D acc: 0.805(R 0.625, F 0.984)] [G loss: 0.468] [G acc: 0.047]\n",
      "5529 [D loss: 0.461(R 0.501, F0.421)] [D acc: 0.773(R 0.656, F 0.891)] [G loss: 0.461] [G acc: 0.141]\n",
      "5530 [D loss: 0.436(R 0.379, F0.494)] [D acc: 0.805(R 0.797, F 0.812)] [G loss: 0.436] [G acc: 0.078]\n",
      "5531 [D loss: 0.568(R 0.551, F0.585)] [D acc: 0.742(R 0.703, F 0.781)] [G loss: 0.568] [G acc: 0.188]\n",
      "5532 [D loss: 0.575(R 0.499, F0.652)] [D acc: 0.734(R 0.703, F 0.766)] [G loss: 0.575] [G acc: 0.078]\n",
      "5533 [D loss: 0.531(R 0.557, F0.505)] [D acc: 0.742(R 0.672, F 0.812)] [G loss: 0.531] [G acc: 0.109]\n",
      "5534 [D loss: 0.390(R 0.403, F0.377)] [D acc: 0.867(R 0.828, F 0.906)] [G loss: 0.390] [G acc: 0.031]\n",
      "5535 [D loss: 0.531(R 0.434, F0.627)] [D acc: 0.789(R 0.828, F 0.750)] [G loss: 0.531] [G acc: 0.078]\n",
      "5536 [D loss: 0.509(R 0.575, F0.444)] [D acc: 0.742(R 0.672, F 0.812)] [G loss: 0.509] [G acc: 0.062]\n",
      "5537 [D loss: 0.547(R 0.565, F0.530)] [D acc: 0.680(R 0.656, F 0.703)] [G loss: 0.547] [G acc: 0.078]\n",
      "5538 [D loss: 0.480(R 0.518, F0.442)] [D acc: 0.781(R 0.703, F 0.859)] [G loss: 0.480] [G acc: 0.031]\n",
      "5539 [D loss: 0.459(R 0.468, F0.449)] [D acc: 0.766(R 0.719, F 0.812)] [G loss: 0.459] [G acc: 0.125]\n",
      "5540 [D loss: 0.495(R 0.425, F0.565)] [D acc: 0.742(R 0.703, F 0.781)] [G loss: 0.495] [G acc: 0.078]\n",
      "5541 [D loss: 0.482(R 0.553, F0.411)] [D acc: 0.805(R 0.688, F 0.922)] [G loss: 0.482] [G acc: 0.062]\n",
      "5542 [D loss: 0.428(R 0.401, F0.455)] [D acc: 0.750(R 0.719, F 0.781)] [G loss: 0.428] [G acc: 0.094]\n",
      "5543 [D loss: 0.425(R 0.466, F0.384)] [D acc: 0.789(R 0.719, F 0.859)] [G loss: 0.425] [G acc: 0.203]\n",
      "5544 [D loss: 0.445(R 0.334, F0.557)] [D acc: 0.805(R 0.812, F 0.797)] [G loss: 0.445] [G acc: 0.031]\n",
      "5545 [D loss: 0.507(R 0.576, F0.438)] [D acc: 0.766(R 0.688, F 0.844)] [G loss: 0.507] [G acc: 0.125]\n",
      "5546 [D loss: 0.530(R 0.523, F0.538)] [D acc: 0.750(R 0.672, F 0.828)] [G loss: 0.530] [G acc: 0.031]\n",
      "5547 [D loss: 0.450(R 0.536, F0.365)] [D acc: 0.766(R 0.688, F 0.844)] [G loss: 0.450] [G acc: 0.078]\n",
      "5548 [D loss: 0.583(R 0.621, F0.545)] [D acc: 0.703(R 0.641, F 0.766)] [G loss: 0.583] [G acc: 0.109]\n",
      "5549 [D loss: 0.491(R 0.548, F0.434)] [D acc: 0.742(R 0.672, F 0.812)] [G loss: 0.491] [G acc: 0.062]\n",
      "5550 [D loss: 0.550(R 0.633, F0.467)] [D acc: 0.727(R 0.625, F 0.828)] [G loss: 0.550] [G acc: 0.125]\n",
      "5551 [D loss: 0.481(R 0.473, F0.490)] [D acc: 0.820(R 0.750, F 0.891)] [G loss: 0.481] [G acc: 0.109]\n",
      "5552 [D loss: 0.413(R 0.393, F0.432)] [D acc: 0.836(R 0.828, F 0.844)] [G loss: 0.413] [G acc: 0.031]\n",
      "5553 [D loss: 0.554(R 0.520, F0.589)] [D acc: 0.695(R 0.672, F 0.719)] [G loss: 0.554] [G acc: 0.047]\n",
      "5554 [D loss: 0.534(R 0.694, F0.374)] [D acc: 0.758(R 0.609, F 0.906)] [G loss: 0.534] [G acc: 0.062]\n",
      "5555 [D loss: 0.478(R 0.519, F0.438)] [D acc: 0.758(R 0.656, F 0.859)] [G loss: 0.478] [G acc: 0.047]\n",
      "5556 [D loss: 0.473(R 0.442, F0.505)] [D acc: 0.812(R 0.734, F 0.891)] [G loss: 0.473] [G acc: 0.078]\n",
      "5557 [D loss: 0.386(R 0.369, F0.402)] [D acc: 0.805(R 0.781, F 0.828)] [G loss: 0.386] [G acc: 0.031]\n",
      "5558 [D loss: 0.513(R 0.525, F0.501)] [D acc: 0.766(R 0.672, F 0.859)] [G loss: 0.513] [G acc: 0.078]\n",
      "5559 [D loss: 0.489(R 0.473, F0.505)] [D acc: 0.773(R 0.719, F 0.828)] [G loss: 0.489] [G acc: 0.016]\n",
      "5560 [D loss: 0.556(R 0.516, F0.596)] [D acc: 0.781(R 0.719, F 0.844)] [G loss: 0.556] [G acc: 0.094]\n",
      "5561 [D loss: 0.533(R 0.574, F0.491)] [D acc: 0.750(R 0.688, F 0.812)] [G loss: 0.533] [G acc: 0.062]\n",
      "5562 [D loss: 0.377(R 0.410, F0.343)] [D acc: 0.836(R 0.750, F 0.922)] [G loss: 0.377] [G acc: 0.094]\n",
      "5563 [D loss: 0.484(R 0.466, F0.501)] [D acc: 0.758(R 0.703, F 0.812)] [G loss: 0.484] [G acc: 0.094]\n",
      "5564 [D loss: 0.533(R 0.534, F0.532)] [D acc: 0.711(R 0.641, F 0.781)] [G loss: 0.533] [G acc: 0.047]\n",
      "5565 [D loss: 0.448(R 0.472, F0.425)] [D acc: 0.789(R 0.750, F 0.828)] [G loss: 0.448] [G acc: 0.031]\n",
      "5566 [D loss: 0.475(R 0.453, F0.498)] [D acc: 0.805(R 0.750, F 0.859)] [G loss: 0.475] [G acc: 0.031]\n",
      "5567 [D loss: 0.592(R 0.641, F0.542)] [D acc: 0.766(R 0.641, F 0.891)] [G loss: 0.592] [G acc: 0.031]\n",
      "5568 [D loss: 0.515(R 0.565, F0.465)] [D acc: 0.766(R 0.703, F 0.828)] [G loss: 0.515] [G acc: 0.125]\n",
      "5569 [D loss: 0.509(R 0.403, F0.615)] [D acc: 0.742(R 0.797, F 0.688)] [G loss: 0.509] [G acc: 0.062]\n",
      "5570 [D loss: 0.516(R 0.523, F0.510)] [D acc: 0.727(R 0.688, F 0.766)] [G loss: 0.516] [G acc: 0.062]\n",
      "5571 [D loss: 0.540(R 0.645, F0.435)] [D acc: 0.703(R 0.625, F 0.781)] [G loss: 0.540] [G acc: 0.031]\n",
      "5572 [D loss: 0.507(R 0.498, F0.515)] [D acc: 0.766(R 0.734, F 0.797)] [G loss: 0.507] [G acc: 0.109]\n",
      "5573 [D loss: 0.560(R 0.700, F0.421)] [D acc: 0.703(R 0.562, F 0.844)] [G loss: 0.560] [G acc: 0.062]\n",
      "5574 [D loss: 0.572(R 0.620, F0.524)] [D acc: 0.648(R 0.578, F 0.719)] [G loss: 0.572] [G acc: 0.047]\n",
      "5575 [D loss: 0.548(R 0.619, F0.477)] [D acc: 0.719(R 0.609, F 0.828)] [G loss: 0.548] [G acc: 0.078]\n",
      "5576 [D loss: 0.552(R 0.584, F0.519)] [D acc: 0.734(R 0.625, F 0.844)] [G loss: 0.552] [G acc: 0.078]\n",
      "5577 [D loss: 0.568(R 0.598, F0.539)] [D acc: 0.688(R 0.641, F 0.734)] [G loss: 0.568] [G acc: 0.062]\n",
      "5578 [D loss: 0.515(R 0.614, F0.415)] [D acc: 0.711(R 0.625, F 0.797)] [G loss: 0.515] [G acc: 0.031]\n",
      "5579 [D loss: 0.522(R 0.536, F0.508)] [D acc: 0.773(R 0.703, F 0.844)] [G loss: 0.522] [G acc: 0.109]\n",
      "5580 [D loss: 0.508(R 0.449, F0.567)] [D acc: 0.734(R 0.688, F 0.781)] [G loss: 0.508] [G acc: 0.047]\n",
      "5581 [D loss: 0.450(R 0.512, F0.388)] [D acc: 0.797(R 0.672, F 0.922)] [G loss: 0.450] [G acc: 0.062]\n",
      "5582 [D loss: 0.534(R 0.527, F0.541)] [D acc: 0.711(R 0.625, F 0.797)] [G loss: 0.534] [G acc: 0.047]\n",
      "5583 [D loss: 0.419(R 0.399, F0.439)] [D acc: 0.805(R 0.766, F 0.844)] [G loss: 0.419] [G acc: 0.016]\n",
      "5584 [D loss: 0.435(R 0.456, F0.413)] [D acc: 0.797(R 0.766, F 0.828)] [G loss: 0.435] [G acc: 0.109]\n",
      "5585 [D loss: 0.458(R 0.458, F0.459)] [D acc: 0.719(R 0.656, F 0.781)] [G loss: 0.458] [G acc: 0.094]\n",
      "5586 [D loss: 0.495(R 0.469, F0.521)] [D acc: 0.750(R 0.703, F 0.797)] [G loss: 0.495] [G acc: 0.078]\n",
      "5587 [D loss: 0.625(R 0.600, F0.649)] [D acc: 0.672(R 0.609, F 0.734)] [G loss: 0.625] [G acc: 0.047]\n",
      "5588 [D loss: 0.430(R 0.488, F0.372)] [D acc: 0.828(R 0.750, F 0.906)] [G loss: 0.430] [G acc: 0.109]\n",
      "5589 [D loss: 0.474(R 0.548, F0.400)] [D acc: 0.773(R 0.672, F 0.875)] [G loss: 0.474] [G acc: 0.062]\n",
      "5590 [D loss: 0.472(R 0.515, F0.429)] [D acc: 0.766(R 0.688, F 0.844)] [G loss: 0.472] [G acc: 0.109]\n",
      "5591 [D loss: 0.426(R 0.387, F0.464)] [D acc: 0.852(R 0.812, F 0.891)] [G loss: 0.426] [G acc: 0.062]\n",
      "5592 [D loss: 0.472(R 0.425, F0.519)] [D acc: 0.742(R 0.703, F 0.781)] [G loss: 0.472] [G acc: 0.156]\n",
      "5593 [D loss: 0.504(R 0.445, F0.563)] [D acc: 0.781(R 0.734, F 0.828)] [G loss: 0.504] [G acc: 0.109]\n",
      "5594 [D loss: 0.477(R 0.549, F0.405)] [D acc: 0.773(R 0.703, F 0.844)] [G loss: 0.477] [G acc: 0.047]\n",
      "5595 [D loss: 0.444(R 0.391, F0.497)] [D acc: 0.820(R 0.797, F 0.844)] [G loss: 0.444] [G acc: 0.047]\n",
      "5596 [D loss: 0.454(R 0.463, F0.445)] [D acc: 0.773(R 0.734, F 0.812)] [G loss: 0.454] [G acc: 0.016]\n",
      "5597 [D loss: 0.430(R 0.414, F0.446)] [D acc: 0.797(R 0.750, F 0.844)] [G loss: 0.430] [G acc: 0.062]\n",
      "5598 [D loss: 0.454(R 0.523, F0.386)] [D acc: 0.773(R 0.656, F 0.891)] [G loss: 0.454] [G acc: 0.031]\n",
      "5599 [D loss: 0.456(R 0.495, F0.416)] [D acc: 0.766(R 0.688, F 0.844)] [G loss: 0.456] [G acc: 0.156]\n",
      "INFO:tensorflow:Assets written to: ram://6c59514a-8043-45a6-91a8-7e828bb0bfc6/assets\n",
      "INFO:tensorflow:Assets written to: ram://2d59343b-d9ab-46a4-87e1-3c4e75f7aec8/assets\n",
      "INFO:tensorflow:Assets written to: ram://7452ed46-fa18-410e-afea-584abdf1a670/assets\n",
      "5600 [D loss: 0.423(R 0.492, F0.354)] [D acc: 0.844(R 0.750, F 0.938)] [G loss: 0.423] [G acc: 0.125]\n",
      "5601 [D loss: 0.542(R 0.535, F0.549)] [D acc: 0.773(R 0.672, F 0.875)] [G loss: 0.542] [G acc: 0.078]\n",
      "5602 [D loss: 0.462(R 0.466, F0.459)] [D acc: 0.789(R 0.766, F 0.812)] [G loss: 0.462] [G acc: 0.156]\n",
      "5603 [D loss: 0.417(R 0.423, F0.411)] [D acc: 0.828(R 0.812, F 0.844)] [G loss: 0.417] [G acc: 0.109]\n",
      "5604 [D loss: 0.534(R 0.626, F0.442)] [D acc: 0.672(R 0.547, F 0.797)] [G loss: 0.534] [G acc: 0.141]\n",
      "5605 [D loss: 0.474(R 0.515, F0.432)] [D acc: 0.766(R 0.703, F 0.828)] [G loss: 0.474] [G acc: 0.094]\n",
      "5606 [D loss: 0.384(R 0.407, F0.361)] [D acc: 0.812(R 0.797, F 0.828)] [G loss: 0.384] [G acc: 0.125]\n",
      "5607 [D loss: 0.459(R 0.402, F0.516)] [D acc: 0.797(R 0.812, F 0.781)] [G loss: 0.459] [G acc: 0.031]\n",
      "5608 [D loss: 0.486(R 0.624, F0.348)] [D acc: 0.742(R 0.609, F 0.875)] [G loss: 0.486] [G acc: 0.094]\n",
      "5609 [D loss: 0.375(R 0.390, F0.360)] [D acc: 0.828(R 0.797, F 0.859)] [G loss: 0.375] [G acc: 0.141]\n",
      "5610 [D loss: 0.434(R 0.478, F0.390)] [D acc: 0.805(R 0.734, F 0.875)] [G loss: 0.434] [G acc: 0.062]\n",
      "5611 [D loss: 0.489(R 0.476, F0.502)] [D acc: 0.773(R 0.703, F 0.844)] [G loss: 0.489] [G acc: 0.031]\n",
      "5612 [D loss: 0.486(R 0.609, F0.363)] [D acc: 0.742(R 0.562, F 0.922)] [G loss: 0.486] [G acc: 0.031]\n",
      "5613 [D loss: 0.531(R 0.552, F0.509)] [D acc: 0.727(R 0.641, F 0.812)] [G loss: 0.531] [G acc: 0.031]\n",
      "5614 [D loss: 0.517(R 0.551, F0.483)] [D acc: 0.773(R 0.672, F 0.875)] [G loss: 0.517] [G acc: 0.047]\n",
      "5615 [D loss: 0.604(R 0.509, F0.699)] [D acc: 0.719(R 0.719, F 0.719)] [G loss: 0.604] [G acc: 0.016]\n",
      "5616 [D loss: 0.427(R 0.431, F0.423)] [D acc: 0.773(R 0.688, F 0.859)] [G loss: 0.427] [G acc: 0.047]\n",
      "5617 [D loss: 0.513(R 0.603, F0.422)] [D acc: 0.695(R 0.594, F 0.797)] [G loss: 0.513] [G acc: 0.031]\n",
      "5618 [D loss: 0.483(R 0.578, F0.387)] [D acc: 0.727(R 0.578, F 0.875)] [G loss: 0.483] [G acc: 0.062]\n",
      "5619 [D loss: 0.487(R 0.505, F0.468)] [D acc: 0.742(R 0.688, F 0.797)] [G loss: 0.487] [G acc: 0.094]\n",
      "5620 [D loss: 0.443(R 0.345, F0.540)] [D acc: 0.805(R 0.828, F 0.781)] [G loss: 0.443] [G acc: 0.062]\n",
      "5621 [D loss: 0.485(R 0.573, F0.398)] [D acc: 0.781(R 0.656, F 0.906)] [G loss: 0.485] [G acc: 0.047]\n",
      "5622 [D loss: 0.479(R 0.528, F0.430)] [D acc: 0.750(R 0.672, F 0.828)] [G loss: 0.479] [G acc: 0.109]\n",
      "5623 [D loss: 0.605(R 0.644, F0.565)] [D acc: 0.734(R 0.672, F 0.797)] [G loss: 0.605] [G acc: 0.062]\n",
      "5624 [D loss: 0.528(R 0.494, F0.562)] [D acc: 0.773(R 0.719, F 0.828)] [G loss: 0.528] [G acc: 0.156]\n",
      "5625 [D loss: 0.492(R 0.553, F0.432)] [D acc: 0.789(R 0.703, F 0.875)] [G loss: 0.492] [G acc: 0.062]\n",
      "5626 [D loss: 0.681(R 0.561, F0.801)] [D acc: 0.727(R 0.688, F 0.766)] [G loss: 0.681] [G acc: 0.062]\n",
      "5627 [D loss: 0.445(R 0.564, F0.325)] [D acc: 0.805(R 0.688, F 0.922)] [G loss: 0.445] [G acc: 0.109]\n",
      "5628 [D loss: 0.510(R 0.566, F0.454)] [D acc: 0.758(R 0.688, F 0.828)] [G loss: 0.510] [G acc: 0.078]\n",
      "5629 [D loss: 0.471(R 0.600, F0.341)] [D acc: 0.781(R 0.656, F 0.906)] [G loss: 0.471] [G acc: 0.031]\n",
      "5630 [D loss: 0.444(R 0.340, F0.548)] [D acc: 0.789(R 0.797, F 0.781)] [G loss: 0.444] [G acc: 0.094]\n",
      "5631 [D loss: 0.435(R 0.515, F0.356)] [D acc: 0.758(R 0.641, F 0.875)] [G loss: 0.435] [G acc: 0.078]\n",
      "5632 [D loss: 0.443(R 0.367, F0.518)] [D acc: 0.773(R 0.781, F 0.766)] [G loss: 0.443] [G acc: 0.062]\n",
      "5633 [D loss: 0.469(R 0.546, F0.392)] [D acc: 0.773(R 0.703, F 0.844)] [G loss: 0.469] [G acc: 0.031]\n",
      "5634 [D loss: 0.542(R 0.402, F0.682)] [D acc: 0.719(R 0.719, F 0.719)] [G loss: 0.542] [G acc: 0.125]\n",
      "5635 [D loss: 0.469(R 0.536, F0.403)] [D acc: 0.766(R 0.672, F 0.859)] [G loss: 0.469] [G acc: 0.000]\n",
      "5636 [D loss: 0.492(R 0.554, F0.430)] [D acc: 0.773(R 0.703, F 0.844)] [G loss: 0.492] [G acc: 0.047]\n",
      "5637 [D loss: 0.511(R 0.540, F0.482)] [D acc: 0.719(R 0.672, F 0.766)] [G loss: 0.511] [G acc: 0.141]\n",
      "5638 [D loss: 0.479(R 0.528, F0.430)] [D acc: 0.742(R 0.656, F 0.828)] [G loss: 0.479] [G acc: 0.062]\n",
      "5639 [D loss: 0.495(R 0.515, F0.476)] [D acc: 0.742(R 0.688, F 0.797)] [G loss: 0.495] [G acc: 0.031]\n",
      "5640 [D loss: 0.482(R 0.575, F0.389)] [D acc: 0.797(R 0.656, F 0.938)] [G loss: 0.482] [G acc: 0.047]\n",
      "5641 [D loss: 0.442(R 0.416, F0.468)] [D acc: 0.789(R 0.797, F 0.781)] [G loss: 0.442] [G acc: 0.078]\n",
      "5642 [D loss: 0.331(R 0.385, F0.278)] [D acc: 0.859(R 0.812, F 0.906)] [G loss: 0.331] [G acc: 0.047]\n",
      "5643 [D loss: 0.541(R 0.499, F0.582)] [D acc: 0.742(R 0.734, F 0.750)] [G loss: 0.541] [G acc: 0.031]\n",
      "5644 [D loss: 0.505(R 0.441, F0.568)] [D acc: 0.734(R 0.719, F 0.750)] [G loss: 0.505] [G acc: 0.016]\n",
      "5645 [D loss: 0.480(R 0.542, F0.419)] [D acc: 0.766(R 0.734, F 0.797)] [G loss: 0.480] [G acc: 0.062]\n",
      "5646 [D loss: 0.481(R 0.517, F0.445)] [D acc: 0.742(R 0.656, F 0.828)] [G loss: 0.481] [G acc: 0.062]\n",
      "5647 [D loss: 0.479(R 0.590, F0.369)] [D acc: 0.789(R 0.672, F 0.906)] [G loss: 0.479] [G acc: 0.141]\n",
      "5648 [D loss: 0.605(R 0.475, F0.734)] [D acc: 0.750(R 0.719, F 0.781)] [G loss: 0.605] [G acc: 0.031]\n",
      "5649 [D loss: 0.525(R 0.586, F0.465)] [D acc: 0.758(R 0.625, F 0.891)] [G loss: 0.525] [G acc: 0.094]\n",
      "5650 [D loss: 0.529(R 0.536, F0.522)] [D acc: 0.766(R 0.703, F 0.828)] [G loss: 0.529] [G acc: 0.016]\n",
      "5651 [D loss: 0.527(R 0.644, F0.409)] [D acc: 0.750(R 0.625, F 0.875)] [G loss: 0.527] [G acc: 0.078]\n",
      "5652 [D loss: 0.437(R 0.503, F0.371)] [D acc: 0.828(R 0.703, F 0.953)] [G loss: 0.437] [G acc: 0.047]\n",
      "5653 [D loss: 0.510(R 0.433, F0.587)] [D acc: 0.766(R 0.766, F 0.766)] [G loss: 0.510] [G acc: 0.094]\n",
      "5654 [D loss: 0.573(R 0.539, F0.606)] [D acc: 0.742(R 0.719, F 0.766)] [G loss: 0.573] [G acc: 0.141]\n",
      "5655 [D loss: 0.574(R 0.639, F0.508)] [D acc: 0.688(R 0.578, F 0.797)] [G loss: 0.574] [G acc: 0.078]\n",
      "5656 [D loss: 0.508(R 0.586, F0.431)] [D acc: 0.719(R 0.594, F 0.844)] [G loss: 0.508] [G acc: 0.141]\n",
      "5657 [D loss: 0.500(R 0.399, F0.602)] [D acc: 0.766(R 0.797, F 0.734)] [G loss: 0.500] [G acc: 0.047]\n",
      "5658 [D loss: 0.539(R 0.651, F0.427)] [D acc: 0.758(R 0.641, F 0.875)] [G loss: 0.539] [G acc: 0.062]\n",
      "5659 [D loss: 0.546(R 0.611, F0.481)] [D acc: 0.703(R 0.625, F 0.781)] [G loss: 0.546] [G acc: 0.156]\n",
      "5660 [D loss: 0.400(R 0.461, F0.340)] [D acc: 0.820(R 0.688, F 0.953)] [G loss: 0.400] [G acc: 0.016]\n",
      "5661 [D loss: 0.577(R 0.511, F0.643)] [D acc: 0.766(R 0.688, F 0.844)] [G loss: 0.577] [G acc: 0.047]\n",
      "5662 [D loss: 0.550(R 0.608, F0.493)] [D acc: 0.766(R 0.656, F 0.875)] [G loss: 0.550] [G acc: 0.047]\n",
      "5663 [D loss: 0.497(R 0.573, F0.422)] [D acc: 0.773(R 0.625, F 0.922)] [G loss: 0.497] [G acc: 0.094]\n",
      "5664 [D loss: 0.508(R 0.429, F0.587)] [D acc: 0.711(R 0.734, F 0.688)] [G loss: 0.508] [G acc: 0.109]\n",
      "5665 [D loss: 0.453(R 0.483, F0.424)] [D acc: 0.797(R 0.703, F 0.891)] [G loss: 0.453] [G acc: 0.047]\n",
      "5666 [D loss: 0.533(R 0.528, F0.539)] [D acc: 0.742(R 0.719, F 0.766)] [G loss: 0.533] [G acc: 0.047]\n",
      "5667 [D loss: 0.462(R 0.491, F0.432)] [D acc: 0.789(R 0.734, F 0.844)] [G loss: 0.462] [G acc: 0.109]\n",
      "5668 [D loss: 0.436(R 0.407, F0.465)] [D acc: 0.805(R 0.781, F 0.828)] [G loss: 0.436] [G acc: 0.031]\n",
      "5669 [D loss: 0.519(R 0.574, F0.464)] [D acc: 0.695(R 0.594, F 0.797)] [G loss: 0.519] [G acc: 0.078]\n",
      "5670 [D loss: 0.419(R 0.415, F0.424)] [D acc: 0.820(R 0.781, F 0.859)] [G loss: 0.419] [G acc: 0.125]\n",
      "5671 [D loss: 0.452(R 0.282, F0.621)] [D acc: 0.805(R 0.844, F 0.766)] [G loss: 0.452] [G acc: 0.078]\n",
      "5672 [D loss: 0.537(R 0.594, F0.479)] [D acc: 0.695(R 0.625, F 0.766)] [G loss: 0.537] [G acc: 0.062]\n",
      "5673 [D loss: 0.532(R 0.644, F0.421)] [D acc: 0.766(R 0.656, F 0.875)] [G loss: 0.532] [G acc: 0.062]\n",
      "5674 [D loss: 0.555(R 0.509, F0.601)] [D acc: 0.734(R 0.688, F 0.781)] [G loss: 0.555] [G acc: 0.109]\n",
      "5675 [D loss: 0.436(R 0.526, F0.345)] [D acc: 0.766(R 0.625, F 0.906)] [G loss: 0.436] [G acc: 0.016]\n",
      "5676 [D loss: 0.444(R 0.372, F0.517)] [D acc: 0.773(R 0.781, F 0.766)] [G loss: 0.444] [G acc: 0.031]\n",
      "5677 [D loss: 0.490(R 0.438, F0.543)] [D acc: 0.781(R 0.750, F 0.812)] [G loss: 0.490] [G acc: 0.094]\n",
      "5678 [D loss: 0.487(R 0.571, F0.404)] [D acc: 0.781(R 0.703, F 0.859)] [G loss: 0.487] [G acc: 0.094]\n",
      "5679 [D loss: 0.542(R 0.633, F0.452)] [D acc: 0.727(R 0.641, F 0.812)] [G loss: 0.542] [G acc: 0.172]\n",
      "5680 [D loss: 0.433(R 0.450, F0.415)] [D acc: 0.805(R 0.766, F 0.844)] [G loss: 0.433] [G acc: 0.094]\n",
      "5681 [D loss: 0.404(R 0.375, F0.433)] [D acc: 0.820(R 0.797, F 0.844)] [G loss: 0.404] [G acc: 0.047]\n",
      "5682 [D loss: 0.448(R 0.421, F0.474)] [D acc: 0.797(R 0.781, F 0.812)] [G loss: 0.448] [G acc: 0.031]\n",
      "5683 [D loss: 0.542(R 0.574, F0.509)] [D acc: 0.734(R 0.672, F 0.797)] [G loss: 0.542] [G acc: 0.094]\n",
      "5684 [D loss: 0.518(R 0.526, F0.510)] [D acc: 0.734(R 0.656, F 0.812)] [G loss: 0.518] [G acc: 0.047]\n",
      "5685 [D loss: 0.657(R 1.014, F0.300)] [D acc: 0.758(R 0.547, F 0.969)] [G loss: 0.657] [G acc: 0.000]\n",
      "5686 [D loss: 0.445(R 0.553, F0.337)] [D acc: 0.773(R 0.641, F 0.906)] [G loss: 0.445] [G acc: 0.031]\n",
      "5687 [D loss: 0.375(R 0.408, F0.341)] [D acc: 0.820(R 0.750, F 0.891)] [G loss: 0.375] [G acc: 0.047]\n",
      "5688 [D loss: 0.458(R 0.475, F0.442)] [D acc: 0.812(R 0.750, F 0.875)] [G loss: 0.458] [G acc: 0.062]\n",
      "5689 [D loss: 0.482(R 0.385, F0.578)] [D acc: 0.742(R 0.781, F 0.703)] [G loss: 0.482] [G acc: 0.047]\n",
      "5690 [D loss: 0.583(R 0.484, F0.681)] [D acc: 0.719(R 0.750, F 0.688)] [G loss: 0.583] [G acc: 0.047]\n",
      "5691 [D loss: 0.543(R 0.558, F0.527)] [D acc: 0.766(R 0.719, F 0.812)] [G loss: 0.543] [G acc: 0.125]\n",
      "5692 [D loss: 0.591(R 0.596, F0.586)] [D acc: 0.648(R 0.578, F 0.719)] [G loss: 0.591] [G acc: 0.047]\n",
      "5693 [D loss: 0.498(R 0.571, F0.426)] [D acc: 0.727(R 0.609, F 0.844)] [G loss: 0.498] [G acc: 0.078]\n",
      "5694 [D loss: 0.479(R 0.592, F0.367)] [D acc: 0.773(R 0.656, F 0.891)] [G loss: 0.479] [G acc: 0.062]\n",
      "5695 [D loss: 0.480(R 0.438, F0.523)] [D acc: 0.773(R 0.750, F 0.797)] [G loss: 0.480] [G acc: 0.141]\n",
      "5696 [D loss: 0.506(R 0.562, F0.450)] [D acc: 0.727(R 0.609, F 0.844)] [G loss: 0.506] [G acc: 0.016]\n",
      "5697 [D loss: 0.530(R 0.524, F0.536)] [D acc: 0.750(R 0.703, F 0.797)] [G loss: 0.530] [G acc: 0.125]\n",
      "5698 [D loss: 0.430(R 0.485, F0.375)] [D acc: 0.820(R 0.750, F 0.891)] [G loss: 0.430] [G acc: 0.141]\n",
      "5699 [D loss: 0.383(R 0.343, F0.422)] [D acc: 0.852(R 0.812, F 0.891)] [G loss: 0.383] [G acc: 0.062]\n",
      "INFO:tensorflow:Assets written to: ram://11647b00-9912-46dd-9970-9ed48db27cb3/assets\n",
      "INFO:tensorflow:Assets written to: ram://84d360b1-f8d2-4046-8249-3e6817622d3b/assets\n",
      "INFO:tensorflow:Assets written to: ram://26b6680a-ed99-48ec-a0f6-7802b5520dd6/assets\n",
      "5700 [D loss: 0.529(R 0.591, F0.466)] [D acc: 0.695(R 0.578, F 0.812)] [G loss: 0.529] [G acc: 0.078]\n",
      "5701 [D loss: 0.554(R 0.596, F0.512)] [D acc: 0.719(R 0.609, F 0.828)] [G loss: 0.554] [G acc: 0.109]\n",
      "5702 [D loss: 0.607(R 0.657, F0.557)] [D acc: 0.664(R 0.516, F 0.812)] [G loss: 0.607] [G acc: 0.078]\n",
      "5703 [D loss: 0.463(R 0.539, F0.387)] [D acc: 0.773(R 0.672, F 0.875)] [G loss: 0.463] [G acc: 0.125]\n",
      "5704 [D loss: 0.503(R 0.577, F0.429)] [D acc: 0.766(R 0.703, F 0.828)] [G loss: 0.503] [G acc: 0.078]\n",
      "5705 [D loss: 0.498(R 0.483, F0.513)] [D acc: 0.773(R 0.750, F 0.797)] [G loss: 0.498] [G acc: 0.109]\n",
      "5706 [D loss: 0.470(R 0.510, F0.429)] [D acc: 0.805(R 0.688, F 0.922)] [G loss: 0.470] [G acc: 0.109]\n",
      "5707 [D loss: 0.406(R 0.397, F0.415)] [D acc: 0.844(R 0.781, F 0.906)] [G loss: 0.406] [G acc: 0.062]\n",
      "5708 [D loss: 0.731(R 0.745, F0.716)] [D acc: 0.664(R 0.594, F 0.734)] [G loss: 0.731] [G acc: 0.078]\n",
      "5709 [D loss: 0.567(R 0.728, F0.406)] [D acc: 0.742(R 0.578, F 0.906)] [G loss: 0.567] [G acc: 0.031]\n",
      "5710 [D loss: 0.483(R 0.463, F0.502)] [D acc: 0.773(R 0.734, F 0.812)] [G loss: 0.483] [G acc: 0.016]\n",
      "5711 [D loss: 0.430(R 0.488, F0.371)] [D acc: 0.797(R 0.703, F 0.891)] [G loss: 0.430] [G acc: 0.188]\n",
      "5712 [D loss: 0.613(R 0.616, F0.611)] [D acc: 0.727(R 0.656, F 0.797)] [G loss: 0.613] [G acc: 0.047]\n",
      "5713 [D loss: 0.485(R 0.465, F0.505)] [D acc: 0.742(R 0.703, F 0.781)] [G loss: 0.485] [G acc: 0.078]\n",
      "5714 [D loss: 0.547(R 0.626, F0.468)] [D acc: 0.727(R 0.656, F 0.797)] [G loss: 0.547] [G acc: 0.062]\n",
      "5715 [D loss: 0.500(R 0.578, F0.423)] [D acc: 0.734(R 0.641, F 0.828)] [G loss: 0.500] [G acc: 0.109]\n",
      "5716 [D loss: 0.512(R 0.520, F0.504)] [D acc: 0.719(R 0.625, F 0.812)] [G loss: 0.512] [G acc: 0.125]\n",
      "5717 [D loss: 0.442(R 0.452, F0.433)] [D acc: 0.797(R 0.750, F 0.844)] [G loss: 0.442] [G acc: 0.109]\n",
      "5718 [D loss: 0.381(R 0.411, F0.351)] [D acc: 0.844(R 0.812, F 0.875)] [G loss: 0.381] [G acc: 0.125]\n",
      "5719 [D loss: 0.557(R 0.443, F0.671)] [D acc: 0.758(R 0.828, F 0.688)] [G loss: 0.557] [G acc: 0.031]\n",
      "5720 [D loss: 0.562(R 0.696, F0.429)] [D acc: 0.742(R 0.625, F 0.859)] [G loss: 0.562] [G acc: 0.188]\n",
      "5721 [D loss: 0.505(R 0.488, F0.521)] [D acc: 0.734(R 0.703, F 0.766)] [G loss: 0.505] [G acc: 0.094]\n",
      "5722 [D loss: 0.561(R 0.539, F0.583)] [D acc: 0.734(R 0.703, F 0.766)] [G loss: 0.561] [G acc: 0.047]\n",
      "5723 [D loss: 0.440(R 0.404, F0.477)] [D acc: 0.805(R 0.797, F 0.812)] [G loss: 0.440] [G acc: 0.141]\n",
      "5724 [D loss: 0.537(R 0.597, F0.477)] [D acc: 0.742(R 0.688, F 0.797)] [G loss: 0.537] [G acc: 0.156]\n",
      "5725 [D loss: 0.530(R 0.516, F0.544)] [D acc: 0.750(R 0.719, F 0.781)] [G loss: 0.530] [G acc: 0.094]\n",
      "5726 [D loss: 0.491(R 0.531, F0.452)] [D acc: 0.781(R 0.688, F 0.875)] [G loss: 0.491] [G acc: 0.062]\n",
      "5727 [D loss: 0.486(R 0.582, F0.389)] [D acc: 0.805(R 0.688, F 0.922)] [G loss: 0.486] [G acc: 0.156]\n",
      "5728 [D loss: 0.504(R 0.475, F0.532)] [D acc: 0.797(R 0.781, F 0.812)] [G loss: 0.504] [G acc: 0.031]\n",
      "5729 [D loss: 0.552(R 0.615, F0.489)] [D acc: 0.750(R 0.625, F 0.875)] [G loss: 0.552] [G acc: 0.031]\n",
      "5730 [D loss: 0.586(R 0.573, F0.599)] [D acc: 0.742(R 0.672, F 0.812)] [G loss: 0.586] [G acc: 0.078]\n",
      "5731 [D loss: 0.479(R 0.511, F0.447)] [D acc: 0.781(R 0.672, F 0.891)] [G loss: 0.479] [G acc: 0.031]\n",
      "5732 [D loss: 0.403(R 0.420, F0.387)] [D acc: 0.812(R 0.734, F 0.891)] [G loss: 0.403] [G acc: 0.125]\n",
      "5733 [D loss: 0.514(R 0.539, F0.489)] [D acc: 0.750(R 0.703, F 0.797)] [G loss: 0.514] [G acc: 0.141]\n",
      "5734 [D loss: 0.442(R 0.411, F0.473)] [D acc: 0.789(R 0.797, F 0.781)] [G loss: 0.442] [G acc: 0.000]\n",
      "5735 [D loss: 0.427(R 0.378, F0.476)] [D acc: 0.797(R 0.812, F 0.781)] [G loss: 0.427] [G acc: 0.078]\n",
      "5736 [D loss: 0.502(R 0.529, F0.475)] [D acc: 0.742(R 0.688, F 0.797)] [G loss: 0.502] [G acc: 0.062]\n",
      "5737 [D loss: 0.565(R 0.459, F0.670)] [D acc: 0.766(R 0.797, F 0.734)] [G loss: 0.565] [G acc: 0.062]\n",
      "5738 [D loss: 0.641(R 0.790, F0.492)] [D acc: 0.656(R 0.516, F 0.797)] [G loss: 0.641] [G acc: 0.156]\n",
      "5739 [D loss: 0.434(R 0.502, F0.365)] [D acc: 0.797(R 0.703, F 0.891)] [G loss: 0.434] [G acc: 0.031]\n",
      "5740 [D loss: 0.447(R 0.523, F0.370)] [D acc: 0.734(R 0.625, F 0.844)] [G loss: 0.447] [G acc: 0.047]\n",
      "5741 [D loss: 0.453(R 0.383, F0.523)] [D acc: 0.758(R 0.750, F 0.766)] [G loss: 0.453] [G acc: 0.109]\n",
      "5742 [D loss: 0.492(R 0.451, F0.533)] [D acc: 0.805(R 0.797, F 0.812)] [G loss: 0.492] [G acc: 0.047]\n",
      "5743 [D loss: 0.394(R 0.540, F0.249)] [D acc: 0.820(R 0.672, F 0.969)] [G loss: 0.394] [G acc: 0.062]\n",
      "5744 [D loss: 0.566(R 0.611, F0.521)] [D acc: 0.766(R 0.703, F 0.828)] [G loss: 0.566] [G acc: 0.109]\n",
      "5745 [D loss: 0.476(R 0.529, F0.422)] [D acc: 0.766(R 0.719, F 0.812)] [G loss: 0.476] [G acc: 0.172]\n",
      "5746 [D loss: 0.481(R 0.444, F0.519)] [D acc: 0.766(R 0.750, F 0.781)] [G loss: 0.481] [G acc: 0.094]\n",
      "5747 [D loss: 0.547(R 0.505, F0.588)] [D acc: 0.742(R 0.672, F 0.812)] [G loss: 0.547] [G acc: 0.078]\n",
      "5748 [D loss: 0.513(R 0.527, F0.500)] [D acc: 0.750(R 0.688, F 0.812)] [G loss: 0.513] [G acc: 0.094]\n",
      "5749 [D loss: 0.522(R 0.674, F0.371)] [D acc: 0.734(R 0.578, F 0.891)] [G loss: 0.522] [G acc: 0.047]\n",
      "5750 [D loss: 0.424(R 0.451, F0.397)] [D acc: 0.805(R 0.750, F 0.859)] [G loss: 0.424] [G acc: 0.047]\n",
      "5751 [D loss: 0.475(R 0.538, F0.413)] [D acc: 0.734(R 0.641, F 0.828)] [G loss: 0.475] [G acc: 0.047]\n",
      "5752 [D loss: 0.602(R 0.566, F0.637)] [D acc: 0.672(R 0.594, F 0.750)] [G loss: 0.602] [G acc: 0.109]\n",
      "5753 [D loss: 0.490(R 0.528, F0.452)] [D acc: 0.773(R 0.703, F 0.844)] [G loss: 0.490] [G acc: 0.016]\n",
      "5754 [D loss: 0.418(R 0.411, F0.425)] [D acc: 0.812(R 0.766, F 0.859)] [G loss: 0.418] [G acc: 0.062]\n",
      "5755 [D loss: 0.504(R 0.620, F0.387)] [D acc: 0.750(R 0.641, F 0.859)] [G loss: 0.504] [G acc: 0.094]\n",
      "5756 [D loss: 0.519(R 0.511, F0.528)] [D acc: 0.758(R 0.719, F 0.797)] [G loss: 0.519] [G acc: 0.125]\n",
      "5757 [D loss: 0.478(R 0.445, F0.510)] [D acc: 0.789(R 0.734, F 0.844)] [G loss: 0.478] [G acc: 0.047]\n",
      "5758 [D loss: 0.440(R 0.458, F0.423)] [D acc: 0.781(R 0.734, F 0.828)] [G loss: 0.440] [G acc: 0.125]\n",
      "5759 [D loss: 0.410(R 0.398, F0.422)] [D acc: 0.828(R 0.781, F 0.875)] [G loss: 0.410] [G acc: 0.094]\n",
      "5760 [D loss: 0.642(R 0.529, F0.755)] [D acc: 0.750(R 0.719, F 0.781)] [G loss: 0.642] [G acc: 0.031]\n",
      "5761 [D loss: 0.572(R 0.670, F0.475)] [D acc: 0.695(R 0.594, F 0.797)] [G loss: 0.572] [G acc: 0.141]\n",
      "5762 [D loss: 0.627(R 0.688, F0.567)] [D acc: 0.672(R 0.609, F 0.734)] [G loss: 0.627] [G acc: 0.094]\n",
      "5763 [D loss: 0.425(R 0.477, F0.373)] [D acc: 0.805(R 0.750, F 0.859)] [G loss: 0.425] [G acc: 0.062]\n",
      "5764 [D loss: 0.502(R 0.447, F0.558)] [D acc: 0.758(R 0.766, F 0.750)] [G loss: 0.502] [G acc: 0.078]\n",
      "5765 [D loss: 0.401(R 0.406, F0.396)] [D acc: 0.820(R 0.797, F 0.844)] [G loss: 0.401] [G acc: 0.078]\n",
      "5766 [D loss: 0.589(R 0.568, F0.609)] [D acc: 0.711(R 0.625, F 0.797)] [G loss: 0.589] [G acc: 0.094]\n",
      "5767 [D loss: 0.477(R 0.554, F0.401)] [D acc: 0.773(R 0.688, F 0.859)] [G loss: 0.477] [G acc: 0.125]\n",
      "5768 [D loss: 0.512(R 0.550, F0.473)] [D acc: 0.727(R 0.672, F 0.781)] [G loss: 0.512] [G acc: 0.016]\n",
      "5769 [D loss: 0.519(R 0.583, F0.456)] [D acc: 0.750(R 0.688, F 0.812)] [G loss: 0.519] [G acc: 0.047]\n",
      "5770 [D loss: 0.483(R 0.441, F0.526)] [D acc: 0.766(R 0.719, F 0.812)] [G loss: 0.483] [G acc: 0.047]\n",
      "5771 [D loss: 0.462(R 0.487, F0.437)] [D acc: 0.773(R 0.703, F 0.844)] [G loss: 0.462] [G acc: 0.094]\n",
      "5772 [D loss: 0.557(R 0.540, F0.574)] [D acc: 0.742(R 0.641, F 0.844)] [G loss: 0.557] [G acc: 0.078]\n",
      "5773 [D loss: 0.457(R 0.471, F0.444)] [D acc: 0.781(R 0.719, F 0.844)] [G loss: 0.457] [G acc: 0.094]\n",
      "5774 [D loss: 0.524(R 0.483, F0.566)] [D acc: 0.742(R 0.734, F 0.750)] [G loss: 0.524] [G acc: 0.031]\n",
      "5775 [D loss: 0.491(R 0.559, F0.424)] [D acc: 0.750(R 0.656, F 0.844)] [G loss: 0.491] [G acc: 0.094]\n",
      "5776 [D loss: 0.573(R 0.447, F0.698)] [D acc: 0.758(R 0.766, F 0.750)] [G loss: 0.573] [G acc: 0.078]\n",
      "5777 [D loss: 0.488(R 0.578, F0.398)] [D acc: 0.789(R 0.703, F 0.875)] [G loss: 0.488] [G acc: 0.031]\n",
      "5778 [D loss: 0.571(R 0.504, F0.638)] [D acc: 0.727(R 0.750, F 0.703)] [G loss: 0.571] [G acc: 0.000]\n",
      "5779 [D loss: 0.551(R 0.683, F0.418)] [D acc: 0.773(R 0.641, F 0.906)] [G loss: 0.551] [G acc: 0.094]\n",
      "5780 [D loss: 0.525(R 0.613, F0.438)] [D acc: 0.781(R 0.656, F 0.906)] [G loss: 0.525] [G acc: 0.062]\n",
      "5781 [D loss: 0.456(R 0.396, F0.515)] [D acc: 0.766(R 0.766, F 0.766)] [G loss: 0.456] [G acc: 0.047]\n",
      "5782 [D loss: 0.463(R 0.481, F0.446)] [D acc: 0.797(R 0.750, F 0.844)] [G loss: 0.463] [G acc: 0.047]\n",
      "5783 [D loss: 0.434(R 0.441, F0.428)] [D acc: 0.781(R 0.703, F 0.859)] [G loss: 0.434] [G acc: 0.125]\n",
      "5784 [D loss: 0.395(R 0.448, F0.341)] [D acc: 0.820(R 0.750, F 0.891)] [G loss: 0.395] [G acc: 0.047]\n",
      "5785 [D loss: 0.457(R 0.344, F0.571)] [D acc: 0.797(R 0.797, F 0.797)] [G loss: 0.457] [G acc: 0.031]\n",
      "5786 [D loss: 0.552(R 0.646, F0.458)] [D acc: 0.711(R 0.562, F 0.859)] [G loss: 0.552] [G acc: 0.031]\n",
      "5787 [D loss: 0.609(R 0.676, F0.543)] [D acc: 0.758(R 0.656, F 0.859)] [G loss: 0.609] [G acc: 0.016]\n",
      "5788 [D loss: 0.479(R 0.604, F0.355)] [D acc: 0.766(R 0.625, F 0.906)] [G loss: 0.479] [G acc: 0.078]\n",
      "5789 [D loss: 0.446(R 0.503, F0.389)] [D acc: 0.812(R 0.734, F 0.891)] [G loss: 0.446] [G acc: 0.141]\n",
      "5790 [D loss: 0.392(R 0.308, F0.476)] [D acc: 0.836(R 0.875, F 0.797)] [G loss: 0.392] [G acc: 0.094]\n",
      "5791 [D loss: 0.527(R 0.522, F0.531)] [D acc: 0.727(R 0.656, F 0.797)] [G loss: 0.527] [G acc: 0.016]\n",
      "5792 [D loss: 0.702(R 0.826, F0.578)] [D acc: 0.617(R 0.438, F 0.797)] [G loss: 0.702] [G acc: 0.062]\n",
      "5793 [D loss: 0.510(R 0.505, F0.516)] [D acc: 0.742(R 0.688, F 0.797)] [G loss: 0.510] [G acc: 0.062]\n",
      "5794 [D loss: 0.502(R 0.509, F0.495)] [D acc: 0.719(R 0.641, F 0.797)] [G loss: 0.502] [G acc: 0.047]\n",
      "5795 [D loss: 0.524(R 0.586, F0.461)] [D acc: 0.719(R 0.641, F 0.797)] [G loss: 0.524] [G acc: 0.125]\n",
      "5796 [D loss: 0.495(R 0.525, F0.465)] [D acc: 0.773(R 0.688, F 0.859)] [G loss: 0.495] [G acc: 0.031]\n",
      "5797 [D loss: 0.514(R 0.586, F0.442)] [D acc: 0.789(R 0.703, F 0.875)] [G loss: 0.514] [G acc: 0.125]\n",
      "5798 [D loss: 0.471(R 0.418, F0.523)] [D acc: 0.781(R 0.734, F 0.828)] [G loss: 0.471] [G acc: 0.156]\n",
      "5799 [D loss: 0.472(R 0.497, F0.446)] [D acc: 0.766(R 0.734, F 0.797)] [G loss: 0.472] [G acc: 0.078]\n",
      "INFO:tensorflow:Assets written to: ram://2e469202-5f27-4328-a5bb-0925c28f4bd7/assets\n",
      "INFO:tensorflow:Assets written to: ram://1a07622b-8615-4983-bce7-b5602349522c/assets\n",
      "INFO:tensorflow:Assets written to: ram://7c5e9e54-0010-47a7-8099-448684c6c70d/assets\n",
      "5800 [D loss: 0.604(R 0.655, F0.552)] [D acc: 0.680(R 0.578, F 0.781)] [G loss: 0.604] [G acc: 0.109]\n",
      "5801 [D loss: 0.420(R 0.458, F0.382)] [D acc: 0.805(R 0.750, F 0.859)] [G loss: 0.420] [G acc: 0.109]\n",
      "5802 [D loss: 0.420(R 0.443, F0.396)] [D acc: 0.805(R 0.703, F 0.906)] [G loss: 0.420] [G acc: 0.062]\n",
      "5803 [D loss: 0.563(R 0.450, F0.676)] [D acc: 0.773(R 0.781, F 0.766)] [G loss: 0.563] [G acc: 0.078]\n",
      "5804 [D loss: 0.563(R 0.579, F0.546)] [D acc: 0.703(R 0.625, F 0.781)] [G loss: 0.563] [G acc: 0.109]\n",
      "5805 [D loss: 0.452(R 0.533, F0.371)] [D acc: 0.789(R 0.688, F 0.891)] [G loss: 0.452] [G acc: 0.078]\n",
      "5806 [D loss: 0.423(R 0.365, F0.481)] [D acc: 0.805(R 0.828, F 0.781)] [G loss: 0.423] [G acc: 0.078]\n",
      "5807 [D loss: 0.538(R 0.572, F0.504)] [D acc: 0.742(R 0.688, F 0.797)] [G loss: 0.538] [G acc: 0.031]\n",
      "5808 [D loss: 0.502(R 0.546, F0.458)] [D acc: 0.758(R 0.688, F 0.828)] [G loss: 0.502] [G acc: 0.016]\n",
      "5809 [D loss: 0.528(R 0.597, F0.460)] [D acc: 0.719(R 0.625, F 0.812)] [G loss: 0.528] [G acc: 0.047]\n",
      "5810 [D loss: 0.426(R 0.446, F0.406)] [D acc: 0.766(R 0.719, F 0.812)] [G loss: 0.426] [G acc: 0.078]\n",
      "5811 [D loss: 0.484(R 0.526, F0.443)] [D acc: 0.742(R 0.672, F 0.812)] [G loss: 0.484] [G acc: 0.109]\n",
      "5812 [D loss: 0.416(R 0.404, F0.427)] [D acc: 0.836(R 0.781, F 0.891)] [G loss: 0.416] [G acc: 0.016]\n",
      "5813 [D loss: 0.405(R 0.367, F0.443)] [D acc: 0.797(R 0.766, F 0.828)] [G loss: 0.405] [G acc: 0.016]\n",
      "5814 [D loss: 0.439(R 0.455, F0.424)] [D acc: 0.789(R 0.766, F 0.812)] [G loss: 0.439] [G acc: 0.094]\n",
      "5815 [D loss: 0.532(R 0.436, F0.629)] [D acc: 0.750(R 0.766, F 0.734)] [G loss: 0.532] [G acc: 0.078]\n",
      "5816 [D loss: 0.586(R 0.601, F0.570)] [D acc: 0.734(R 0.672, F 0.797)] [G loss: 0.586] [G acc: 0.125]\n",
      "5817 [D loss: 0.447(R 0.491, F0.403)] [D acc: 0.789(R 0.688, F 0.891)] [G loss: 0.447] [G acc: 0.078]\n",
      "5818 [D loss: 0.486(R 0.513, F0.459)] [D acc: 0.805(R 0.781, F 0.828)] [G loss: 0.486] [G acc: 0.016]\n",
      "5819 [D loss: 0.578(R 0.507, F0.650)] [D acc: 0.711(R 0.719, F 0.703)] [G loss: 0.578] [G acc: 0.031]\n",
      "5820 [D loss: 0.487(R 0.520, F0.454)] [D acc: 0.758(R 0.719, F 0.797)] [G loss: 0.487] [G acc: 0.094]\n",
      "5821 [D loss: 0.478(R 0.553, F0.403)] [D acc: 0.766(R 0.625, F 0.906)] [G loss: 0.478] [G acc: 0.281]\n",
      "5822 [D loss: 0.490(R 0.500, F0.480)] [D acc: 0.789(R 0.719, F 0.859)] [G loss: 0.490] [G acc: 0.188]\n",
      "5823 [D loss: 0.526(R 0.499, F0.552)] [D acc: 0.727(R 0.672, F 0.781)] [G loss: 0.526] [G acc: 0.125]\n",
      "5824 [D loss: 0.466(R 0.524, F0.408)] [D acc: 0.750(R 0.641, F 0.859)] [G loss: 0.466] [G acc: 0.125]\n",
      "5825 [D loss: 0.459(R 0.476, F0.442)] [D acc: 0.766(R 0.734, F 0.797)] [G loss: 0.459] [G acc: 0.109]\n",
      "5826 [D loss: 0.445(R 0.417, F0.473)] [D acc: 0.820(R 0.781, F 0.859)] [G loss: 0.445] [G acc: 0.078]\n",
      "5827 [D loss: 0.431(R 0.424, F0.438)] [D acc: 0.812(R 0.781, F 0.844)] [G loss: 0.431] [G acc: 0.047]\n",
      "5828 [D loss: 0.520(R 0.509, F0.531)] [D acc: 0.727(R 0.672, F 0.781)] [G loss: 0.520] [G acc: 0.094]\n",
      "5829 [D loss: 0.544(R 0.554, F0.534)] [D acc: 0.750(R 0.703, F 0.797)] [G loss: 0.544] [G acc: 0.062]\n",
      "5830 [D loss: 0.530(R 0.588, F0.471)] [D acc: 0.742(R 0.672, F 0.812)] [G loss: 0.530] [G acc: 0.047]\n",
      "5831 [D loss: 0.488(R 0.499, F0.478)] [D acc: 0.773(R 0.719, F 0.828)] [G loss: 0.488] [G acc: 0.047]\n",
      "5832 [D loss: 0.573(R 0.519, F0.626)] [D acc: 0.758(R 0.703, F 0.812)] [G loss: 0.573] [G acc: 0.031]\n",
      "5833 [D loss: 0.513(R 0.577, F0.449)] [D acc: 0.742(R 0.656, F 0.828)] [G loss: 0.513] [G acc: 0.047]\n",
      "5834 [D loss: 0.463(R 0.497, F0.428)] [D acc: 0.773(R 0.719, F 0.828)] [G loss: 0.463] [G acc: 0.062]\n",
      "5835 [D loss: 0.488(R 0.452, F0.524)] [D acc: 0.789(R 0.766, F 0.812)] [G loss: 0.488] [G acc: 0.125]\n",
      "5836 [D loss: 0.446(R 0.456, F0.436)] [D acc: 0.789(R 0.766, F 0.812)] [G loss: 0.446] [G acc: 0.062]\n",
      "5837 [D loss: 0.539(R 0.655, F0.423)] [D acc: 0.711(R 0.562, F 0.859)] [G loss: 0.539] [G acc: 0.031]\n",
      "5838 [D loss: 0.530(R 0.544, F0.517)] [D acc: 0.695(R 0.641, F 0.750)] [G loss: 0.530] [G acc: 0.078]\n",
      "5839 [D loss: 0.575(R 0.521, F0.629)] [D acc: 0.750(R 0.750, F 0.750)] [G loss: 0.575] [G acc: 0.062]\n",
      "5840 [D loss: 0.479(R 0.484, F0.474)] [D acc: 0.773(R 0.703, F 0.844)] [G loss: 0.479] [G acc: 0.141]\n",
      "5841 [D loss: 0.443(R 0.452, F0.433)] [D acc: 0.773(R 0.734, F 0.812)] [G loss: 0.443] [G acc: 0.109]\n",
      "5842 [D loss: 0.598(R 0.485, F0.711)] [D acc: 0.688(R 0.672, F 0.703)] [G loss: 0.598] [G acc: 0.078]\n",
      "5843 [D loss: 0.565(R 0.572, F0.559)] [D acc: 0.711(R 0.656, F 0.766)] [G loss: 0.565] [G acc: 0.094]\n",
      "5844 [D loss: 0.510(R 0.552, F0.468)] [D acc: 0.750(R 0.672, F 0.828)] [G loss: 0.510] [G acc: 0.141]\n",
      "5845 [D loss: 0.439(R 0.444, F0.434)] [D acc: 0.773(R 0.703, F 0.844)] [G loss: 0.439] [G acc: 0.141]\n",
      "5846 [D loss: 0.380(R 0.393, F0.368)] [D acc: 0.812(R 0.781, F 0.844)] [G loss: 0.380] [G acc: 0.000]\n",
      "5847 [D loss: 0.555(R 0.478, F0.631)] [D acc: 0.734(R 0.750, F 0.719)] [G loss: 0.555] [G acc: 0.078]\n",
      "5848 [D loss: 0.556(R 0.692, F0.419)] [D acc: 0.703(R 0.578, F 0.828)] [G loss: 0.556] [G acc: 0.062]\n",
      "5849 [D loss: 0.487(R 0.514, F0.461)] [D acc: 0.781(R 0.719, F 0.844)] [G loss: 0.487] [G acc: 0.172]\n",
      "5850 [D loss: 0.482(R 0.601, F0.364)] [D acc: 0.781(R 0.656, F 0.906)] [G loss: 0.482] [G acc: 0.047]\n",
      "5851 [D loss: 0.623(R 0.640, F0.606)] [D acc: 0.680(R 0.594, F 0.766)] [G loss: 0.623] [G acc: 0.047]\n",
      "5852 [D loss: 0.382(R 0.351, F0.413)] [D acc: 0.812(R 0.781, F 0.844)] [G loss: 0.382] [G acc: 0.094]\n",
      "5853 [D loss: 0.492(R 0.535, F0.449)] [D acc: 0.758(R 0.688, F 0.828)] [G loss: 0.492] [G acc: 0.047]\n",
      "5854 [D loss: 0.547(R 0.512, F0.582)] [D acc: 0.719(R 0.656, F 0.781)] [G loss: 0.547] [G acc: 0.031]\n",
      "5855 [D loss: 0.579(R 0.566, F0.592)] [D acc: 0.672(R 0.625, F 0.719)] [G loss: 0.579] [G acc: 0.156]\n",
      "5856 [D loss: 0.539(R 0.625, F0.454)] [D acc: 0.703(R 0.594, F 0.812)] [G loss: 0.539] [G acc: 0.094]\n",
      "5857 [D loss: 0.478(R 0.518, F0.439)] [D acc: 0.797(R 0.734, F 0.859)] [G loss: 0.478] [G acc: 0.094]\n",
      "5858 [D loss: 0.540(R 0.591, F0.489)] [D acc: 0.727(R 0.688, F 0.766)] [G loss: 0.540] [G acc: 0.062]\n",
      "5859 [D loss: 0.504(R 0.522, F0.486)] [D acc: 0.758(R 0.672, F 0.844)] [G loss: 0.504] [G acc: 0.078]\n",
      "5860 [D loss: 0.480(R 0.495, F0.466)] [D acc: 0.828(R 0.797, F 0.859)] [G loss: 0.480] [G acc: 0.031]\n",
      "5861 [D loss: 0.574(R 0.645, F0.503)] [D acc: 0.680(R 0.594, F 0.766)] [G loss: 0.574] [G acc: 0.109]\n",
      "5862 [D loss: 0.527(R 0.529, F0.525)] [D acc: 0.766(R 0.719, F 0.812)] [G loss: 0.527] [G acc: 0.141]\n",
      "5863 [D loss: 0.519(R 0.542, F0.495)] [D acc: 0.758(R 0.688, F 0.828)] [G loss: 0.519] [G acc: 0.109]\n",
      "5864 [D loss: 0.444(R 0.333, F0.554)] [D acc: 0.805(R 0.875, F 0.734)] [G loss: 0.444] [G acc: 0.219]\n",
      "5865 [D loss: 0.501(R 0.524, F0.478)] [D acc: 0.766(R 0.688, F 0.844)] [G loss: 0.501] [G acc: 0.094]\n",
      "5866 [D loss: 0.438(R 0.389, F0.488)] [D acc: 0.820(R 0.797, F 0.844)] [G loss: 0.438] [G acc: 0.109]\n",
      "5867 [D loss: 0.486(R 0.457, F0.516)] [D acc: 0.797(R 0.766, F 0.828)] [G loss: 0.486] [G acc: 0.094]\n",
      "5868 [D loss: 0.624(R 0.715, F0.532)] [D acc: 0.719(R 0.625, F 0.812)] [G loss: 0.624] [G acc: 0.062]\n",
      "5869 [D loss: 0.487(R 0.483, F0.492)] [D acc: 0.820(R 0.781, F 0.859)] [G loss: 0.487] [G acc: 0.109]\n",
      "5870 [D loss: 0.519(R 0.542, F0.497)] [D acc: 0.727(R 0.703, F 0.750)] [G loss: 0.519] [G acc: 0.125]\n",
      "5871 [D loss: 0.413(R 0.400, F0.426)] [D acc: 0.781(R 0.750, F 0.812)] [G loss: 0.413] [G acc: 0.078]\n",
      "5872 [D loss: 0.499(R 0.522, F0.476)] [D acc: 0.734(R 0.656, F 0.812)] [G loss: 0.499] [G acc: 0.094]\n",
      "5873 [D loss: 0.573(R 0.495, F0.651)] [D acc: 0.734(R 0.719, F 0.750)] [G loss: 0.573] [G acc: 0.047]\n",
      "5874 [D loss: 0.466(R 0.554, F0.378)] [D acc: 0.773(R 0.656, F 0.891)] [G loss: 0.466] [G acc: 0.109]\n",
      "5875 [D loss: 0.512(R 0.557, F0.467)] [D acc: 0.766(R 0.688, F 0.844)] [G loss: 0.512] [G acc: 0.188]\n",
      "5876 [D loss: 0.507(R 0.560, F0.453)] [D acc: 0.766(R 0.688, F 0.844)] [G loss: 0.507] [G acc: 0.109]\n",
      "5877 [D loss: 0.520(R 0.526, F0.514)] [D acc: 0.766(R 0.734, F 0.797)] [G loss: 0.520] [G acc: 0.047]\n",
      "5878 [D loss: 0.479(R 0.467, F0.490)] [D acc: 0.766(R 0.781, F 0.750)] [G loss: 0.479] [G acc: 0.031]\n",
      "5879 [D loss: 0.584(R 0.510, F0.659)] [D acc: 0.719(R 0.703, F 0.734)] [G loss: 0.584] [G acc: 0.062]\n",
      "5880 [D loss: 0.562(R 0.664, F0.460)] [D acc: 0.750(R 0.656, F 0.844)] [G loss: 0.562] [G acc: 0.047]\n",
      "5881 [D loss: 0.502(R 0.527, F0.478)] [D acc: 0.773(R 0.719, F 0.828)] [G loss: 0.502] [G acc: 0.094]\n",
      "5882 [D loss: 0.513(R 0.549, F0.477)] [D acc: 0.727(R 0.688, F 0.766)] [G loss: 0.513] [G acc: 0.109]\n",
      "5883 [D loss: 0.497(R 0.363, F0.630)] [D acc: 0.758(R 0.797, F 0.719)] [G loss: 0.497] [G acc: 0.078]\n",
      "5884 [D loss: 0.474(R 0.575, F0.373)] [D acc: 0.773(R 0.641, F 0.906)] [G loss: 0.474] [G acc: 0.031]\n",
      "5885 [D loss: 0.450(R 0.539, F0.361)] [D acc: 0.789(R 0.688, F 0.891)] [G loss: 0.450] [G acc: 0.109]\n",
      "5886 [D loss: 0.447(R 0.390, F0.504)] [D acc: 0.797(R 0.797, F 0.797)] [G loss: 0.447] [G acc: 0.047]\n",
      "5887 [D loss: 0.416(R 0.436, F0.397)] [D acc: 0.812(R 0.750, F 0.875)] [G loss: 0.416] [G acc: 0.047]\n",
      "5888 [D loss: 0.516(R 0.458, F0.574)] [D acc: 0.742(R 0.750, F 0.734)] [G loss: 0.516] [G acc: 0.062]\n",
      "5889 [D loss: 0.459(R 0.450, F0.469)] [D acc: 0.758(R 0.734, F 0.781)] [G loss: 0.459] [G acc: 0.047]\n",
      "5890 [D loss: 0.543(R 0.533, F0.554)] [D acc: 0.695(R 0.672, F 0.719)] [G loss: 0.543] [G acc: 0.094]\n",
      "5891 [D loss: 0.555(R 0.624, F0.487)] [D acc: 0.703(R 0.641, F 0.766)] [G loss: 0.555] [G acc: 0.062]\n",
      "5892 [D loss: 0.494(R 0.456, F0.533)] [D acc: 0.773(R 0.766, F 0.781)] [G loss: 0.494] [G acc: 0.156]\n",
      "5893 [D loss: 0.496(R 0.479, F0.512)] [D acc: 0.734(R 0.703, F 0.766)] [G loss: 0.496] [G acc: 0.125]\n",
      "5894 [D loss: 0.507(R 0.478, F0.535)] [D acc: 0.734(R 0.672, F 0.797)] [G loss: 0.507] [G acc: 0.062]\n",
      "5895 [D loss: 0.494(R 0.546, F0.442)] [D acc: 0.789(R 0.719, F 0.859)] [G loss: 0.494] [G acc: 0.156]\n",
      "5896 [D loss: 0.479(R 0.533, F0.425)] [D acc: 0.758(R 0.688, F 0.828)] [G loss: 0.479] [G acc: 0.094]\n",
      "5897 [D loss: 0.564(R 0.549, F0.579)] [D acc: 0.703(R 0.656, F 0.750)] [G loss: 0.564] [G acc: 0.078]\n",
      "5898 [D loss: 0.512(R 0.577, F0.448)] [D acc: 0.703(R 0.625, F 0.781)] [G loss: 0.512] [G acc: 0.031]\n",
      "5899 [D loss: 0.448(R 0.524, F0.373)] [D acc: 0.758(R 0.641, F 0.875)] [G loss: 0.448] [G acc: 0.047]\n",
      "INFO:tensorflow:Assets written to: ram://f123ece3-9345-4c41-bb37-7f27c249186e/assets\n",
      "INFO:tensorflow:Assets written to: ram://4c38edb1-855a-4b77-bf1a-7a37a231f936/assets\n",
      "INFO:tensorflow:Assets written to: ram://fe145d8d-a3bc-4fa7-b91a-08d04acd6ea6/assets\n",
      "5900 [D loss: 0.510(R 0.479, F0.541)] [D acc: 0.719(R 0.703, F 0.734)] [G loss: 0.510] [G acc: 0.141]\n",
      "5901 [D loss: 0.546(R 0.640, F0.452)] [D acc: 0.711(R 0.562, F 0.859)] [G loss: 0.546] [G acc: 0.094]\n",
      "5902 [D loss: 0.518(R 0.277, F0.759)] [D acc: 0.773(R 0.844, F 0.703)] [G loss: 0.518] [G acc: 0.094]\n",
      "5903 [D loss: 0.437(R 0.463, F0.411)] [D acc: 0.820(R 0.734, F 0.906)] [G loss: 0.437] [G acc: 0.031]\n",
      "5904 [D loss: 0.515(R 0.561, F0.469)] [D acc: 0.695(R 0.625, F 0.766)] [G loss: 0.515] [G acc: 0.062]\n",
      "5905 [D loss: 0.538(R 0.576, F0.500)] [D acc: 0.719(R 0.594, F 0.844)] [G loss: 0.538] [G acc: 0.047]\n",
      "5906 [D loss: 0.458(R 0.311, F0.606)] [D acc: 0.789(R 0.844, F 0.734)] [G loss: 0.458] [G acc: 0.078]\n",
      "5907 [D loss: 0.537(R 0.590, F0.484)] [D acc: 0.750(R 0.656, F 0.844)] [G loss: 0.537] [G acc: 0.078]\n",
      "5908 [D loss: 0.484(R 0.550, F0.417)] [D acc: 0.773(R 0.688, F 0.859)] [G loss: 0.484] [G acc: 0.109]\n",
      "5909 [D loss: 0.453(R 0.442, F0.464)] [D acc: 0.773(R 0.750, F 0.797)] [G loss: 0.453] [G acc: 0.109]\n",
      "5910 [D loss: 0.532(R 0.540, F0.524)] [D acc: 0.750(R 0.703, F 0.797)] [G loss: 0.532] [G acc: 0.031]\n",
      "5911 [D loss: 0.566(R 0.598, F0.534)] [D acc: 0.703(R 0.609, F 0.797)] [G loss: 0.566] [G acc: 0.078]\n",
      "5912 [D loss: 0.492(R 0.504, F0.480)] [D acc: 0.773(R 0.688, F 0.859)] [G loss: 0.492] [G acc: 0.047]\n",
      "5913 [D loss: 0.476(R 0.447, F0.504)] [D acc: 0.789(R 0.766, F 0.812)] [G loss: 0.476] [G acc: 0.078]\n",
      "5914 [D loss: 0.552(R 0.592, F0.511)] [D acc: 0.758(R 0.688, F 0.828)] [G loss: 0.552] [G acc: 0.047]\n",
      "5915 [D loss: 0.542(R 0.646, F0.439)] [D acc: 0.734(R 0.609, F 0.859)] [G loss: 0.542] [G acc: 0.047]\n",
      "5916 [D loss: 0.448(R 0.418, F0.478)] [D acc: 0.812(R 0.750, F 0.875)] [G loss: 0.448] [G acc: 0.109]\n",
      "5917 [D loss: 0.493(R 0.544, F0.443)] [D acc: 0.742(R 0.656, F 0.828)] [G loss: 0.493] [G acc: 0.016]\n",
      "5918 [D loss: 0.663(R 0.685, F0.641)] [D acc: 0.734(R 0.609, F 0.859)] [G loss: 0.663] [G acc: 0.047]\n",
      "5919 [D loss: 0.544(R 0.553, F0.535)] [D acc: 0.742(R 0.688, F 0.797)] [G loss: 0.544] [G acc: 0.047]\n",
      "5920 [D loss: 0.445(R 0.536, F0.354)] [D acc: 0.797(R 0.672, F 0.922)] [G loss: 0.445] [G acc: 0.031]\n",
      "5921 [D loss: 0.534(R 0.574, F0.493)] [D acc: 0.727(R 0.672, F 0.781)] [G loss: 0.534] [G acc: 0.031]\n",
      "5922 [D loss: 0.508(R 0.521, F0.496)] [D acc: 0.766(R 0.734, F 0.797)] [G loss: 0.508] [G acc: 0.047]\n",
      "5923 [D loss: 0.472(R 0.515, F0.428)] [D acc: 0.742(R 0.703, F 0.781)] [G loss: 0.472] [G acc: 0.109]\n",
      "5924 [D loss: 0.419(R 0.449, F0.390)] [D acc: 0.797(R 0.750, F 0.844)] [G loss: 0.419] [G acc: 0.078]\n",
      "5925 [D loss: 0.432(R 0.413, F0.452)] [D acc: 0.820(R 0.797, F 0.844)] [G loss: 0.432] [G acc: 0.031]\n",
      "5926 [D loss: 0.450(R 0.455, F0.445)] [D acc: 0.789(R 0.750, F 0.828)] [G loss: 0.450] [G acc: 0.094]\n",
      "5927 [D loss: 0.611(R 0.683, F0.539)] [D acc: 0.664(R 0.578, F 0.750)] [G loss: 0.611] [G acc: 0.109]\n",
      "5928 [D loss: 0.471(R 0.337, F0.605)] [D acc: 0.781(R 0.812, F 0.750)] [G loss: 0.471] [G acc: 0.078]\n",
      "5929 [D loss: 0.576(R 0.634, F0.519)] [D acc: 0.703(R 0.609, F 0.797)] [G loss: 0.576] [G acc: 0.062]\n",
      "5930 [D loss: 0.554(R 0.704, F0.404)] [D acc: 0.719(R 0.547, F 0.891)] [G loss: 0.554] [G acc: 0.047]\n",
      "5931 [D loss: 0.469(R 0.451, F0.488)] [D acc: 0.781(R 0.766, F 0.797)] [G loss: 0.469] [G acc: 0.047]\n",
      "5932 [D loss: 0.520(R 0.449, F0.591)] [D acc: 0.734(R 0.703, F 0.766)] [G loss: 0.520] [G acc: 0.078]\n",
      "5933 [D loss: 0.520(R 0.585, F0.455)] [D acc: 0.742(R 0.641, F 0.844)] [G loss: 0.520] [G acc: 0.062]\n",
      "5934 [D loss: 0.490(R 0.553, F0.427)] [D acc: 0.758(R 0.672, F 0.844)] [G loss: 0.490] [G acc: 0.094]\n",
      "5935 [D loss: 0.509(R 0.521, F0.496)] [D acc: 0.727(R 0.672, F 0.781)] [G loss: 0.509] [G acc: 0.094]\n",
      "5936 [D loss: 0.467(R 0.324, F0.611)] [D acc: 0.805(R 0.844, F 0.766)] [G loss: 0.467] [G acc: 0.047]\n",
      "5937 [D loss: 0.488(R 0.628, F0.349)] [D acc: 0.750(R 0.594, F 0.906)] [G loss: 0.488] [G acc: 0.141]\n",
      "5938 [D loss: 0.499(R 0.477, F0.521)] [D acc: 0.758(R 0.734, F 0.781)] [G loss: 0.499] [G acc: 0.078]\n",
      "5939 [D loss: 0.405(R 0.390, F0.420)] [D acc: 0.773(R 0.734, F 0.812)] [G loss: 0.405] [G acc: 0.078]\n",
      "5940 [D loss: 0.494(R 0.560, F0.429)] [D acc: 0.766(R 0.703, F 0.828)] [G loss: 0.494] [G acc: 0.031]\n",
      "5941 [D loss: 0.578(R 0.537, F0.620)] [D acc: 0.750(R 0.734, F 0.766)] [G loss: 0.578] [G acc: 0.078]\n",
      "5942 [D loss: 0.421(R 0.514, F0.328)] [D acc: 0.805(R 0.672, F 0.938)] [G loss: 0.421] [G acc: 0.047]\n",
      "5943 [D loss: 0.571(R 0.570, F0.573)] [D acc: 0.742(R 0.703, F 0.781)] [G loss: 0.571] [G acc: 0.156]\n",
      "5944 [D loss: 0.605(R 0.564, F0.646)] [D acc: 0.750(R 0.656, F 0.844)] [G loss: 0.605] [G acc: 0.062]\n",
      "5945 [D loss: 0.509(R 0.530, F0.488)] [D acc: 0.742(R 0.688, F 0.797)] [G loss: 0.509] [G acc: 0.031]\n",
      "5946 [D loss: 0.524(R 0.561, F0.487)] [D acc: 0.758(R 0.656, F 0.859)] [G loss: 0.524] [G acc: 0.062]\n",
      "5947 [D loss: 0.509(R 0.500, F0.518)] [D acc: 0.742(R 0.734, F 0.750)] [G loss: 0.509] [G acc: 0.031]\n",
      "5948 [D loss: 0.622(R 0.645, F0.598)] [D acc: 0.711(R 0.625, F 0.797)] [G loss: 0.622] [G acc: 0.047]\n",
      "5949 [D loss: 0.467(R 0.537, F0.398)] [D acc: 0.758(R 0.672, F 0.844)] [G loss: 0.467] [G acc: 0.062]\n",
      "5950 [D loss: 0.439(R 0.430, F0.447)] [D acc: 0.773(R 0.703, F 0.844)] [G loss: 0.439] [G acc: 0.031]\n",
      "5951 [D loss: 0.424(R 0.388, F0.459)] [D acc: 0.812(R 0.766, F 0.859)] [G loss: 0.424] [G acc: 0.156]\n",
      "5952 [D loss: 0.469(R 0.535, F0.403)] [D acc: 0.820(R 0.719, F 0.922)] [G loss: 0.469] [G acc: 0.031]\n",
      "5953 [D loss: 0.598(R 0.558, F0.637)] [D acc: 0.703(R 0.656, F 0.750)] [G loss: 0.598] [G acc: 0.047]\n",
      "5954 [D loss: 0.520(R 0.529, F0.510)] [D acc: 0.742(R 0.719, F 0.766)] [G loss: 0.520] [G acc: 0.125]\n",
      "5955 [D loss: 0.493(R 0.484, F0.502)] [D acc: 0.758(R 0.688, F 0.828)] [G loss: 0.493] [G acc: 0.031]\n",
      "5956 [D loss: 0.567(R 0.435, F0.699)] [D acc: 0.766(R 0.719, F 0.812)] [G loss: 0.567] [G acc: 0.016]\n",
      "5957 [D loss: 0.569(R 0.620, F0.518)] [D acc: 0.672(R 0.625, F 0.719)] [G loss: 0.569] [G acc: 0.062]\n",
      "5958 [D loss: 0.474(R 0.482, F0.466)] [D acc: 0.797(R 0.719, F 0.875)] [G loss: 0.474] [G acc: 0.078]\n",
      "5959 [D loss: 0.455(R 0.463, F0.447)] [D acc: 0.750(R 0.672, F 0.828)] [G loss: 0.455] [G acc: 0.094]\n",
      "5960 [D loss: 0.667(R 0.587, F0.747)] [D acc: 0.727(R 0.703, F 0.750)] [G loss: 0.667] [G acc: 0.172]\n",
      "5961 [D loss: 0.462(R 0.552, F0.371)] [D acc: 0.789(R 0.656, F 0.922)] [G loss: 0.462] [G acc: 0.062]\n",
      "5962 [D loss: 0.492(R 0.489, F0.496)] [D acc: 0.750(R 0.734, F 0.766)] [G loss: 0.492] [G acc: 0.062]\n",
      "5963 [D loss: 0.484(R 0.468, F0.501)] [D acc: 0.750(R 0.719, F 0.781)] [G loss: 0.484] [G acc: 0.109]\n",
      "5964 [D loss: 0.455(R 0.562, F0.348)] [D acc: 0.766(R 0.625, F 0.906)] [G loss: 0.455] [G acc: 0.094]\n",
      "5965 [D loss: 0.435(R 0.373, F0.497)] [D acc: 0.773(R 0.797, F 0.750)] [G loss: 0.435] [G acc: 0.203]\n",
      "5966 [D loss: 0.492(R 0.380, F0.604)] [D acc: 0.758(R 0.766, F 0.750)] [G loss: 0.492] [G acc: 0.062]\n",
      "5967 [D loss: 0.463(R 0.506, F0.421)] [D acc: 0.781(R 0.703, F 0.859)] [G loss: 0.463] [G acc: 0.031]\n",
      "5968 [D loss: 0.551(R 0.541, F0.561)] [D acc: 0.742(R 0.703, F 0.781)] [G loss: 0.551] [G acc: 0.000]\n",
      "5969 [D loss: 0.538(R 0.579, F0.497)] [D acc: 0.781(R 0.703, F 0.859)] [G loss: 0.538] [G acc: 0.062]\n",
      "5970 [D loss: 0.543(R 0.644, F0.442)] [D acc: 0.742(R 0.609, F 0.875)] [G loss: 0.543] [G acc: 0.062]\n",
      "5971 [D loss: 0.580(R 0.462, F0.698)] [D acc: 0.727(R 0.766, F 0.688)] [G loss: 0.580] [G acc: 0.094]\n",
      "5972 [D loss: 0.439(R 0.486, F0.392)] [D acc: 0.797(R 0.719, F 0.875)] [G loss: 0.439] [G acc: 0.031]\n",
      "5973 [D loss: 0.427(R 0.462, F0.392)] [D acc: 0.828(R 0.797, F 0.859)] [G loss: 0.427] [G acc: 0.078]\n",
      "5974 [D loss: 0.418(R 0.428, F0.409)] [D acc: 0.828(R 0.797, F 0.859)] [G loss: 0.418] [G acc: 0.031]\n",
      "5975 [D loss: 0.518(R 0.525, F0.511)] [D acc: 0.758(R 0.688, F 0.828)] [G loss: 0.518] [G acc: 0.062]\n",
      "5976 [D loss: 0.371(R 0.426, F0.315)] [D acc: 0.852(R 0.781, F 0.922)] [G loss: 0.371] [G acc: 0.031]\n",
      "5977 [D loss: 0.466(R 0.431, F0.501)] [D acc: 0.812(R 0.766, F 0.859)] [G loss: 0.466] [G acc: 0.031]\n",
      "5978 [D loss: 0.435(R 0.514, F0.355)] [D acc: 0.828(R 0.734, F 0.922)] [G loss: 0.435] [G acc: 0.031]\n",
      "5979 [D loss: 0.508(R 0.370, F0.645)] [D acc: 0.773(R 0.812, F 0.734)] [G loss: 0.508] [G acc: 0.047]\n",
      "5980 [D loss: 0.529(R 0.697, F0.362)] [D acc: 0.750(R 0.594, F 0.906)] [G loss: 0.529] [G acc: 0.109]\n",
      "5981 [D loss: 0.494(R 0.620, F0.368)] [D acc: 0.742(R 0.594, F 0.891)] [G loss: 0.494] [G acc: 0.047]\n",
      "5982 [D loss: 0.504(R 0.534, F0.473)] [D acc: 0.742(R 0.703, F 0.781)] [G loss: 0.504] [G acc: 0.062]\n",
      "5983 [D loss: 0.434(R 0.457, F0.411)] [D acc: 0.789(R 0.734, F 0.844)] [G loss: 0.434] [G acc: 0.062]\n",
      "5984 [D loss: 0.420(R 0.438, F0.402)] [D acc: 0.820(R 0.766, F 0.875)] [G loss: 0.420] [G acc: 0.078]\n",
      "5985 [D loss: 0.479(R 0.470, F0.487)] [D acc: 0.719(R 0.656, F 0.781)] [G loss: 0.479] [G acc: 0.031]\n",
      "5986 [D loss: 0.595(R 0.555, F0.635)] [D acc: 0.719(R 0.734, F 0.703)] [G loss: 0.595] [G acc: 0.062]\n",
      "5987 [D loss: 0.467(R 0.476, F0.458)] [D acc: 0.805(R 0.750, F 0.859)] [G loss: 0.467] [G acc: 0.062]\n",
      "5988 [D loss: 0.508(R 0.496, F0.519)] [D acc: 0.727(R 0.734, F 0.719)] [G loss: 0.508] [G acc: 0.094]\n",
      "5989 [D loss: 0.542(R 0.610, F0.474)] [D acc: 0.781(R 0.688, F 0.875)] [G loss: 0.542] [G acc: 0.062]\n",
      "5990 [D loss: 0.528(R 0.578, F0.478)] [D acc: 0.719(R 0.578, F 0.859)] [G loss: 0.528] [G acc: 0.125]\n",
      "5991 [D loss: 0.485(R 0.402, F0.567)] [D acc: 0.789(R 0.797, F 0.781)] [G loss: 0.485] [G acc: 0.172]\n",
      "5992 [D loss: 0.534(R 0.545, F0.522)] [D acc: 0.695(R 0.656, F 0.734)] [G loss: 0.534] [G acc: 0.078]\n",
      "5993 [D loss: 0.546(R 0.663, F0.428)] [D acc: 0.734(R 0.594, F 0.875)] [G loss: 0.546] [G acc: 0.141]\n",
      "5994 [D loss: 0.385(R 0.423, F0.347)] [D acc: 0.820(R 0.719, F 0.922)] [G loss: 0.385] [G acc: 0.109]\n",
      "5995 [D loss: 0.559(R 0.619, F0.499)] [D acc: 0.734(R 0.641, F 0.828)] [G loss: 0.559] [G acc: 0.031]\n",
      "5996 [D loss: 0.406(R 0.372, F0.441)] [D acc: 0.836(R 0.812, F 0.859)] [G loss: 0.406] [G acc: 0.125]\n",
      "5997 [D loss: 0.470(R 0.485, F0.455)] [D acc: 0.766(R 0.672, F 0.859)] [G loss: 0.470] [G acc: 0.109]\n",
      "5998 [D loss: 0.438(R 0.444, F0.433)] [D acc: 0.797(R 0.766, F 0.828)] [G loss: 0.438] [G acc: 0.109]\n",
      "5999 [D loss: 0.557(R 0.580, F0.535)] [D acc: 0.750(R 0.656, F 0.844)] [G loss: 0.557] [G acc: 0.078]\n",
      "INFO:tensorflow:Assets written to: ram://23fdaaec-aeb6-49d2-a452-3269ee2d0462/assets\n",
      "INFO:tensorflow:Assets written to: ram://7b4eed0b-18fd-45e2-8ebc-e6d5b44d4ba2/assets\n",
      "INFO:tensorflow:Assets written to: ram://5d29ec0b-3662-465e-baa2-108cd64b8429/assets\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 64\n",
    "EPOCHS = 6000\n",
    "PRINT_EVERY_N_BATCHES = 100\n",
    "\n",
    "gan.train(\n",
    "    x_train,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    run_folder=RUN_FOLDER,\n",
    "    print_every_n_batches=PRINT_EVERY_N_BATCHES\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练损失可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "plt.plot([x[0] for x in gan.d_losses], color=\"black\", linewidth=0.25)\n",
    "plt.plot([x[1] for x in gan.d_losses], color=\"green\", linewidth=0.25)\n",
    "plt.plot([x[2] for x in gan.d_losses], color=\"red\", linewidth=0.25)\n",
    "plt.plot([x[0] for x in gan.g_losses], color=\"orange\", linewidth=0.25)\n",
    "\n",
    "plt.xlabel(\"batch\", fontsize=18)\n",
    "plt.ylabel(\"loss\", fontsize=16)\n",
    "plt.xlim(0, 2000)\n",
    "plt.ylim(0, 2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "plt.plot([x[3] for x in gan.d_losses], color=\"black\", linewidth=0.25)\n",
    "plt.plot([x[4] for x in gan.d_losses], color=\"green\", linewidth=0.25)\n",
    "plt.plot([x[5] for x in gan.d_losses], color=\"red\", linewidth=0.25)\n",
    "plt.plot([x[1] for x in gan.g_losses], color=\"orange\", linewidth=0.25)\n",
    "\n",
    "plt.xlabel(\"batch\", fontsize=18)\n",
    "plt.ylabel(\"accuracy\", fontsize=16)\n",
    "plt.xlim(0, 2000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 生成图片及对比与生成图片相似的原始图片"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_images(img1, img2):\n",
    "    return np.mean(np.abs(img1 - img2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "r, c = 5, 5\n",
    "\n",
    "idx = np.random.randint(0, x_train.shape[0], 32)\n",
    "true_imgs = (x_train[idx] + 1) * 0.5\n",
    "\n",
    "fig, axs = plt.subplots(r, c, figsize=(15, 15))\n",
    "cnt = 0\n",
    "for i in range(r):\n",
    "    for j in range(c):\n",
    "        axs[i, j].imshow(true_imgs[cnt], cmap=\"Greys\")\n",
    "        axs[i, j].axis(\"off\")\n",
    "        cnt += 1\n",
    "fig.savefig(os.path.join(RUN_FOLDER, \"images/real.png\"))\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "r, c = 5, 5\n",
    "noise = np.random.normal(0, 1, (r * c, gan.z_dim))\n",
    "gen_imgs = gan.generator.predict(noise)\n",
    "gen_imgs = 0.5 * (gen_imgs + 1)\n",
    "\n",
    "fig, axs = plt.subplots(r, c, figsize=(15, 15))\n",
    "cnt = 0\n",
    "for i in range(r):\n",
    "    for j in range(c):\n",
    "        axs[i, j].imshow(np.squeeze(gen_imgs[cnt, :, :, :]), cmap=\"Greys\")\n",
    "        axs[i, j].axis(\"off\")\n",
    "        cnt += 1\n",
    "fig.savefig(os.path.join(RUN_FOLDER, \"images/sample.png\"))\n",
    "plt.close()\n",
    "\n",
    "fig, axs = plt.subplots(r, c, figsize=(15, 15))\n",
    "cnt = 0\n",
    "for i in range(r):\n",
    "    for j in range(c):\n",
    "        c_diff = 99999\n",
    "        c_img = None\n",
    "        for k_idx, k in enumerate((x_train + 1) * 0.5):\n",
    "            diff = compare_images(gen_imgs[cnt, :, :, :], k)\n",
    "            if diff < c_diff:\n",
    "                c_img = np.copy(k)\n",
    "                c_diff = diff\n",
    "        axs[i, j].imshow(c_img, cmap=\"Greys\")\n",
    "        axs[i, j].axis(\"off\")\n",
    "        cnt += 1\n",
    "fig.savefig(os.path.join(RUN_FOLDER, \"images/sample_closest.png\"))\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('TensorFlow')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "d71d38bd0d71aa8fb096966ce492050b4e1d8055a06fdbaefbf5b2c66243d19c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
