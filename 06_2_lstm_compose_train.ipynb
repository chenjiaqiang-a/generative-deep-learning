{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy\n",
    "from music21 import note, chord\n",
    "\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "\n",
    "import tensorflow.keras as keras\n",
    "from models import get_distinct, create_lookups, prepare_sequences, get_music_list, create_network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run params\n",
    "section = 'compose'\n",
    "run_id = '0001'\n",
    "music_name = 'cello'\n",
    "run_folder = 'run/{}/'.format(section)\n",
    "run_folder += '_'.join([run_id, music_name])\n",
    "\n",
    "store_folder = os.path.join(run_folder, 'store')\n",
    "data_folder = os.path.join('data', music_name)\n",
    "\n",
    "if not os.path.exists(run_folder):\n",
    "    os.makedirs(run_folder)\n",
    "    os.makedirs(os.path.join(run_folder, 'store'))\n",
    "    os.makedirs(os.path.join(run_folder, 'output'))\n",
    "    os.makedirs(os.path.join(run_folder, 'weights'))\n",
    "    os.makedirs(os.path.join(run_folder, 'viz'))\n",
    "\n",
    "mode = 'build'  # 'load'\n",
    "\n",
    "# data params\n",
    "intervals = range(1)\n",
    "seq_len = 32\n",
    "\n",
    "# model params\n",
    "embed_size = 100\n",
    "rnn_units = 256\n",
    "use_attention = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if mode == 'build':\n",
    "    music_list, parser = get_music_list(data_folder)\n",
    "    print(len(music_list), 'files in total')\n",
    "\n",
    "    notes = []\n",
    "    durations = []\n",
    "\n",
    "    for i, file in enumerate(music_list):\n",
    "        print(i+1, \"Parsing %s\" % file)\n",
    "        original_score = parser.parse(file).chordify()\n",
    "\n",
    "        for interval in intervals:\n",
    "            score = original_score.transpose(interval)\n",
    "\n",
    "            notes.extend(['START'] * seq_len)\n",
    "            durations.extend([0] * seq_len)\n",
    "\n",
    "            for element in score.flat:\n",
    "                if isinstance(element, note.Note):\n",
    "                    if element.isRest:\n",
    "                        notes.append(str(element.name))\n",
    "                        durations.append(element.duration.quarterLength)\n",
    "                    else:\n",
    "                        notes.append(str(element.nameWithOctave))\n",
    "                        durations.append(element.duration.quarterLength)\n",
    "\n",
    "                if isinstance(element, chord.Chord):\n",
    "                    notes.append(\n",
    "                        '.'.join(n.nameWithOctave for n in element.pitches))\n",
    "                    durations.append(element.duration.quarterLength)\n",
    "\n",
    "    with open(os.path.join(store_folder, 'notes'), 'wb') as f:\n",
    "        # ['G2', 'D3', 'B3', 'A3', 'B3', 'D3', 'B3', 'D3', 'G2',...]\n",
    "        pickle.dump(notes, f)\n",
    "    with open(os.path.join(store_folder, 'durations'), 'wb') as f:\n",
    "        pickle.dump(durations, f)\n",
    "else:\n",
    "    with open(os.path.join(store_folder, 'notes'), 'rb') as f:\n",
    "        # ['G2', 'D3', 'B3', 'A3', 'B3', 'D3', 'B3', 'D3', 'G2',...]\n",
    "        notes = pickle.load(f)\n",
    "    with open(os.path.join(store_folder, 'durations'), 'rb') as f:\n",
    "        durations = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the distinct sets of notes and durations\n",
    "note_names, n_notes = get_distinct(notes)\n",
    "duration_names, n_durations = get_distinct(durations)\n",
    "distincts = [note_names, n_notes, duration_names, n_durations]\n",
    "\n",
    "with open(os.path.join(store_folder, 'distincts'), 'wb') as f:\n",
    "    pickle.dump(distincts, f)\n",
    "\n",
    "# make the lookup dictionaries for notes and dictionaries and save\n",
    "note_to_int, int_to_note = create_lookups(note_names)\n",
    "duration_to_int, int_to_duration = create_lookups(duration_names)\n",
    "lookups = [note_to_int, int_to_note, duration_to_int, int_to_duration]\n",
    "\n",
    "with open(os.path.join(store_folder, 'lookups'), 'wb') as f:\n",
    "    pickle.dump(lookups, f)\n",
    "\n",
    "print('\\nnote_to_int')\n",
    "note_to_int\n",
    "print('\\nduration_to_int')\n",
    "duration_to_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_input, network_output = prepare_sequences(notes, durations, lookups, distincts, seq_len)\n",
    "\n",
    "print('pitch input')\n",
    "print(network_input[0][0])\n",
    "print('duration input')\n",
    "print(network_input[1][0])\n",
    "print('pitch output')\n",
    "print(network_output[0][0])\n",
    "print('duration output')\n",
    "print(network_output[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, att_model = create_network(\n",
    "    n_notes, n_durations, embed_size, rnn_units, use_attention)\n",
    "keras.utils.plot_model(model, to_file=os.path.join(\n",
    "    run_folder, 'viz/model.png'), show_shapes=True, show_layer_names=True)\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_folder = os.path.join(run_folder, 'weights')\n",
    "\n",
    "checkpoint1 = keras.callbacks.ModelCheckpoint(\n",
    "    os.path.join(weights_folder,\n",
    "                 \"weights-improvement-{epoch:02d}-{loss:.4f}-bigger.h5\"),\n",
    "    monitor='loss',\n",
    "    verbose=0,\n",
    "    save_best_only=True,\n",
    "    mode='min'\n",
    ")\n",
    "\n",
    "checkpoint2 = keras.callbacks.ModelCheckpoint(\n",
    "    os.path.join(weights_folder, \"weights.h5\"),\n",
    "    monitor='loss',\n",
    "    verbose=0,\n",
    "    save_best_only=True,\n",
    "    mode='min'\n",
    ")\n",
    "\n",
    "early_stopping = keras.callbacks.EarlyStopping(\n",
    "    monitor='loss', restore_best_weights=True, patience=10\n",
    ")\n",
    "\n",
    "\n",
    "callbacks_list = [\n",
    "    checkpoint1, checkpoint2, early_stopping\n",
    "]\n",
    "\n",
    "model.save_weights(os.path.join(weights_folder, \"weights.h5\"))\n",
    "model.fit(network_input, network_output, epochs=2000000, batch_size=32, validation_split=0.2, callbacks=callbacks_list, shuffle=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('TensorFlow')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d71d38bd0d71aa8fb096966ce492050b4e1d8055a06fdbaefbf5b2c66243d19c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
